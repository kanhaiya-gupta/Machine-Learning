{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6869281-a631-4338-a63a-faf0434161eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714b598b-354e-4d12-afff-eee0d7f16c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "using version 2.5.1+cpu\n",
      "Epoch: 0 | MAE train loss: 116.280228 | MAE test loss: 1.51387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_train)\n\u001b[0;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 87\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     90\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (torch.__version__)\n",
    "\n",
    "_ = torch.manual_seed (2022)\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "print(f'using version {torch.__version__}')\n",
    "\n",
    "# create some known parameters\n",
    "p1 = 2\n",
    "p2 = -13\n",
    "p3 = 26\n",
    "p4 = -7\n",
    "p5 = -28\n",
    "p6 = 20\n",
    "p7 = 1\n",
    "\n",
    "# generate some data\n",
    "def poly(x): \n",
    "    return p1*x**6 + p2*x**5 + p3*x**4 + p4*x**3 + p5*x**2 + p6*x + p7\n",
    "size = 100\n",
    "start = -1\n",
    "end = 3\n",
    "X = torch.arange(start, end, (end-start)/size)\n",
    "y = poly(X) # + torch.normal(0, 0.75, size=(size,)) # if you want to add noise\n",
    "\n",
    "# Train test split\n",
    "X_train = torch.cat((X[:40], X[50:]))\n",
    "y_train = torch.cat((y[:40], y[50:]))\n",
    "X_test = X[40:50]\n",
    "y_test = y[40:50]\n",
    "\n",
    "# Build the model:\n",
    "class PolynomialRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.p1 = nn.Parameter(torch.rand( 1,\n",
    "                                                requires_grad=True,\n",
    "                                                dtype=torch.float32))\n",
    "        self.p2 = nn.Parameter(torch.rand( 1,\n",
    "                                                requires_grad=True,\n",
    "                                                dtype=torch.float32))\n",
    "        self.p3 = nn.Parameter(torch.rand( 1,\n",
    "                                                requires_grad=True,\n",
    "                                                dtype=torch.float32))\n",
    "        self.p4 = nn.Parameter(torch.rand( 1,\n",
    "                                                requires_grad=True,\n",
    "                                                dtype=torch.float32))\n",
    "        self.p5 = nn.Parameter(torch.rand( 1,\n",
    "                                                requires_grad=True, \n",
    "                                                dtype=torch.float32))\n",
    "        self.p6 = nn.Parameter(torch.rand( 1,\n",
    "                                                requires_grad=True,\n",
    "                                                dtype=torch.float32))\n",
    "        self.p7 = nn.Parameter(torch.rand( 1,\n",
    "                                                requires_grad=True,\n",
    "                                                dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        # replace self.p7*x with self.p6*x\n",
    "        return self.p1*x**6 + self.p2*x**5 + self.p3*x**4 + self.p4*x**3 + self.p5*x**2 + self.p6*x + self.p7\n",
    "\n",
    "# Create the model\n",
    "torch.manual_seed(42)\n",
    "model = PolynomialRegressionModel()\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "# learning_rate = 0.0001\n",
    "learning_rate = 0.00001   # reduce the learning rate for stability\n",
    "# add momentum to SGD to speed up training dramatically\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), \n",
    "                            lr = learning_rate, momentum = 0.99)\n",
    "\n",
    "# Train the model\n",
    "# epochs = 10000\n",
    "epochs = 1000001   # one hundred times as many epochs\n",
    "epoch_num = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X_test)\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "        # if epoch % 10 == 0:\n",
    "        if epoch % 100000 == 0:  # print less frequently\n",
    "            epoch_num.append(epoch)\n",
    "            train_losses.append(loss.item())\n",
    "            test_losses.append(test_loss.item())\n",
    "            print(f'Epoch: {epoch} | MAE train loss: {round(loss.item(), 6)} | MAE test loss: {round(test_loss.item(), 6)}')\n",
    "\n",
    "# compare fit polynomial coefficients to originals\n",
    "print ('p1:', p1, model.p1.item())\n",
    "print ('p2:', p2, model.p2.item())\n",
    "print ('p3:', p3, model.p3.item())\n",
    "print ('p4:', p4, model.p4.item())\n",
    "print ('p5:', p5, model.p5.item())\n",
    "print ('p6:', p6, model.p6.item())\n",
    "print ('p7:', p7, model.p7.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f21572-5948-41ad-acc0-83c42db95cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd3a79-5609-4597-be3d-a615a64a6562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
