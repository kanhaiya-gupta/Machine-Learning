{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ce3fed-6b19-4d4e-9ae9-f59d3e535141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanha\\AppData\\Local\\Temp\\ipykernel_18436\\3730382899.py:33: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y = x.dot(w)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"UAssignment 1.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1v55kaL1i7OT1YXE5jXS-ixAkNoRsnZCu\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib . pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Use the following helper code below to visualize the polynomial above (you have to report a figure). \n",
    "# Do not forget the 0-th degree coefficient in the argument coeffs.\n",
    "\n",
    "def plot_polynomial(coeffs , z_range , color = 'b'):\n",
    "  z = np.linspace(z_range[0], z_range[1], 100)\n",
    "  y = np.polynomial.polynomial.polyval(z, coeffs)\n",
    "  plt.plot(z, y, color)\n",
    "plot_polynomial([0, -5, 2, 1, 0.05], [-3,3])\n",
    "\n",
    "# Complete the code below that allows you to generate the data D described above.\n",
    "\n",
    "def create_dataset(w, z_range, sample_size, sigma, seed = 42):\n",
    "  random_state = np.random.RandomState(seed)\n",
    "  z = random_state.uniform(z_range[0], z_range[1], (sample_size))\n",
    "  x = np.zeros((sample_size, w.shape[0]))\n",
    "  for i in range(sample_size):\n",
    "     x[i,0] = 1 # this is the first 1 in the xi vector\n",
    "     for j in range(1, w.shape[0]):\n",
    "       x[i,j] = pow(z[i], j)\n",
    "  y = x.dot(w)\n",
    "  if(sigma > 0):\n",
    "    y = y + random_state.normal(0.0, sigma, sample_size)\n",
    "  return x, y\n",
    "\n",
    "# Use the completed code and the following parameters to generate training and validation data points:\n",
    "\n",
    "#dataset for training\n",
    "x_train, y_train = create_dataset(torch.tensor([0,-5,2,1,0.05]).T, torch.tensor([-3, 3]), 500, 0.5, 0)\n",
    "\n",
    "#dataset for validation\n",
    "x_validate, y_validate = create_dataset(torch.tensor([0,-5,2,1,0.05]).T, torch.tensor([-3, 3]), 500, 0.5, 1)\n",
    "\n",
    "# Visualize the generated training data points (i.e., the (z, y) pairs) in a 2D scatterplot.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x_train[:, 1][:, None], y_train, alpha = 0.3, marker = 'o', color = 'r') # we take just the column of x-es so the column with index 1 because index 0 is the bias column\n",
    "plt.title('2D Scatter plot ')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "#  Do the same for the validation set (report two separate figures).\n",
    "\n",
    "\n",
    "plt.scatter(x_validate[:, 1][:, None], y_validate, alpha = 0.3, marker = 'o', color = 'b') # we take just the column of x-es so the column with index 1 because index 0 is the bias column\n",
    "plt.title('2D Scatter plot ')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8755c4b1-3514-4b0f-9184-96b6fde0b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight values: Parameter containing:\n",
      "tensor([[-0.2925, -0.3556,  0.0153,  0.3984,  0.4211]], requires_grad=True)\n",
      "Step 0: train loss: 24.180421829223633\n",
      "Step 0: val loss: 17.971250534057617\n",
      "Step 1: train loss: 20.039182662963867\n",
      "Step 1: val loss: 17.799156188964844\n",
      "Step 2: train loss: 18.388994216918945\n",
      "Step 2: val loss: 16.363000869750977\n",
      "Step 3: train loss: 17.671615600585938\n",
      "Step 3: val loss: 16.66244125366211\n",
      "Step 4: train loss: 17.319150924682617\n",
      "Step 4: val loss: 16.231794357299805\n",
      "Step 5: train loss: 17.118186950683594\n",
      "Step 5: val loss: 16.37504005432129\n",
      "Step 6: train loss: 16.9848575592041\n",
      "Step 6: val loss: 16.21623992919922\n",
      "Step 7: train loss: 16.884132385253906\n",
      "Step 7: val loss: 16.25263023376465\n",
      "Step 8: train loss: 16.800315856933594\n",
      "Step 8: val loss: 16.17341423034668\n",
      "Step 9: train loss: 16.725812911987305\n",
      "Step 9: val loss: 16.15964126586914\n",
      "Step 10: train loss: 16.656700134277344\n",
      "Step 10: val loss: 16.1021785736084\n",
      "Step 11: train loss: 16.590839385986328\n",
      "Step 11: val loss: 16.06580352783203\n",
      "Step 12: train loss: 16.527023315429688\n",
      "Step 12: val loss: 16.012666702270508\n",
      "Step 13: train loss: 16.46454620361328\n",
      "Step 13: val loss: 15.965846061706543\n",
      "Step 14: train loss: 16.402999877929688\n",
      "Step 14: val loss: 15.912402153015137\n",
      "Step 15: train loss: 16.34213638305664\n",
      "Step 15: val loss: 15.860604286193848\n",
      "Step 16: train loss: 16.28180503845215\n",
      "Step 16: val loss: 15.80611515045166\n",
      "Step 17: train loss: 16.22191619873047\n",
      "Step 17: val loss: 15.751941680908203\n",
      "Step 18: train loss: 16.16242218017578\n",
      "Step 18: val loss: 15.69668197631836\n",
      "Step 19: train loss: 16.103281021118164\n",
      "Step 19: val loss: 15.641448974609375\n",
      "Step 20: train loss: 16.044479370117188\n",
      "Step 20: val loss: 15.58582878112793\n",
      "Step 21: train loss: 15.98599624633789\n",
      "Step 21: val loss: 15.53024673461914\n",
      "Step 22: train loss: 15.927825927734375\n",
      "Step 22: val loss: 15.47459888458252\n",
      "Step 23: train loss: 15.869965553283691\n",
      "Step 23: val loss: 15.41905689239502\n",
      "Step 24: train loss: 15.812408447265625\n",
      "Step 24: val loss: 15.363605499267578\n",
      "Step 25: train loss: 15.75515079498291\n",
      "Step 25: val loss: 15.308324813842773\n",
      "Step 26: train loss: 15.698187828063965\n",
      "Step 26: val loss: 15.253220558166504\n",
      "Step 27: train loss: 15.641523361206055\n",
      "Step 27: val loss: 15.198326110839844\n",
      "Step 28: train loss: 15.585149765014648\n",
      "Step 28: val loss: 15.143651962280273\n",
      "Step 29: train loss: 15.52906608581543\n",
      "Step 29: val loss: 15.089218139648438\n",
      "Step 30: train loss: 15.473272323608398\n",
      "Step 30: val loss: 15.035028457641602\n",
      "Step 31: train loss: 15.417764663696289\n",
      "Step 31: val loss: 14.981090545654297\n",
      "Step 32: train loss: 15.362543106079102\n",
      "Step 32: val loss: 14.927412986755371\n",
      "Step 33: train loss: 15.307605743408203\n",
      "Step 33: val loss: 14.873992919921875\n",
      "Step 34: train loss: 15.252948760986328\n",
      "Step 34: val loss: 14.820841789245605\n",
      "Step 35: train loss: 15.198573112487793\n",
      "Step 35: val loss: 14.767952919006348\n",
      "Step 36: train loss: 15.144474983215332\n",
      "Step 36: val loss: 14.715331077575684\n",
      "Step 37: train loss: 15.090652465820312\n",
      "Step 37: val loss: 14.662973403930664\n",
      "Step 38: train loss: 15.037105560302734\n",
      "Step 38: val loss: 14.610884666442871\n",
      "Step 39: train loss: 14.983830451965332\n",
      "Step 39: val loss: 14.559059143066406\n",
      "Step 40: train loss: 14.930828094482422\n",
      "Step 40: val loss: 14.507499694824219\n",
      "Step 41: train loss: 14.878093719482422\n",
      "Step 41: val loss: 14.45620346069336\n",
      "Step 42: train loss: 14.825630187988281\n",
      "Step 42: val loss: 14.405169486999512\n",
      "Step 43: train loss: 14.773431777954102\n",
      "Step 43: val loss: 14.35439682006836\n",
      "Step 44: train loss: 14.721498489379883\n",
      "Step 44: val loss: 14.30388355255127\n",
      "Step 45: train loss: 14.669828414916992\n",
      "Step 45: val loss: 14.25362777709961\n",
      "Step 46: train loss: 14.618419647216797\n",
      "Step 46: val loss: 14.203632354736328\n",
      "Step 47: train loss: 14.567272186279297\n",
      "Step 47: val loss: 14.153890609741211\n",
      "Step 48: train loss: 14.516382217407227\n",
      "Step 48: val loss: 14.104403495788574\n",
      "Step 49: train loss: 14.46574878692627\n",
      "Step 49: val loss: 14.055168151855469\n",
      "Step 50: train loss: 14.41537094116211\n",
      "Step 50: val loss: 14.006184577941895\n",
      "Step 51: train loss: 14.36524772644043\n",
      "Step 51: val loss: 13.957451820373535\n",
      "Step 52: train loss: 14.315377235412598\n",
      "Step 52: val loss: 13.908968925476074\n",
      "Step 53: train loss: 14.265755653381348\n",
      "Step 53: val loss: 13.860730171203613\n",
      "Step 54: train loss: 14.216385841369629\n",
      "Step 54: val loss: 13.812737464904785\n",
      "Step 55: train loss: 14.167262077331543\n",
      "Step 55: val loss: 13.764986991882324\n",
      "Step 56: train loss: 14.118382453918457\n",
      "Step 56: val loss: 13.717480659484863\n",
      "Step 57: train loss: 14.069750785827637\n",
      "Step 57: val loss: 13.670214653015137\n",
      "Step 58: train loss: 14.0213623046875\n",
      "Step 58: val loss: 13.623188018798828\n",
      "Step 59: train loss: 13.973214149475098\n",
      "Step 59: val loss: 13.576400756835938\n",
      "Step 60: train loss: 13.925305366516113\n",
      "Step 60: val loss: 13.529847145080566\n",
      "Step 61: train loss: 13.87763786315918\n",
      "Step 61: val loss: 13.48353099822998\n",
      "Step 62: train loss: 13.830206871032715\n",
      "Step 62: val loss: 13.437445640563965\n",
      "Step 63: train loss: 13.783010482788086\n",
      "Step 63: val loss: 13.391593933105469\n",
      "Step 64: train loss: 13.736051559448242\n",
      "Step 64: val loss: 13.345973014831543\n",
      "Step 65: train loss: 13.689324378967285\n",
      "Step 65: val loss: 13.300583839416504\n",
      "Step 66: train loss: 13.642827033996582\n",
      "Step 66: val loss: 13.255417823791504\n",
      "Step 67: train loss: 13.596562385559082\n",
      "Step 67: val loss: 13.210481643676758\n",
      "Step 68: train loss: 13.55052661895752\n",
      "Step 68: val loss: 13.165769577026367\n",
      "Step 69: train loss: 13.504715919494629\n",
      "Step 69: val loss: 13.121280670166016\n",
      "Step 70: train loss: 13.459134101867676\n",
      "Step 70: val loss: 13.077012062072754\n",
      "Step 71: train loss: 13.413776397705078\n",
      "Step 71: val loss: 13.032968521118164\n",
      "Step 72: train loss: 13.368642807006836\n",
      "Step 72: val loss: 12.989143371582031\n",
      "Step 73: train loss: 13.32373046875\n",
      "Step 73: val loss: 12.945534706115723\n",
      "Step 74: train loss: 13.27903938293457\n",
      "Step 74: val loss: 12.902146339416504\n",
      "Step 75: train loss: 13.234565734863281\n",
      "Step 75: val loss: 12.85897159576416\n",
      "Step 76: train loss: 13.190311431884766\n",
      "Step 76: val loss: 12.816011428833008\n",
      "Step 77: train loss: 13.146275520324707\n",
      "Step 77: val loss: 12.77326488494873\n",
      "Step 78: train loss: 13.10245418548584\n",
      "Step 78: val loss: 12.730729103088379\n",
      "Step 79: train loss: 13.058846473693848\n",
      "Step 79: val loss: 12.688403129577637\n",
      "Step 80: train loss: 13.01545238494873\n",
      "Step 80: val loss: 12.646289825439453\n",
      "Step 81: train loss: 12.972270965576172\n",
      "Step 81: val loss: 12.604379653930664\n",
      "Step 82: train loss: 12.929298400878906\n",
      "Step 82: val loss: 12.562676429748535\n",
      "Step 83: train loss: 12.886534690856934\n",
      "Step 83: val loss: 12.521183013916016\n",
      "Step 84: train loss: 12.84398078918457\n",
      "Step 84: val loss: 12.479890823364258\n",
      "Step 85: train loss: 12.801631927490234\n",
      "Step 85: val loss: 12.438801765441895\n",
      "Step 86: train loss: 12.759489059448242\n",
      "Step 86: val loss: 12.397913932800293\n",
      "Step 87: train loss: 12.717548370361328\n",
      "Step 87: val loss: 12.357226371765137\n",
      "Step 88: train loss: 12.675814628601074\n",
      "Step 88: val loss: 12.31673812866211\n",
      "Step 89: train loss: 12.63428020477295\n",
      "Step 89: val loss: 12.276449203491211\n",
      "Step 90: train loss: 12.592947006225586\n",
      "Step 90: val loss: 12.236355781555176\n",
      "Step 91: train loss: 12.551811218261719\n",
      "Step 91: val loss: 12.19645881652832\n",
      "Step 92: train loss: 12.51087760925293\n",
      "Step 92: val loss: 12.156755447387695\n",
      "Step 93: train loss: 12.470137596130371\n",
      "Step 93: val loss: 12.1172456741333\n",
      "Step 94: train loss: 12.429594993591309\n",
      "Step 94: val loss: 12.077927589416504\n",
      "Step 95: train loss: 12.389246940612793\n",
      "Step 95: val loss: 12.038802146911621\n",
      "Step 96: train loss: 12.349091529846191\n",
      "Step 96: val loss: 11.999863624572754\n",
      "Step 97: train loss: 12.30912971496582\n",
      "Step 97: val loss: 11.961114883422852\n",
      "Step 98: train loss: 12.269359588623047\n",
      "Step 98: val loss: 11.922554016113281\n",
      "Step 99: train loss: 12.229776382446289\n",
      "Step 99: val loss: 11.88417911529541\n",
      "Step 100: train loss: 12.190384864807129\n",
      "Step 100: val loss: 11.845989227294922\n",
      "Step 101: train loss: 12.151179313659668\n",
      "Step 101: val loss: 11.807984352111816\n",
      "Step 102: train loss: 12.112161636352539\n",
      "Step 102: val loss: 11.770162582397461\n",
      "Step 103: train loss: 12.073331832885742\n",
      "Step 103: val loss: 11.732521057128906\n",
      "Step 104: train loss: 12.03468132019043\n",
      "Step 104: val loss: 11.695062637329102\n",
      "Step 105: train loss: 11.99621868133545\n",
      "Step 105: val loss: 11.657782554626465\n",
      "Step 106: train loss: 11.95793628692627\n",
      "Step 106: val loss: 11.62067985534668\n",
      "Step 107: train loss: 11.919835090637207\n",
      "Step 107: val loss: 11.583756446838379\n",
      "Step 108: train loss: 11.881916046142578\n",
      "Step 108: val loss: 11.547009468078613\n",
      "Step 109: train loss: 11.844173431396484\n",
      "Step 109: val loss: 11.510437965393066\n",
      "Step 110: train loss: 11.806610107421875\n",
      "Step 110: val loss: 11.474040031433105\n",
      "Step 111: train loss: 11.769224166870117\n",
      "Step 111: val loss: 11.43781566619873\n",
      "Step 112: train loss: 11.732013702392578\n",
      "Step 112: val loss: 11.401763916015625\n",
      "Step 113: train loss: 11.694978713989258\n",
      "Step 113: val loss: 11.365882873535156\n",
      "Step 114: train loss: 11.658117294311523\n",
      "Step 114: val loss: 11.330172538757324\n",
      "Step 115: train loss: 11.621427536010742\n",
      "Step 115: val loss: 11.294631004333496\n",
      "Step 116: train loss: 11.584912300109863\n",
      "Step 116: val loss: 11.259258270263672\n",
      "Step 117: train loss: 11.548564910888672\n",
      "Step 117: val loss: 11.224051475524902\n",
      "Step 118: train loss: 11.5123872756958\n",
      "Step 118: val loss: 11.189011573791504\n",
      "Step 119: train loss: 11.4763822555542\n",
      "Step 119: val loss: 11.154136657714844\n",
      "Step 120: train loss: 11.440542221069336\n",
      "Step 120: val loss: 11.119426727294922\n",
      "Step 121: train loss: 11.404869079589844\n",
      "Step 121: val loss: 11.084879875183105\n",
      "Step 122: train loss: 11.369362831115723\n",
      "Step 122: val loss: 11.050495147705078\n",
      "Step 123: train loss: 11.334019660949707\n",
      "Step 123: val loss: 11.016271591186523\n",
      "Step 124: train loss: 11.298843383789062\n",
      "Step 124: val loss: 10.982208251953125\n",
      "Step 125: train loss: 11.26382827758789\n",
      "Step 125: val loss: 10.948304176330566\n",
      "Step 126: train loss: 11.228974342346191\n",
      "Step 126: val loss: 10.914557456970215\n",
      "Step 127: train loss: 11.194283485412598\n",
      "Step 127: val loss: 10.880970001220703\n",
      "Step 128: train loss: 11.159750938415527\n",
      "Step 128: val loss: 10.847538948059082\n",
      "Step 129: train loss: 11.125378608703613\n",
      "Step 129: val loss: 10.814262390136719\n",
      "Step 130: train loss: 11.091164588928223\n",
      "Step 130: val loss: 10.78114128112793\n",
      "Step 131: train loss: 11.057106971740723\n",
      "Step 131: val loss: 10.748174667358398\n",
      "Step 132: train loss: 11.02320671081543\n",
      "Step 132: val loss: 10.715360641479492\n",
      "Step 133: train loss: 10.989461898803711\n",
      "Step 133: val loss: 10.682697296142578\n",
      "Step 134: train loss: 10.955870628356934\n",
      "Step 134: val loss: 10.650186538696289\n",
      "Step 135: train loss: 10.922433853149414\n",
      "Step 135: val loss: 10.617826461791992\n",
      "Step 136: train loss: 10.889151573181152\n",
      "Step 136: val loss: 10.585612297058105\n",
      "Step 137: train loss: 10.856019973754883\n",
      "Step 137: val loss: 10.553550720214844\n",
      "Step 138: train loss: 10.823039054870605\n",
      "Step 138: val loss: 10.52163314819336\n",
      "Step 139: train loss: 10.790207862854004\n",
      "Step 139: val loss: 10.489864349365234\n",
      "Step 140: train loss: 10.757526397705078\n",
      "Step 140: val loss: 10.458240509033203\n",
      "Step 141: train loss: 10.724991798400879\n",
      "Step 141: val loss: 10.426761627197266\n",
      "Step 142: train loss: 10.692607879638672\n",
      "Step 142: val loss: 10.395425796508789\n",
      "Step 143: train loss: 10.660367965698242\n",
      "Step 143: val loss: 10.36423397064209\n",
      "Step 144: train loss: 10.628274917602539\n",
      "Step 144: val loss: 10.333185195922852\n",
      "Step 145: train loss: 10.59632682800293\n",
      "Step 145: val loss: 10.302277565002441\n",
      "Step 146: train loss: 10.564523696899414\n",
      "Step 146: val loss: 10.27151107788086\n",
      "Step 147: train loss: 10.53286361694336\n",
      "Step 147: val loss: 10.240883827209473\n",
      "Step 148: train loss: 10.501344680786133\n",
      "Step 148: val loss: 10.210395812988281\n",
      "Step 149: train loss: 10.46996784210205\n",
      "Step 149: val loss: 10.180045127868652\n",
      "Step 150: train loss: 10.43873405456543\n",
      "Step 150: val loss: 10.149832725524902\n",
      "Step 151: train loss: 10.407638549804688\n",
      "Step 151: val loss: 10.119756698608398\n",
      "Step 152: train loss: 10.37668228149414\n",
      "Step 152: val loss: 10.08981704711914\n",
      "Step 153: train loss: 10.345865249633789\n",
      "Step 153: val loss: 10.060011863708496\n",
      "Step 154: train loss: 10.315184593200684\n",
      "Step 154: val loss: 10.030342102050781\n",
      "Step 155: train loss: 10.28464126586914\n",
      "Step 155: val loss: 10.000804901123047\n",
      "Step 156: train loss: 10.254234313964844\n",
      "Step 156: val loss: 9.971400260925293\n",
      "Step 157: train loss: 10.223962783813477\n",
      "Step 157: val loss: 9.94212818145752\n",
      "Step 158: train loss: 10.193824768066406\n",
      "Step 158: val loss: 9.912986755371094\n",
      "Step 159: train loss: 10.163823127746582\n",
      "Step 159: val loss: 9.883974075317383\n",
      "Step 160: train loss: 10.133951187133789\n",
      "Step 160: val loss: 9.85509204864502\n",
      "Step 161: train loss: 10.104211807250977\n",
      "Step 161: val loss: 9.826339721679688\n",
      "Step 162: train loss: 10.074605941772461\n",
      "Step 162: val loss: 9.797715187072754\n",
      "Step 163: train loss: 10.04512882232666\n",
      "Step 163: val loss: 9.769216537475586\n",
      "Step 164: train loss: 10.015783309936523\n",
      "Step 164: val loss: 9.740845680236816\n",
      "Step 165: train loss: 9.986565589904785\n",
      "Step 165: val loss: 9.712601661682129\n",
      "Step 166: train loss: 9.957478523254395\n",
      "Step 166: val loss: 9.684481620788574\n",
      "Step 167: train loss: 9.928515434265137\n",
      "Step 167: val loss: 9.656487464904785\n",
      "Step 168: train loss: 9.89968204498291\n",
      "Step 168: val loss: 9.628615379333496\n",
      "Step 169: train loss: 9.87097454071045\n",
      "Step 169: val loss: 9.600865364074707\n",
      "Step 170: train loss: 9.842391967773438\n",
      "Step 170: val loss: 9.573240280151367\n",
      "Step 171: train loss: 9.813933372497559\n",
      "Step 171: val loss: 9.545734405517578\n",
      "Step 172: train loss: 9.785601615905762\n",
      "Step 172: val loss: 9.518350601196289\n",
      "Step 173: train loss: 9.757391929626465\n",
      "Step 173: val loss: 9.491085052490234\n",
      "Step 174: train loss: 9.729304313659668\n",
      "Step 174: val loss: 9.46394157409668\n",
      "Step 175: train loss: 9.701339721679688\n",
      "Step 175: val loss: 9.43691635131836\n",
      "Step 176: train loss: 9.67349624633789\n",
      "Step 176: val loss: 9.410008430480957\n",
      "Step 177: train loss: 9.645773887634277\n",
      "Step 177: val loss: 9.383217811584473\n",
      "Step 178: train loss: 9.618169784545898\n",
      "Step 178: val loss: 9.35654354095459\n",
      "Step 179: train loss: 9.59068775177002\n",
      "Step 179: val loss: 9.329986572265625\n",
      "Step 180: train loss: 9.563321113586426\n",
      "Step 180: val loss: 9.303544998168945\n",
      "Step 181: train loss: 9.536075592041016\n",
      "Step 181: val loss: 9.277217864990234\n",
      "Step 182: train loss: 9.50894546508789\n",
      "Step 182: val loss: 9.251005172729492\n",
      "Step 183: train loss: 9.481931686401367\n",
      "Step 183: val loss: 9.224903106689453\n",
      "Step 184: train loss: 9.455034255981445\n",
      "Step 184: val loss: 9.198917388916016\n",
      "Step 185: train loss: 9.428252220153809\n",
      "Step 185: val loss: 9.173042297363281\n",
      "Step 186: train loss: 9.401583671569824\n",
      "Step 186: val loss: 9.147278785705566\n",
      "Step 187: train loss: 9.375031471252441\n",
      "Step 187: val loss: 9.121625900268555\n",
      "Step 188: train loss: 9.348590850830078\n",
      "Step 188: val loss: 9.096083641052246\n",
      "Step 189: train loss: 9.322263717651367\n",
      "Step 189: val loss: 9.070651054382324\n",
      "Step 190: train loss: 9.296049118041992\n",
      "Step 190: val loss: 9.045326232910156\n",
      "Step 191: train loss: 9.269944190979004\n",
      "Step 191: val loss: 9.020110130310059\n",
      "Step 192: train loss: 9.243951797485352\n",
      "Step 192: val loss: 8.995001792907715\n",
      "Step 193: train loss: 9.218069076538086\n",
      "Step 193: val loss: 8.970001220703125\n",
      "Step 194: train loss: 9.192296028137207\n",
      "Step 194: val loss: 8.945106506347656\n",
      "Step 195: train loss: 9.166631698608398\n",
      "Step 195: val loss: 8.920318603515625\n",
      "Step 196: train loss: 9.141075134277344\n",
      "Step 196: val loss: 8.895634651184082\n",
      "Step 197: train loss: 9.11562728881836\n",
      "Step 197: val loss: 8.871055603027344\n",
      "Step 198: train loss: 9.090287208557129\n",
      "Step 198: val loss: 8.84658145904541\n",
      "Step 199: train loss: 9.065052032470703\n",
      "Step 199: val loss: 8.822209358215332\n",
      "Step 200: train loss: 9.039923667907715\n",
      "Step 200: val loss: 8.797941207885742\n",
      "Step 201: train loss: 9.014900207519531\n",
      "Step 201: val loss: 8.773775100708008\n",
      "Step 202: train loss: 8.989982604980469\n",
      "Step 202: val loss: 8.749710083007812\n",
      "Step 203: train loss: 8.965167999267578\n",
      "Step 203: val loss: 8.725746154785156\n",
      "Step 204: train loss: 8.940458297729492\n",
      "Step 204: val loss: 8.701885223388672\n",
      "Step 205: train loss: 8.915849685668945\n",
      "Step 205: val loss: 8.678123474121094\n",
      "Step 206: train loss: 8.891345977783203\n",
      "Step 206: val loss: 8.654458999633789\n",
      "Step 207: train loss: 8.866942405700684\n",
      "Step 207: val loss: 8.630894660949707\n",
      "Step 208: train loss: 8.842641830444336\n",
      "Step 208: val loss: 8.607428550720215\n",
      "Step 209: train loss: 8.818439483642578\n",
      "Step 209: val loss: 8.584060668945312\n",
      "Step 210: train loss: 8.794339179992676\n",
      "Step 210: val loss: 8.560790061950684\n",
      "Step 211: train loss: 8.77033805847168\n",
      "Step 211: val loss: 8.537614822387695\n",
      "Step 212: train loss: 8.74643611907959\n",
      "Step 212: val loss: 8.51453685760498\n",
      "Step 213: train loss: 8.722631454467773\n",
      "Step 213: val loss: 8.491554260253906\n",
      "Step 214: train loss: 8.69892692565918\n",
      "Step 214: val loss: 8.468666076660156\n",
      "Step 215: train loss: 8.67531967163086\n",
      "Step 215: val loss: 8.44587516784668\n",
      "Step 216: train loss: 8.65180778503418\n",
      "Step 216: val loss: 8.423174858093262\n",
      "Step 217: train loss: 8.628393173217773\n",
      "Step 217: val loss: 8.400568962097168\n",
      "Step 218: train loss: 8.605073928833008\n",
      "Step 218: val loss: 8.378056526184082\n",
      "Step 219: train loss: 8.5818510055542\n",
      "Step 219: val loss: 8.355635643005371\n",
      "Step 220: train loss: 8.558722496032715\n",
      "Step 220: val loss: 8.333308219909668\n",
      "Step 221: train loss: 8.535687446594238\n",
      "Step 221: val loss: 8.311071395874023\n",
      "Step 222: train loss: 8.512746810913086\n",
      "Step 222: val loss: 8.288926124572754\n",
      "Step 223: train loss: 8.489900588989258\n",
      "Step 223: val loss: 8.26686954498291\n",
      "Step 224: train loss: 8.467146873474121\n",
      "Step 224: val loss: 8.244903564453125\n",
      "Step 225: train loss: 8.44448471069336\n",
      "Step 225: val loss: 8.223027229309082\n",
      "Step 226: train loss: 8.421913146972656\n",
      "Step 226: val loss: 8.201239585876465\n",
      "Step 227: train loss: 8.399433135986328\n",
      "Step 227: val loss: 8.179539680480957\n",
      "Step 228: train loss: 8.377045631408691\n",
      "Step 228: val loss: 8.157929420471191\n",
      "Step 229: train loss: 8.354747772216797\n",
      "Step 229: val loss: 8.136404991149902\n",
      "Step 230: train loss: 8.332538604736328\n",
      "Step 230: val loss: 8.114967346191406\n",
      "Step 231: train loss: 8.310419082641602\n",
      "Step 231: val loss: 8.09361743927002\n",
      "Step 232: train loss: 8.288389205932617\n",
      "Step 232: val loss: 8.072352409362793\n",
      "Step 233: train loss: 8.266446113586426\n",
      "Step 233: val loss: 8.051173210144043\n",
      "Step 234: train loss: 8.244592666625977\n",
      "Step 234: val loss: 8.030078887939453\n",
      "Step 235: train loss: 8.222825050354004\n",
      "Step 235: val loss: 8.009068489074707\n",
      "Step 236: train loss: 8.201144218444824\n",
      "Step 236: val loss: 7.988144397735596\n",
      "Step 237: train loss: 8.179551124572754\n",
      "Step 237: val loss: 7.967301845550537\n",
      "Step 238: train loss: 8.15804386138916\n",
      "Step 238: val loss: 7.946542263031006\n",
      "Step 239: train loss: 8.13662052154541\n",
      "Step 239: val loss: 7.925866603851318\n",
      "Step 240: train loss: 8.115283012390137\n",
      "Step 240: val loss: 7.9052734375\n",
      "Step 241: train loss: 8.094030380249023\n",
      "Step 241: val loss: 7.88476037979126\n",
      "Step 242: train loss: 8.072861671447754\n",
      "Step 242: val loss: 7.864330291748047\n",
      "Step 243: train loss: 8.051776885986328\n",
      "Step 243: val loss: 7.843980312347412\n",
      "Step 244: train loss: 8.03077507019043\n",
      "Step 244: val loss: 7.82371187210083\n",
      "Step 245: train loss: 8.009855270385742\n",
      "Step 245: val loss: 7.803520679473877\n",
      "Step 246: train loss: 7.98901891708374\n",
      "Step 246: val loss: 7.783411502838135\n",
      "Step 247: train loss: 7.968263149261475\n",
      "Step 247: val loss: 7.763381004333496\n",
      "Step 248: train loss: 7.947589874267578\n",
      "Step 248: val loss: 7.743428707122803\n",
      "Step 249: train loss: 7.926997184753418\n",
      "Step 249: val loss: 7.723555088043213\n",
      "Step 250: train loss: 7.906484603881836\n",
      "Step 250: val loss: 7.70375919342041\n",
      "Step 251: train loss: 7.886052131652832\n",
      "Step 251: val loss: 7.684040546417236\n",
      "Step 252: train loss: 7.865699291229248\n",
      "Step 252: val loss: 7.66439962387085\n",
      "Step 253: train loss: 7.845426559448242\n",
      "Step 253: val loss: 7.644834041595459\n",
      "Step 254: train loss: 7.825231552124023\n",
      "Step 254: val loss: 7.625345230102539\n",
      "Step 255: train loss: 7.805115699768066\n",
      "Step 255: val loss: 7.605931758880615\n",
      "Step 256: train loss: 7.7850775718688965\n",
      "Step 256: val loss: 7.586594104766846\n",
      "Step 257: train loss: 7.765117168426514\n",
      "Step 257: val loss: 7.5673322677612305\n",
      "Step 258: train loss: 7.745233535766602\n",
      "Step 258: val loss: 7.548142910003662\n",
      "Step 259: train loss: 7.72542667388916\n",
      "Step 259: val loss: 7.529029846191406\n",
      "Step 260: train loss: 7.7056965827941895\n",
      "Step 260: val loss: 7.509988784790039\n",
      "Step 261: train loss: 7.686041831970215\n",
      "Step 261: val loss: 7.491021633148193\n",
      "Step 262: train loss: 7.6664628982543945\n",
      "Step 262: val loss: 7.472127914428711\n",
      "Step 263: train loss: 7.646958827972412\n",
      "Step 263: val loss: 7.453306674957275\n",
      "Step 264: train loss: 7.627530097961426\n",
      "Step 264: val loss: 7.43455696105957\n",
      "Step 265: train loss: 7.608175754547119\n",
      "Step 265: val loss: 7.415879249572754\n",
      "Step 266: train loss: 7.588894844055176\n",
      "Step 266: val loss: 7.397273540496826\n",
      "Step 267: train loss: 7.569687843322754\n",
      "Step 267: val loss: 7.378737449645996\n",
      "Step 268: train loss: 7.550553321838379\n",
      "Step 268: val loss: 7.360273361206055\n",
      "Step 269: train loss: 7.531491756439209\n",
      "Step 269: val loss: 7.341878414154053\n",
      "Step 270: train loss: 7.512503623962402\n",
      "Step 270: val loss: 7.323554039001465\n",
      "Step 271: train loss: 7.493586540222168\n",
      "Step 271: val loss: 7.305298328399658\n",
      "Step 272: train loss: 7.474740505218506\n",
      "Step 272: val loss: 7.287112712860107\n",
      "Step 273: train loss: 7.455966949462891\n",
      "Step 273: val loss: 7.268996238708496\n",
      "Step 274: train loss: 7.437263488769531\n",
      "Step 274: val loss: 7.250946521759033\n",
      "Step 275: train loss: 7.418631076812744\n",
      "Step 275: val loss: 7.23296594619751\n",
      "Step 276: train loss: 7.400068283081055\n",
      "Step 276: val loss: 7.215053081512451\n",
      "Step 277: train loss: 7.381575107574463\n",
      "Step 277: val loss: 7.197206974029541\n",
      "Step 278: train loss: 7.363152027130127\n",
      "Step 278: val loss: 7.179427623748779\n",
      "Step 279: train loss: 7.344797134399414\n",
      "Step 279: val loss: 7.161715030670166\n",
      "Step 280: train loss: 7.326510906219482\n",
      "Step 280: val loss: 7.144069194793701\n",
      "Step 281: train loss: 7.308293342590332\n",
      "Step 281: val loss: 7.12648868560791\n",
      "Step 282: train loss: 7.290143966674805\n",
      "Step 282: val loss: 7.108973979949951\n",
      "Step 283: train loss: 7.272062301635742\n",
      "Step 283: val loss: 7.091524600982666\n",
      "Step 284: train loss: 7.254047870635986\n",
      "Step 284: val loss: 7.074139595031738\n",
      "Step 285: train loss: 7.236100196838379\n",
      "Step 285: val loss: 7.056818962097168\n",
      "Step 286: train loss: 7.218218803405762\n",
      "Step 286: val loss: 7.039563179016113\n",
      "Step 287: train loss: 7.200404167175293\n",
      "Step 287: val loss: 7.0223708152771\n",
      "Step 288: train loss: 7.182655334472656\n",
      "Step 288: val loss: 7.005242347717285\n",
      "Step 289: train loss: 7.164971351623535\n",
      "Step 289: val loss: 6.988175868988037\n",
      "Step 290: train loss: 7.147353172302246\n",
      "Step 290: val loss: 6.971173286437988\n",
      "Step 291: train loss: 7.129799842834473\n",
      "Step 291: val loss: 6.9542317390441895\n",
      "Step 292: train loss: 7.112310409545898\n",
      "Step 292: val loss: 6.937354564666748\n",
      "Step 293: train loss: 7.09488582611084\n",
      "Step 293: val loss: 6.92053747177124\n",
      "Step 294: train loss: 7.0775251388549805\n",
      "Step 294: val loss: 6.903782367706299\n",
      "Step 295: train loss: 7.060226917266846\n",
      "Step 295: val loss: 6.887088775634766\n",
      "Step 296: train loss: 7.04299259185791\n",
      "Step 296: val loss: 6.870456218719482\n",
      "Step 297: train loss: 7.025821208953857\n",
      "Step 297: val loss: 6.853883743286133\n",
      "Step 298: train loss: 7.0087127685546875\n",
      "Step 298: val loss: 6.837371349334717\n",
      "Step 299: train loss: 6.991665840148926\n",
      "Step 299: val loss: 6.820919990539551\n",
      "Step 300: train loss: 6.974681854248047\n",
      "Step 300: val loss: 6.8045268058776855\n",
      "Step 301: train loss: 6.957758903503418\n",
      "Step 301: val loss: 6.788193702697754\n",
      "Step 302: train loss: 6.940896511077881\n",
      "Step 302: val loss: 6.771919250488281\n",
      "Step 303: train loss: 6.924095630645752\n",
      "Step 303: val loss: 6.755703926086426\n",
      "Step 304: train loss: 6.907355308532715\n",
      "Step 304: val loss: 6.739545822143555\n",
      "Step 305: train loss: 6.8906755447387695\n",
      "Step 305: val loss: 6.723447799682617\n",
      "Step 306: train loss: 6.874056339263916\n",
      "Step 306: val loss: 6.7074055671691895\n",
      "Step 307: train loss: 6.857495307922363\n",
      "Step 307: val loss: 6.691422462463379\n",
      "Step 308: train loss: 6.840995788574219\n",
      "Step 308: val loss: 6.675495147705078\n",
      "Step 309: train loss: 6.824553489685059\n",
      "Step 309: val loss: 6.65962553024292\n",
      "Step 310: train loss: 6.808170795440674\n",
      "Step 310: val loss: 6.643812656402588\n",
      "Step 311: train loss: 6.79184627532959\n",
      "Step 311: val loss: 6.628055095672607\n",
      "Step 312: train loss: 6.775580883026123\n",
      "Step 312: val loss: 6.612354755401611\n",
      "Step 313: train loss: 6.759373188018799\n",
      "Step 313: val loss: 6.596709728240967\n",
      "Step 314: train loss: 6.743222236633301\n",
      "Step 314: val loss: 6.58112096786499\n",
      "Step 315: train loss: 6.7271294593811035\n",
      "Step 315: val loss: 6.565585136413574\n",
      "Step 316: train loss: 6.711093425750732\n",
      "Step 316: val loss: 6.550105571746826\n",
      "Step 317: train loss: 6.6951141357421875\n",
      "Step 317: val loss: 6.5346808433532715\n",
      "Step 318: train loss: 6.679191589355469\n",
      "Step 318: val loss: 6.519310474395752\n",
      "Step 319: train loss: 6.66332483291626\n",
      "Step 319: val loss: 6.503993511199951\n",
      "Step 320: train loss: 6.647514820098877\n",
      "Step 320: val loss: 6.488730430603027\n",
      "Step 321: train loss: 6.6317596435546875\n",
      "Step 321: val loss: 6.473520278930664\n",
      "Step 322: train loss: 6.616060733795166\n",
      "Step 322: val loss: 6.4583659172058105\n",
      "Step 323: train loss: 6.6004157066345215\n",
      "Step 323: val loss: 6.443261623382568\n",
      "Step 324: train loss: 6.5848259925842285\n",
      "Step 324: val loss: 6.428210735321045\n",
      "Step 325: train loss: 6.569290637969971\n",
      "Step 325: val loss: 6.41321325302124\n",
      "Step 326: train loss: 6.553809642791748\n",
      "Step 326: val loss: 6.398268222808838\n",
      "Step 327: train loss: 6.5383830070495605\n",
      "Step 327: val loss: 6.383372783660889\n",
      "Step 328: train loss: 6.523009300231934\n",
      "Step 328: val loss: 6.368530750274658\n",
      "Step 329: train loss: 6.507689476013184\n",
      "Step 329: val loss: 6.353739261627197\n",
      "Step 330: train loss: 6.492424011230469\n",
      "Step 330: val loss: 6.338998794555664\n",
      "Step 331: train loss: 6.47721004486084\n",
      "Step 331: val loss: 6.324311256408691\n",
      "Step 332: train loss: 6.4620490074157715\n",
      "Step 332: val loss: 6.3096723556518555\n",
      "Step 333: train loss: 6.446940898895264\n",
      "Step 333: val loss: 6.295083999633789\n",
      "Step 334: train loss: 6.431884288787842\n",
      "Step 334: val loss: 6.280545234680176\n",
      "Step 335: train loss: 6.4168806076049805\n",
      "Step 335: val loss: 6.266057968139648\n",
      "Step 336: train loss: 6.401927471160889\n",
      "Step 336: val loss: 6.251619338989258\n",
      "Step 337: train loss: 6.387026309967041\n",
      "Step 337: val loss: 6.23723030090332\n",
      "Step 338: train loss: 6.372175693511963\n",
      "Step 338: val loss: 6.2228899002075195\n",
      "Step 339: train loss: 6.357377052307129\n",
      "Step 339: val loss: 6.208599090576172\n",
      "Step 340: train loss: 6.342628002166748\n",
      "Step 340: val loss: 6.194356441497803\n",
      "Step 341: train loss: 6.327929496765137\n",
      "Step 341: val loss: 6.180161952972412\n",
      "Step 342: train loss: 6.313281059265137\n",
      "Step 342: val loss: 6.166016101837158\n",
      "Step 343: train loss: 6.298683166503906\n",
      "Step 343: val loss: 6.151916980743408\n",
      "Step 344: train loss: 6.284134387969971\n",
      "Step 344: val loss: 6.137866497039795\n",
      "Step 345: train loss: 6.26963472366333\n",
      "Step 345: val loss: 6.123863220214844\n",
      "Step 346: train loss: 6.255184650421143\n",
      "Step 346: val loss: 6.109907150268555\n",
      "Step 347: train loss: 6.240783214569092\n",
      "Step 347: val loss: 6.095998764038086\n",
      "Step 348: train loss: 6.226430892944336\n",
      "Step 348: val loss: 6.082135200500488\n",
      "Step 349: train loss: 6.212126731872559\n",
      "Step 349: val loss: 6.068320274353027\n",
      "Step 350: train loss: 6.197870254516602\n",
      "Step 350: val loss: 6.054550647735596\n",
      "Step 351: train loss: 6.183662414550781\n",
      "Step 351: val loss: 6.040827751159668\n",
      "Step 352: train loss: 6.169501781463623\n",
      "Step 352: val loss: 6.027149200439453\n",
      "Step 353: train loss: 6.15539026260376\n",
      "Step 353: val loss: 6.013516902923584\n",
      "Step 354: train loss: 6.141324043273926\n",
      "Step 354: val loss: 5.999929904937744\n",
      "Step 355: train loss: 6.12730598449707\n",
      "Step 355: val loss: 5.986388206481934\n",
      "Step 356: train loss: 6.113334655761719\n",
      "Step 356: val loss: 5.972891807556152\n",
      "Step 357: train loss: 6.099409580230713\n",
      "Step 357: val loss: 5.959439754486084\n",
      "Step 358: train loss: 6.085530757904053\n",
      "Step 358: val loss: 5.9460320472717285\n",
      "Step 359: train loss: 6.071698188781738\n",
      "Step 359: val loss: 5.932669162750244\n",
      "Step 360: train loss: 6.057912826538086\n",
      "Step 360: val loss: 5.9193501472473145\n",
      "Step 361: train loss: 6.04417085647583\n",
      "Step 361: val loss: 5.906074047088623\n",
      "Step 362: train loss: 6.030476093292236\n",
      "Step 362: val loss: 5.892842769622803\n",
      "Step 363: train loss: 6.0168256759643555\n",
      "Step 363: val loss: 5.879654407501221\n",
      "Step 364: train loss: 6.003220558166504\n",
      "Step 364: val loss: 5.866508960723877\n",
      "Step 365: train loss: 5.989660263061523\n",
      "Step 365: val loss: 5.853407382965088\n",
      "Step 366: train loss: 5.9761457443237305\n",
      "Step 366: val loss: 5.8403472900390625\n",
      "Step 367: train loss: 5.962673664093018\n",
      "Step 367: val loss: 5.827330589294434\n",
      "Step 368: train loss: 5.949246883392334\n",
      "Step 368: val loss: 5.814356327056885\n",
      "Step 369: train loss: 5.935863971710205\n",
      "Step 369: val loss: 5.801424026489258\n",
      "Step 370: train loss: 5.922524929046631\n",
      "Step 370: val loss: 5.788533687591553\n",
      "Step 371: train loss: 5.909228801727295\n",
      "Step 371: val loss: 5.775684356689453\n",
      "Step 372: train loss: 5.895977020263672\n",
      "Step 372: val loss: 5.762877941131592\n",
      "Step 373: train loss: 5.882768630981445\n",
      "Step 373: val loss: 5.750113010406494\n",
      "Step 374: train loss: 5.869601726531982\n",
      "Step 374: val loss: 5.737388610839844\n",
      "Step 375: train loss: 5.856479167938232\n",
      "Step 375: val loss: 5.724705219268799\n",
      "Step 376: train loss: 5.843398094177246\n",
      "Step 376: val loss: 5.712061882019043\n",
      "Step 377: train loss: 5.830359935760498\n",
      "Step 377: val loss: 5.699460029602051\n",
      "Step 378: train loss: 5.817363739013672\n",
      "Step 378: val loss: 5.686898231506348\n",
      "Step 379: train loss: 5.804409027099609\n",
      "Step 379: val loss: 5.67437744140625\n",
      "Step 380: train loss: 5.791496753692627\n",
      "Step 380: val loss: 5.661894798278809\n",
      "Step 381: train loss: 5.778625965118408\n",
      "Step 381: val loss: 5.649454116821289\n",
      "Step 382: train loss: 5.765796184539795\n",
      "Step 382: val loss: 5.637052059173584\n",
      "Step 383: train loss: 5.753007888793945\n",
      "Step 383: val loss: 5.62468957901001\n",
      "Step 384: train loss: 5.740260124206543\n",
      "Step 384: val loss: 5.612366199493408\n",
      "Step 385: train loss: 5.7275543212890625\n",
      "Step 385: val loss: 5.600081443786621\n",
      "Step 386: train loss: 5.714887619018555\n",
      "Step 386: val loss: 5.587836742401123\n",
      "Step 387: train loss: 5.7022624015808105\n",
      "Step 387: val loss: 5.575629711151123\n",
      "Step 388: train loss: 5.689676761627197\n",
      "Step 388: val loss: 5.563461780548096\n",
      "Step 389: train loss: 5.677131175994873\n",
      "Step 389: val loss: 5.551331996917725\n",
      "Step 390: train loss: 5.664626121520996\n",
      "Step 390: val loss: 5.53924036026001\n",
      "Step 391: train loss: 5.652160167694092\n",
      "Step 391: val loss: 5.527186393737793\n",
      "Step 392: train loss: 5.639733791351318\n",
      "Step 392: val loss: 5.515171051025391\n",
      "Step 393: train loss: 5.627346515655518\n",
      "Step 393: val loss: 5.503193378448486\n",
      "Step 394: train loss: 5.6149983406066895\n",
      "Step 394: val loss: 5.491251468658447\n",
      "Step 395: train loss: 5.602688789367676\n",
      "Step 395: val loss: 5.479348659515381\n",
      "Step 396: train loss: 5.590419769287109\n",
      "Step 396: val loss: 5.467482089996338\n",
      "Step 397: train loss: 5.578187942504883\n",
      "Step 397: val loss: 5.455652236938477\n",
      "Step 398: train loss: 5.5659942626953125\n",
      "Step 398: val loss: 5.443859577178955\n",
      "Step 399: train loss: 5.553839683532715\n",
      "Step 399: val loss: 5.43210506439209\n",
      "Step 400: train loss: 5.541722774505615\n",
      "Step 400: val loss: 5.420384883880615\n",
      "Step 401: train loss: 5.529643535614014\n",
      "Step 401: val loss: 5.408700942993164\n",
      "Step 402: train loss: 5.517602443695068\n",
      "Step 402: val loss: 5.397055149078369\n",
      "Step 403: train loss: 5.505599021911621\n",
      "Step 403: val loss: 5.385442733764648\n",
      "Step 404: train loss: 5.4936323165893555\n",
      "Step 404: val loss: 5.373867511749268\n",
      "Step 405: train loss: 5.481703758239746\n",
      "Step 405: val loss: 5.362328052520752\n",
      "Step 406: train loss: 5.469810962677002\n",
      "Step 406: val loss: 5.350823402404785\n",
      "Step 407: train loss: 5.457955360412598\n",
      "Step 407: val loss: 5.339354038238525\n",
      "Step 408: train loss: 5.446137428283691\n",
      "Step 408: val loss: 5.327919960021973\n",
      "Step 409: train loss: 5.434354782104492\n",
      "Step 409: val loss: 5.316520690917969\n",
      "Step 410: train loss: 5.422609329223633\n",
      "Step 410: val loss: 5.305156230926514\n",
      "Step 411: train loss: 5.410899639129639\n",
      "Step 411: val loss: 5.293826580047607\n",
      "Step 412: train loss: 5.399226188659668\n",
      "Step 412: val loss: 5.282530784606934\n",
      "Step 413: train loss: 5.387588977813721\n",
      "Step 413: val loss: 5.271270275115967\n",
      "Step 414: train loss: 5.3759870529174805\n",
      "Step 414: val loss: 5.260044097900391\n",
      "Step 415: train loss: 5.3644208908081055\n",
      "Step 415: val loss: 5.248851299285889\n",
      "Step 416: train loss: 5.352889537811279\n",
      "Step 416: val loss: 5.237692356109619\n",
      "Step 417: train loss: 5.34139347076416\n",
      "Step 417: val loss: 5.226566314697266\n",
      "Step 418: train loss: 5.329932689666748\n",
      "Step 418: val loss: 5.215475082397461\n",
      "Step 419: train loss: 5.318506717681885\n",
      "Step 419: val loss: 5.2044172286987305\n",
      "Step 420: train loss: 5.3071160316467285\n",
      "Step 420: val loss: 5.193392276763916\n",
      "Step 421: train loss: 5.2957587242126465\n",
      "Step 421: val loss: 5.182399749755859\n",
      "Step 422: train loss: 5.28443717956543\n",
      "Step 422: val loss: 5.171441078186035\n",
      "Step 423: train loss: 5.273149013519287\n",
      "Step 423: val loss: 5.1605143547058105\n",
      "Step 424: train loss: 5.261895656585693\n",
      "Step 424: val loss: 5.14962100982666\n",
      "Step 425: train loss: 5.25067663192749\n",
      "Step 425: val loss: 5.138760089874268\n",
      "Step 426: train loss: 5.239490985870361\n",
      "Step 426: val loss: 5.127931594848633\n",
      "Step 427: train loss: 5.228338241577148\n",
      "Step 427: val loss: 5.117135047912598\n",
      "Step 428: train loss: 5.217219829559326\n",
      "Step 428: val loss: 5.106369972229004\n",
      "Step 429: train loss: 5.206134796142578\n",
      "Step 429: val loss: 5.095638275146484\n",
      "Step 430: train loss: 5.195082664489746\n",
      "Step 430: val loss: 5.0849385261535645\n",
      "Step 431: train loss: 5.1840643882751465\n",
      "Step 431: val loss: 5.074268817901611\n",
      "Step 432: train loss: 5.173078536987305\n",
      "Step 432: val loss: 5.063631534576416\n",
      "Step 433: train loss: 5.162125587463379\n",
      "Step 433: val loss: 5.053025245666504\n",
      "Step 434: train loss: 5.151205539703369\n",
      "Step 434: val loss: 5.042450428009033\n",
      "Step 435: train loss: 5.140317440032959\n",
      "Step 435: val loss: 5.031906604766846\n",
      "Step 436: train loss: 5.129462242126465\n",
      "Step 436: val loss: 5.0213942527771\n",
      "Step 437: train loss: 5.118638515472412\n",
      "Step 437: val loss: 5.010911464691162\n",
      "Step 438: train loss: 5.107848167419434\n",
      "Step 438: val loss: 5.000460624694824\n",
      "Step 439: train loss: 5.09708833694458\n",
      "Step 439: val loss: 4.990039348602295\n",
      "Step 440: train loss: 5.086361408233643\n",
      "Step 440: val loss: 4.979649543762207\n",
      "Step 441: train loss: 5.075665473937988\n",
      "Step 441: val loss: 4.9692888259887695\n",
      "Step 442: train loss: 5.065001964569092\n",
      "Step 442: val loss: 4.958959579467773\n",
      "Step 443: train loss: 5.05436897277832\n",
      "Step 443: val loss: 4.948659896850586\n",
      "Step 444: train loss: 5.04376745223999\n",
      "Step 444: val loss: 4.938390254974365\n",
      "Step 445: train loss: 5.03319787979126\n",
      "Step 445: val loss: 4.928149700164795\n",
      "Step 446: train loss: 5.022658824920654\n",
      "Step 446: val loss: 4.917939186096191\n",
      "Step 447: train loss: 5.012149810791016\n",
      "Step 447: val loss: 4.907757759094238\n",
      "Step 448: train loss: 5.001672267913818\n",
      "Step 448: val loss: 4.897606372833252\n",
      "Step 449: train loss: 4.9912261962890625\n",
      "Step 449: val loss: 4.887484073638916\n",
      "Step 450: train loss: 4.980809211730957\n",
      "Step 450: val loss: 4.8773908615112305\n",
      "Step 451: train loss: 4.970423221588135\n",
      "Step 451: val loss: 4.8673272132873535\n",
      "Step 452: train loss: 4.960067272186279\n",
      "Step 452: val loss: 4.857292175292969\n",
      "Step 453: train loss: 4.949742317199707\n",
      "Step 453: val loss: 4.847285270690918\n",
      "Step 454: train loss: 4.939445972442627\n",
      "Step 454: val loss: 4.837306976318359\n",
      "Step 455: train loss: 4.929180145263672\n",
      "Step 455: val loss: 4.827357292175293\n",
      "Step 456: train loss: 4.918943881988525\n",
      "Step 456: val loss: 4.817437171936035\n",
      "Step 457: train loss: 4.9087371826171875\n",
      "Step 457: val loss: 4.807543754577637\n",
      "Step 458: train loss: 4.8985595703125\n",
      "Step 458: val loss: 4.797679901123047\n",
      "Step 459: train loss: 4.888411045074463\n",
      "Step 459: val loss: 4.787842750549316\n",
      "Step 460: train loss: 4.878293037414551\n",
      "Step 460: val loss: 4.778034210205078\n",
      "Step 461: train loss: 4.868203163146973\n",
      "Step 461: val loss: 4.768253326416016\n",
      "Step 462: train loss: 4.858142375946045\n",
      "Step 462: val loss: 4.758500576019287\n",
      "Step 463: train loss: 4.848110675811768\n",
      "Step 463: val loss: 4.748774528503418\n",
      "Step 464: train loss: 4.838108062744141\n",
      "Step 464: val loss: 4.739076614379883\n",
      "Step 465: train loss: 4.8281331062316895\n",
      "Step 465: val loss: 4.729405403137207\n",
      "Step 466: train loss: 4.8181867599487305\n",
      "Step 466: val loss: 4.719761848449707\n",
      "Step 467: train loss: 4.808269500732422\n",
      "Step 467: val loss: 4.710145473480225\n",
      "Step 468: train loss: 4.798379898071289\n",
      "Step 468: val loss: 4.700555801391602\n",
      "Step 469: train loss: 4.788518905639648\n",
      "Step 469: val loss: 4.690992832183838\n",
      "Step 470: train loss: 4.778685092926025\n",
      "Step 470: val loss: 4.681457042694092\n",
      "Step 471: train loss: 4.7688798904418945\n",
      "Step 471: val loss: 4.671947002410889\n",
      "Step 472: train loss: 4.759101867675781\n",
      "Step 472: val loss: 4.662463665008545\n",
      "Step 473: train loss: 4.749351978302002\n",
      "Step 473: val loss: 4.653008460998535\n",
      "Step 474: train loss: 4.739629745483398\n",
      "Step 474: val loss: 4.643577575683594\n",
      "Step 475: train loss: 4.729934215545654\n",
      "Step 475: val loss: 4.634174346923828\n",
      "Step 476: train loss: 4.720267295837402\n",
      "Step 476: val loss: 4.624795913696289\n",
      "Step 477: train loss: 4.710626125335693\n",
      "Step 477: val loss: 4.615444660186768\n",
      "Step 478: train loss: 4.701013088226318\n",
      "Step 478: val loss: 4.606118202209473\n",
      "Step 479: train loss: 4.6914262771606445\n",
      "Step 479: val loss: 4.596817493438721\n",
      "Step 480: train loss: 4.68186616897583\n",
      "Step 480: val loss: 4.587543487548828\n",
      "Step 481: train loss: 4.672333717346191\n",
      "Step 481: val loss: 4.578294277191162\n",
      "Step 482: train loss: 4.6628265380859375\n",
      "Step 482: val loss: 4.569070816040039\n",
      "Step 483: train loss: 4.653347015380859\n",
      "Step 483: val loss: 4.559871673583984\n",
      "Step 484: train loss: 4.643893718719482\n",
      "Step 484: val loss: 4.550699710845947\n",
      "Step 485: train loss: 4.634466648101807\n",
      "Step 485: val loss: 4.541551113128662\n",
      "Step 486: train loss: 4.625065803527832\n",
      "Step 486: val loss: 4.532428741455078\n",
      "Step 487: train loss: 4.6156907081604\n",
      "Step 487: val loss: 4.523330211639404\n",
      "Step 488: train loss: 4.606342792510986\n",
      "Step 488: val loss: 4.51425838470459\n",
      "Step 489: train loss: 4.597019672393799\n",
      "Step 489: val loss: 4.505209445953369\n",
      "Step 490: train loss: 4.5877227783203125\n",
      "Step 490: val loss: 4.496185779571533\n",
      "Step 491: train loss: 4.578451156616211\n",
      "Step 491: val loss: 4.487186431884766\n",
      "Step 492: train loss: 4.5692057609558105\n",
      "Step 492: val loss: 4.478211879730225\n",
      "Step 493: train loss: 4.559985160827637\n",
      "Step 493: val loss: 4.469261169433594\n",
      "Step 494: train loss: 4.550790309906006\n",
      "Step 494: val loss: 4.4603352546691895\n",
      "Step 495: train loss: 4.54162073135376\n",
      "Step 495: val loss: 4.4514336585998535\n",
      "Step 496: train loss: 4.53247594833374\n",
      "Step 496: val loss: 4.442554473876953\n",
      "Step 497: train loss: 4.5233564376831055\n",
      "Step 497: val loss: 4.433701038360596\n",
      "Step 498: train loss: 4.5142621994018555\n",
      "Step 498: val loss: 4.424871444702148\n",
      "Step 499: train loss: 4.505192279815674\n",
      "Step 499: val loss: 4.416064262390137\n",
      "Step 500: train loss: 4.496147632598877\n",
      "Step 500: val loss: 4.407281398773193\n",
      "Step 501: train loss: 4.487127304077148\n",
      "Step 501: val loss: 4.398521900177002\n",
      "Step 502: train loss: 4.478131294250488\n",
      "Step 502: val loss: 4.389787197113037\n",
      "Step 503: train loss: 4.469160556793213\n",
      "Step 503: val loss: 4.38107442855835\n",
      "Step 504: train loss: 4.460213661193848\n",
      "Step 504: val loss: 4.3723859786987305\n",
      "Step 505: train loss: 4.451291561126709\n",
      "Step 505: val loss: 4.3637189865112305\n",
      "Step 506: train loss: 4.442392349243164\n",
      "Step 506: val loss: 4.355076789855957\n",
      "Step 507: train loss: 4.433518409729004\n",
      "Step 507: val loss: 4.346456050872803\n",
      "Step 508: train loss: 4.424667835235596\n",
      "Step 508: val loss: 4.3378586769104\n",
      "Step 509: train loss: 4.415841579437256\n",
      "Step 509: val loss: 4.329285144805908\n",
      "Step 510: train loss: 4.407038688659668\n",
      "Step 510: val loss: 4.320733070373535\n",
      "Step 511: train loss: 4.398259162902832\n",
      "Step 511: val loss: 4.3122053146362305\n",
      "Step 512: train loss: 4.389504432678223\n",
      "Step 512: val loss: 4.303698539733887\n",
      "Step 513: train loss: 4.380773544311523\n",
      "Step 513: val loss: 4.295214653015137\n",
      "Step 514: train loss: 4.37206506729126\n",
      "Step 514: val loss: 4.286754608154297\n",
      "Step 515: train loss: 4.363379955291748\n",
      "Step 515: val loss: 4.27831506729126\n",
      "Step 516: train loss: 4.354718208312988\n",
      "Step 516: val loss: 4.269897937774658\n",
      "Step 517: train loss: 4.346080780029297\n",
      "Step 517: val loss: 4.26150369644165\n",
      "Step 518: train loss: 4.337464809417725\n",
      "Step 518: val loss: 4.253130912780762\n",
      "Step 519: train loss: 4.3288726806640625\n",
      "Step 519: val loss: 4.244780540466309\n",
      "Step 520: train loss: 4.320302963256836\n",
      "Step 520: val loss: 4.236451625823975\n",
      "Step 521: train loss: 4.311756610870361\n",
      "Step 521: val loss: 4.228145122528076\n",
      "Step 522: train loss: 4.303232192993164\n",
      "Step 522: val loss: 4.219859600067139\n",
      "Step 523: train loss: 4.2947306632995605\n",
      "Step 523: val loss: 4.211594581604004\n",
      "Step 524: train loss: 4.286251544952393\n",
      "Step 524: val loss: 4.203353404998779\n",
      "Step 525: train loss: 4.277793884277344\n",
      "Step 525: val loss: 4.195131778717041\n",
      "Step 526: train loss: 4.269360065460205\n",
      "Step 526: val loss: 4.186932563781738\n",
      "Step 527: train loss: 4.260948181152344\n",
      "Step 527: val loss: 4.178753852844238\n",
      "Step 528: train loss: 4.25255823135376\n",
      "Step 528: val loss: 4.170597076416016\n",
      "Step 529: train loss: 4.244190692901611\n",
      "Step 529: val loss: 4.162460803985596\n",
      "Step 530: train loss: 4.23584508895874\n",
      "Step 530: val loss: 4.154346466064453\n",
      "Step 531: train loss: 4.2275214195251465\n",
      "Step 531: val loss: 4.146252632141113\n",
      "Step 532: train loss: 4.21921968460083\n",
      "Step 532: val loss: 4.138179302215576\n",
      "Step 533: train loss: 4.210939407348633\n",
      "Step 533: val loss: 4.130126953125\n",
      "Step 534: train loss: 4.202681064605713\n",
      "Step 534: val loss: 4.122095584869385\n",
      "Step 535: train loss: 4.194444179534912\n",
      "Step 535: val loss: 4.114085674285889\n",
      "Step 536: train loss: 4.186229228973389\n",
      "Step 536: val loss: 4.1060943603515625\n",
      "Step 537: train loss: 4.178035259246826\n",
      "Step 537: val loss: 4.0981245040893555\n",
      "Step 538: train loss: 4.169862747192383\n",
      "Step 538: val loss: 4.090175151824951\n",
      "Step 539: train loss: 4.1617112159729\n",
      "Step 539: val loss: 4.08224630355835\n",
      "Step 540: train loss: 4.1535820960998535\n",
      "Step 540: val loss: 4.074337005615234\n",
      "Step 541: train loss: 4.145472526550293\n",
      "Step 541: val loss: 4.066448211669922\n",
      "Step 542: train loss: 4.13738489151001\n",
      "Step 542: val loss: 4.058579444885254\n",
      "Step 543: train loss: 4.129317760467529\n",
      "Step 543: val loss: 4.050731182098389\n",
      "Step 544: train loss: 4.12127161026001\n",
      "Step 544: val loss: 4.04290246963501\n",
      "Step 545: train loss: 4.113245964050293\n",
      "Step 545: val loss: 4.035093307495117\n",
      "Step 546: train loss: 4.105241775512695\n",
      "Step 546: val loss: 4.027304172515869\n",
      "Step 547: train loss: 4.097257614135742\n",
      "Step 547: val loss: 4.019535064697266\n",
      "Step 548: train loss: 4.08929443359375\n",
      "Step 548: val loss: 4.011785507202148\n",
      "Step 549: train loss: 4.081352233886719\n",
      "Step 549: val loss: 4.004055500030518\n",
      "Step 550: train loss: 4.073429107666016\n",
      "Step 550: val loss: 3.996344804763794\n",
      "Step 551: train loss: 4.065526962280273\n",
      "Step 551: val loss: 3.9886531829833984\n",
      "Step 552: train loss: 4.057644844055176\n",
      "Step 552: val loss: 3.9809823036193848\n",
      "Step 553: train loss: 4.049783229827881\n",
      "Step 553: val loss: 3.973329782485962\n",
      "Step 554: train loss: 4.041942119598389\n",
      "Step 554: val loss: 3.9656965732574463\n",
      "Step 555: train loss: 4.034120559692383\n",
      "Step 555: val loss: 3.958082437515259\n",
      "Step 556: train loss: 4.02631950378418\n",
      "Step 556: val loss: 3.9504871368408203\n",
      "Step 557: train loss: 4.0185370445251465\n",
      "Step 557: val loss: 3.9429116249084473\n",
      "Step 558: train loss: 4.010775566101074\n",
      "Step 558: val loss: 3.935354709625244\n",
      "Step 559: train loss: 4.003033638000488\n",
      "Step 559: val loss: 3.927816390991211\n",
      "Step 560: train loss: 3.9953110218048096\n",
      "Step 560: val loss: 3.920297145843506\n",
      "Step 561: train loss: 3.9876081943511963\n",
      "Step 561: val loss: 3.9127964973449707\n",
      "Step 562: train loss: 3.9799251556396484\n",
      "Step 562: val loss: 3.9053142070770264\n",
      "Step 563: train loss: 3.9722609519958496\n",
      "Step 563: val loss: 3.897850751876831\n",
      "Step 564: train loss: 3.9646167755126953\n",
      "Step 564: val loss: 3.8904061317443848\n",
      "Step 565: train loss: 3.956991672515869\n",
      "Step 565: val loss: 3.8829798698425293\n",
      "Step 566: train loss: 3.9493861198425293\n",
      "Step 566: val loss: 3.8755719661712646\n",
      "Step 567: train loss: 3.9417994022369385\n",
      "Step 567: val loss: 3.868182420730591\n",
      "Step 568: train loss: 3.9342315196990967\n",
      "Step 568: val loss: 3.8608105182647705\n",
      "Step 569: train loss: 3.926683187484741\n",
      "Step 569: val loss: 3.853457450866699\n",
      "Step 570: train loss: 3.9191532135009766\n",
      "Step 570: val loss: 3.846122980117798\n",
      "Step 571: train loss: 3.9116427898406982\n",
      "Step 571: val loss: 3.838805913925171\n",
      "Step 572: train loss: 3.90415096282959\n",
      "Step 572: val loss: 3.8315067291259766\n",
      "Step 573: train loss: 3.8966777324676514\n",
      "Step 573: val loss: 3.824225664138794\n",
      "Step 574: train loss: 3.88922381401062\n",
      "Step 574: val loss: 3.816962957382202\n",
      "Step 575: train loss: 3.8817880153656006\n",
      "Step 575: val loss: 3.8097174167633057\n",
      "Step 576: train loss: 3.8743703365325928\n",
      "Step 576: val loss: 3.802489757537842\n",
      "Step 577: train loss: 3.8669722080230713\n",
      "Step 577: val loss: 3.7952797412872314\n",
      "Step 578: train loss: 3.8595917224884033\n",
      "Step 578: val loss: 3.788087844848633\n",
      "Step 579: train loss: 3.8522298336029053\n",
      "Step 579: val loss: 3.7809133529663086\n",
      "Step 580: train loss: 3.844886302947998\n",
      "Step 580: val loss: 3.7737560272216797\n",
      "Step 581: train loss: 3.8375611305236816\n",
      "Step 581: val loss: 3.766616106033325\n",
      "Step 582: train loss: 3.8302536010742188\n",
      "Step 582: val loss: 3.759493827819824\n",
      "Step 583: train loss: 3.822965145111084\n",
      "Step 583: val loss: 3.7523891925811768\n",
      "Step 584: train loss: 3.8156938552856445\n",
      "Step 584: val loss: 3.7453014850616455\n",
      "Step 585: train loss: 3.808441162109375\n",
      "Step 585: val loss: 3.7382309436798096\n",
      "Step 586: train loss: 3.80120587348938\n",
      "Step 586: val loss: 3.7311770915985107\n",
      "Step 587: train loss: 3.7939884662628174\n",
      "Step 587: val loss: 3.7241406440734863\n",
      "Step 588: train loss: 3.7867887020111084\n",
      "Step 588: val loss: 3.717121124267578\n",
      "Step 589: train loss: 3.779606819152832\n",
      "Step 589: val loss: 3.7101192474365234\n",
      "Step 590: train loss: 3.7724435329437256\n",
      "Step 590: val loss: 3.7031333446502686\n",
      "Step 591: train loss: 3.765296697616577\n",
      "Step 591: val loss: 3.696165084838867\n",
      "Step 592: train loss: 3.7581677436828613\n",
      "Step 592: val loss: 3.6892123222351074\n",
      "Step 593: train loss: 3.751055955886841\n",
      "Step 593: val loss: 3.682277202606201\n",
      "Step 594: train loss: 3.743962049484253\n",
      "Step 594: val loss: 3.675358772277832\n",
      "Step 595: train loss: 3.736884832382202\n",
      "Step 595: val loss: 3.668456792831421\n",
      "Step 596: train loss: 3.729825973510742\n",
      "Step 596: val loss: 3.661571502685547\n",
      "Step 597: train loss: 3.7227840423583984\n",
      "Step 597: val loss: 3.654703140258789\n",
      "Step 598: train loss: 3.71575927734375\n",
      "Step 598: val loss: 3.647850513458252\n",
      "Step 599: train loss: 3.7087509632110596\n",
      "Step 599: val loss: 3.6410140991210938\n",
      "Step 600: train loss: 3.7017605304718018\n",
      "Step 600: val loss: 3.6341943740844727\n",
      "Step 601: train loss: 3.694786787033081\n",
      "Step 601: val loss: 3.6273913383483887\n",
      "Step 602: train loss: 3.6878297328948975\n",
      "Step 602: val loss: 3.6206040382385254\n",
      "Step 603: train loss: 3.68088960647583\n",
      "Step 603: val loss: 3.613832950592041\n",
      "Step 604: train loss: 3.673966646194458\n",
      "Step 604: val loss: 3.6070783138275146\n",
      "Step 605: train loss: 3.667060375213623\n",
      "Step 605: val loss: 3.600339412689209\n",
      "Step 606: train loss: 3.660170793533325\n",
      "Step 606: val loss: 3.593616247177124\n",
      "Step 607: train loss: 3.6532976627349854\n",
      "Step 607: val loss: 3.586909294128418\n",
      "Step 608: train loss: 3.6464412212371826\n",
      "Step 608: val loss: 3.5802180767059326\n",
      "Step 609: train loss: 3.639601230621338\n",
      "Step 609: val loss: 3.573542833328247\n",
      "Step 610: train loss: 3.6327781677246094\n",
      "Step 610: val loss: 3.5668833255767822\n",
      "Step 611: train loss: 3.6259710788726807\n",
      "Step 611: val loss: 3.560239791870117\n",
      "Step 612: train loss: 3.619180679321289\n",
      "Step 612: val loss: 3.5536117553710938\n",
      "Step 613: train loss: 3.6124067306518555\n",
      "Step 613: val loss: 3.546999216079712\n",
      "Step 614: train loss: 3.605648994445801\n",
      "Step 614: val loss: 3.540402889251709\n",
      "Step 615: train loss: 3.598907232284546\n",
      "Step 615: val loss: 3.5338213443756104\n",
      "Step 616: train loss: 3.5921812057495117\n",
      "Step 616: val loss: 3.5272552967071533\n",
      "Step 617: train loss: 3.5854716300964355\n",
      "Step 617: val loss: 3.520704746246338\n",
      "Step 618: train loss: 3.5787782669067383\n",
      "Step 618: val loss: 3.514169454574585\n",
      "Step 619: train loss: 3.5721006393432617\n",
      "Step 619: val loss: 3.5076498985290527\n",
      "Step 620: train loss: 3.565439462661743\n",
      "Step 620: val loss: 3.501145124435425\n",
      "Step 621: train loss: 3.558793783187866\n",
      "Step 621: val loss: 3.4946560859680176\n",
      "Step 622: train loss: 3.552164077758789\n",
      "Step 622: val loss: 3.4881815910339355\n",
      "Step 623: train loss: 3.5455496311187744\n",
      "Step 623: val loss: 3.481722354888916\n",
      "Step 624: train loss: 3.5389511585235596\n",
      "Step 624: val loss: 3.475278615951538\n",
      "Step 625: train loss: 3.5323686599731445\n",
      "Step 625: val loss: 3.4688496589660645\n",
      "Step 626: train loss: 3.525801658630371\n",
      "Step 626: val loss: 3.462435722351074\n",
      "Step 627: train loss: 3.5192503929138184\n",
      "Step 627: val loss: 3.456036329269409\n",
      "Step 628: train loss: 3.51271390914917\n",
      "Step 628: val loss: 3.4496521949768066\n",
      "Step 629: train loss: 3.5061938762664795\n",
      "Step 629: val loss: 3.4432826042175293\n",
      "Step 630: train loss: 3.4996888637542725\n",
      "Step 630: val loss: 3.4369282722473145\n",
      "Step 631: train loss: 3.493199348449707\n",
      "Step 631: val loss: 3.4305880069732666\n",
      "Step 632: train loss: 3.486724853515625\n",
      "Step 632: val loss: 3.4242632389068604\n",
      "Step 633: train loss: 3.4802658557891846\n",
      "Step 633: val loss: 3.417952060699463\n",
      "Step 634: train loss: 3.4738218784332275\n",
      "Step 634: val loss: 3.411656141281128\n",
      "Step 635: train loss: 3.467393159866333\n",
      "Step 635: val loss: 3.405374765396118\n",
      "Step 636: train loss: 3.460979461669922\n",
      "Step 636: val loss: 3.3991079330444336\n",
      "Step 637: train loss: 3.4545810222625732\n",
      "Step 637: val loss: 3.392855167388916\n",
      "Step 638: train loss: 3.448197841644287\n",
      "Step 638: val loss: 3.3866169452667236\n",
      "Step 639: train loss: 3.4418294429779053\n",
      "Step 639: val loss: 3.3803932666778564\n",
      "Step 640: train loss: 3.4354755878448486\n",
      "Step 640: val loss: 3.3741841316223145\n",
      "Step 641: train loss: 3.4291372299194336\n",
      "Step 641: val loss: 3.3679888248443604\n",
      "Step 642: train loss: 3.4228131771087646\n",
      "Step 642: val loss: 3.3618075847625732\n",
      "Step 643: train loss: 3.416504144668579\n",
      "Step 643: val loss: 3.355640411376953\n",
      "Step 644: train loss: 3.4102094173431396\n",
      "Step 644: val loss: 3.3494873046875\n",
      "Step 645: train loss: 3.4039297103881836\n",
      "Step 645: val loss: 3.3433494567871094\n",
      "Step 646: train loss: 3.3976645469665527\n",
      "Step 646: val loss: 3.337224006652832\n",
      "Step 647: train loss: 3.3914146423339844\n",
      "Step 647: val loss: 3.331113576889038\n",
      "Step 648: train loss: 3.385178804397583\n",
      "Step 648: val loss: 3.3250162601470947\n",
      "Step 649: train loss: 3.3789567947387695\n",
      "Step 649: val loss: 3.3189339637756348\n",
      "Step 650: train loss: 3.3727500438690186\n",
      "Step 650: val loss: 3.3128647804260254\n",
      "Step 651: train loss: 3.3665571212768555\n",
      "Step 651: val loss: 3.306809663772583\n",
      "Step 652: train loss: 3.3603787422180176\n",
      "Step 652: val loss: 3.3007678985595703\n",
      "Step 653: train loss: 3.354214906692505\n",
      "Step 653: val loss: 3.2947404384613037\n",
      "Step 654: train loss: 3.34806489944458\n",
      "Step 654: val loss: 3.2887260913848877\n",
      "Step 655: train loss: 3.341928482055664\n",
      "Step 655: val loss: 3.2827255725860596\n",
      "Step 656: train loss: 3.3358078002929688\n",
      "Step 656: val loss: 3.276737928390503\n",
      "Step 657: train loss: 3.329699993133545\n",
      "Step 657: val loss: 3.2707650661468506\n",
      "Step 658: train loss: 3.3236069679260254\n",
      "Step 658: val loss: 3.264805316925049\n",
      "Step 659: train loss: 3.317528009414673\n",
      "Step 659: val loss: 3.258859157562256\n",
      "Step 660: train loss: 3.3114631175994873\n",
      "Step 660: val loss: 3.2529258728027344\n",
      "Step 661: train loss: 3.3054115772247314\n",
      "Step 661: val loss: 3.247006893157959\n",
      "Step 662: train loss: 3.29937481880188\n",
      "Step 662: val loss: 3.241100549697876\n",
      "Step 663: train loss: 3.293351173400879\n",
      "Step 663: val loss: 3.2352073192596436\n",
      "Step 664: train loss: 3.287341356277466\n",
      "Step 664: val loss: 3.229327440261841\n",
      "Step 665: train loss: 3.2813456058502197\n",
      "Step 665: val loss: 3.223461151123047\n",
      "Step 666: train loss: 3.2753632068634033\n",
      "Step 666: val loss: 3.2176079750061035\n",
      "Step 667: train loss: 3.269395351409912\n",
      "Step 667: val loss: 3.21176815032959\n",
      "Step 668: train loss: 3.263439893722534\n",
      "Step 668: val loss: 3.2059407234191895\n",
      "Step 669: train loss: 3.2574989795684814\n",
      "Step 669: val loss: 3.2001264095306396\n",
      "Step 670: train loss: 3.2515714168548584\n",
      "Step 670: val loss: 3.1943256855010986\n",
      "Step 671: train loss: 3.245657444000244\n",
      "Step 671: val loss: 3.18853759765625\n",
      "Step 672: train loss: 3.2397568225860596\n",
      "Step 672: val loss: 3.182762861251831\n",
      "Step 673: train loss: 3.2338693141937256\n",
      "Step 673: val loss: 3.1770007610321045\n",
      "Step 674: train loss: 3.2279956340789795\n",
      "Step 674: val loss: 3.171251058578491\n",
      "Step 675: train loss: 3.222135066986084\n",
      "Step 675: val loss: 3.1655149459838867\n",
      "Step 676: train loss: 3.2162883281707764\n",
      "Step 676: val loss: 3.1597912311553955\n",
      "Step 677: train loss: 3.2104547023773193\n",
      "Step 677: val loss: 3.1540801525115967\n",
      "Step 678: train loss: 3.204633951187134\n",
      "Step 678: val loss: 3.1483819484710693\n",
      "Step 679: train loss: 3.1988260746002197\n",
      "Step 679: val loss: 3.1426966190338135\n",
      "Step 680: train loss: 3.1930322647094727\n",
      "Step 680: val loss: 3.1370232105255127\n",
      "Step 681: train loss: 3.187251091003418\n",
      "Step 681: val loss: 3.1313626766204834\n",
      "Step 682: train loss: 3.1814823150634766\n",
      "Step 682: val loss: 3.1257150173187256\n",
      "Step 683: train loss: 3.175727605819702\n",
      "Step 683: val loss: 3.12007999420166\n",
      "Step 684: train loss: 3.169985771179199\n",
      "Step 684: val loss: 3.114457130432129\n",
      "Step 685: train loss: 3.1642565727233887\n",
      "Step 685: val loss: 3.108846664428711\n",
      "Step 686: train loss: 3.1585397720336914\n",
      "Step 686: val loss: 3.1032488346099854\n",
      "Step 687: train loss: 3.152836561203003\n",
      "Step 687: val loss: 3.097663164138794\n",
      "Step 688: train loss: 3.147145986557007\n",
      "Step 688: val loss: 3.092089891433716\n",
      "Step 689: train loss: 3.1414682865142822\n",
      "Step 689: val loss: 3.086528778076172\n",
      "Step 690: train loss: 3.135803461074829\n",
      "Step 690: val loss: 3.0809803009033203\n",
      "Step 691: train loss: 3.1301512718200684\n",
      "Step 691: val loss: 3.0754432678222656\n",
      "Step 692: train loss: 3.12451171875\n",
      "Step 692: val loss: 3.0699193477630615\n",
      "Step 693: train loss: 3.118884563446045\n",
      "Step 693: val loss: 3.0644068717956543\n",
      "Step 694: train loss: 3.1132700443267822\n",
      "Step 694: val loss: 3.0589065551757812\n",
      "Step 695: train loss: 3.107668161392212\n",
      "Step 695: val loss: 3.0534181594848633\n",
      "Step 696: train loss: 3.102078437805176\n",
      "Step 696: val loss: 3.047942638397217\n",
      "Step 697: train loss: 3.0965025424957275\n",
      "Step 697: val loss: 3.0424790382385254\n",
      "Step 698: train loss: 3.090937852859497\n",
      "Step 698: val loss: 3.037026882171631\n",
      "Step 699: train loss: 3.085386276245117\n",
      "Step 699: val loss: 3.0315868854522705\n",
      "Step 700: train loss: 3.0798468589782715\n",
      "Step 700: val loss: 3.0261590480804443\n",
      "Step 701: train loss: 3.074319362640381\n",
      "Step 701: val loss: 3.020742416381836\n",
      "Step 702: train loss: 3.0688047409057617\n",
      "Step 702: val loss: 3.01533842086792\n",
      "Step 703: train loss: 3.0633018016815186\n",
      "Step 703: val loss: 3.0099456310272217\n",
      "Step 704: train loss: 3.0578114986419678\n",
      "Step 704: val loss: 3.0045645236968994\n",
      "Step 705: train loss: 3.0523335933685303\n",
      "Step 705: val loss: 2.9991953372955322\n",
      "Step 706: train loss: 3.046867609024048\n",
      "Step 706: val loss: 2.993837833404541\n",
      "Step 707: train loss: 3.0414130687713623\n",
      "Step 707: val loss: 2.9884917736053467\n",
      "Step 708: train loss: 3.0359716415405273\n",
      "Step 708: val loss: 2.9831576347351074\n",
      "Step 709: train loss: 3.0305416584014893\n",
      "Step 709: val loss: 2.977835178375244\n",
      "Step 710: train loss: 3.0251240730285645\n",
      "Step 710: val loss: 2.972524404525757\n",
      "Step 711: train loss: 3.0197181701660156\n",
      "Step 711: val loss: 2.967224597930908\n",
      "Step 712: train loss: 3.01432466506958\n",
      "Step 712: val loss: 2.9619369506835938\n",
      "Step 713: train loss: 3.0089423656463623\n",
      "Step 713: val loss: 2.956660509109497\n",
      "Step 714: train loss: 3.003572463989258\n",
      "Step 714: val loss: 2.951395034790039\n",
      "Step 715: train loss: 2.9982144832611084\n",
      "Step 715: val loss: 2.9461421966552734\n",
      "Step 716: train loss: 2.992868185043335\n",
      "Step 716: val loss: 2.940899133682251\n",
      "Step 717: train loss: 2.9875330924987793\n",
      "Step 717: val loss: 2.9356677532196045\n",
      "Step 718: train loss: 2.9822099208831787\n",
      "Step 718: val loss: 2.930448293685913\n",
      "Step 719: train loss: 2.976898670196533\n",
      "Step 719: val loss: 2.925239086151123\n",
      "Step 720: train loss: 2.9715991020202637\n",
      "Step 720: val loss: 2.920042037963867\n",
      "Step 721: train loss: 2.966310739517212\n",
      "Step 721: val loss: 2.914855480194092\n",
      "Step 722: train loss: 2.9610345363616943\n",
      "Step 722: val loss: 2.9096810817718506\n",
      "Step 723: train loss: 2.9557697772979736\n",
      "Step 723: val loss: 2.904517650604248\n",
      "Step 724: train loss: 2.950516939163208\n",
      "Step 724: val loss: 2.8993639945983887\n",
      "Step 725: train loss: 2.945274829864502\n",
      "Step 725: val loss: 2.8942227363586426\n",
      "Step 726: train loss: 2.9400441646575928\n",
      "Step 726: val loss: 2.889091730117798\n",
      "Step 727: train loss: 2.9348254203796387\n",
      "Step 727: val loss: 2.88397216796875\n",
      "Step 728: train loss: 2.9296181201934814\n",
      "Step 728: val loss: 2.87886381149292\n",
      "Step 729: train loss: 2.924422025680542\n",
      "Step 729: val loss: 2.8737664222717285\n",
      "Step 730: train loss: 2.9192371368408203\n",
      "Step 730: val loss: 2.8686790466308594\n",
      "Step 731: train loss: 2.9140634536743164\n",
      "Step 731: val loss: 2.8636035919189453\n",
      "Step 732: train loss: 2.9089014530181885\n",
      "Step 732: val loss: 2.8585381507873535\n",
      "Step 733: train loss: 2.903750419616699\n",
      "Step 733: val loss: 2.8534841537475586\n",
      "Step 734: train loss: 2.8986105918884277\n",
      "Step 734: val loss: 2.848440408706665\n",
      "Step 735: train loss: 2.893481731414795\n",
      "Step 735: val loss: 2.84340763092041\n",
      "Step 736: train loss: 2.888364315032959\n",
      "Step 736: val loss: 2.838386058807373\n",
      "Step 737: train loss: 2.8832578659057617\n",
      "Step 737: val loss: 2.833374500274658\n",
      "Step 738: train loss: 2.8781626224517822\n",
      "Step 738: val loss: 2.8283743858337402\n",
      "Step 739: train loss: 2.8730785846710205\n",
      "Step 739: val loss: 2.8233842849731445\n",
      "Step 740: train loss: 2.8680050373077393\n",
      "Step 740: val loss: 2.818404197692871\n",
      "Step 741: train loss: 2.862942695617676\n",
      "Step 741: val loss: 2.8134360313415527\n",
      "Step 742: train loss: 2.857891321182251\n",
      "Step 742: val loss: 2.8084781169891357\n",
      "Step 743: train loss: 2.852851390838623\n",
      "Step 743: val loss: 2.803530216217041\n",
      "Step 744: train loss: 2.8478212356567383\n",
      "Step 744: val loss: 2.7985928058624268\n",
      "Step 745: train loss: 2.8428025245666504\n",
      "Step 745: val loss: 2.7936670780181885\n",
      "Step 746: train loss: 2.8377950191497803\n",
      "Step 746: val loss: 2.788749933242798\n",
      "Step 747: train loss: 2.8327975273132324\n",
      "Step 747: val loss: 2.783844232559204\n",
      "Step 748: train loss: 2.8278112411499023\n",
      "Step 748: val loss: 2.77894926071167\n",
      "Step 749: train loss: 2.82283616065979\n",
      "Step 749: val loss: 2.7740638256073\n",
      "Step 750: train loss: 2.81787109375\n",
      "Step 750: val loss: 2.7691893577575684\n",
      "Step 751: train loss: 2.812917470932007\n",
      "Step 751: val loss: 2.764324426651001\n",
      "Step 752: train loss: 2.807973623275757\n",
      "Step 752: val loss: 2.7594707012176514\n",
      "Step 753: train loss: 2.8030407428741455\n",
      "Step 753: val loss: 2.754626512527466\n",
      "Step 754: train loss: 2.7981183528900146\n",
      "Step 754: val loss: 2.749793291091919\n",
      "Step 755: train loss: 2.7932064533233643\n",
      "Step 755: val loss: 2.7449698448181152\n",
      "Step 756: train loss: 2.7883059978485107\n",
      "Step 756: val loss: 2.7401561737060547\n",
      "Step 757: train loss: 2.7834150791168213\n",
      "Step 757: val loss: 2.7353532314300537\n",
      "Step 758: train loss: 2.7785348892211914\n",
      "Step 758: val loss: 2.730560064315796\n",
      "Step 759: train loss: 2.773664951324463\n",
      "Step 759: val loss: 2.7257769107818604\n",
      "Step 760: train loss: 2.768805980682373\n",
      "Step 760: val loss: 2.721003770828247\n",
      "Step 761: train loss: 2.7639565467834473\n",
      "Step 761: val loss: 2.716240644454956\n",
      "Step 762: train loss: 2.75911808013916\n",
      "Step 762: val loss: 2.7114875316619873\n",
      "Step 763: train loss: 2.7542901039123535\n",
      "Step 763: val loss: 2.706745147705078\n",
      "Step 764: train loss: 2.74947190284729\n",
      "Step 764: val loss: 2.702011823654175\n",
      "Step 765: train loss: 2.744663953781128\n",
      "Step 765: val loss: 2.697288751602173\n",
      "Step 766: train loss: 2.7398669719696045\n",
      "Step 766: val loss: 2.692575454711914\n",
      "Step 767: train loss: 2.735079526901245\n",
      "Step 767: val loss: 2.6878721714019775\n",
      "Step 768: train loss: 2.7303030490875244\n",
      "Step 768: val loss: 2.683178663253784\n",
      "Step 769: train loss: 2.7255358695983887\n",
      "Step 769: val loss: 2.678495407104492\n",
      "Step 770: train loss: 2.7207791805267334\n",
      "Step 770: val loss: 2.673821210861206\n",
      "Step 771: train loss: 2.7160327434539795\n",
      "Step 771: val loss: 2.669157028198242\n",
      "Step 772: train loss: 2.7112960815429688\n",
      "Step 772: val loss: 2.6645023822784424\n",
      "Step 773: train loss: 2.7065701484680176\n",
      "Step 773: val loss: 2.659857988357544\n",
      "Step 774: train loss: 2.7018532752990723\n",
      "Step 774: val loss: 2.6552231311798096\n",
      "Step 775: train loss: 2.6971466541290283\n",
      "Step 775: val loss: 2.65059757232666\n",
      "Step 776: train loss: 2.692450761795044\n",
      "Step 776: val loss: 2.645982265472412\n",
      "Step 777: train loss: 2.6877636909484863\n",
      "Step 777: val loss: 2.641375780105591\n",
      "Step 778: train loss: 2.6830873489379883\n",
      "Step 778: val loss: 2.636779308319092\n",
      "Step 779: train loss: 2.6784205436706543\n",
      "Step 779: val loss: 2.6321921348571777\n",
      "Step 780: train loss: 2.6737639904022217\n",
      "Step 780: val loss: 2.627614974975586\n",
      "Step 781: train loss: 2.669116735458374\n",
      "Step 781: val loss: 2.623046875\n",
      "Step 782: train loss: 2.6644797325134277\n",
      "Step 782: val loss: 2.6184885501861572\n",
      "Step 783: train loss: 2.6598517894744873\n",
      "Step 783: val loss: 2.6139395236968994\n",
      "Step 784: train loss: 2.6552345752716064\n",
      "Step 784: val loss: 2.6093997955322266\n",
      "Step 785: train loss: 2.6506261825561523\n",
      "Step 785: val loss: 2.6048696041107178\n",
      "Step 786: train loss: 2.6460280418395996\n",
      "Step 786: val loss: 2.600348949432373\n",
      "Step 787: train loss: 2.641439914703369\n",
      "Step 787: val loss: 2.5958375930786133\n",
      "Step 788: train loss: 2.6368608474731445\n",
      "Step 788: val loss: 2.5913355350494385\n",
      "Step 789: train loss: 2.632291793823242\n",
      "Step 789: val loss: 2.5868425369262695\n",
      "Step 790: train loss: 2.6277318000793457\n",
      "Step 790: val loss: 2.5823593139648438\n",
      "Step 791: train loss: 2.6231815814971924\n",
      "Step 791: val loss: 2.577885150909424\n",
      "Step 792: train loss: 2.6186413764953613\n",
      "Step 792: val loss: 2.5734200477600098\n",
      "Step 793: train loss: 2.6141104698181152\n",
      "Step 793: val loss: 2.5689640045166016\n",
      "Step 794: train loss: 2.609588623046875\n",
      "Step 794: val loss: 2.5645177364349365\n",
      "Step 795: train loss: 2.605076551437378\n",
      "Step 795: val loss: 2.5600805282592773\n",
      "Step 796: train loss: 2.6005747318267822\n",
      "Step 796: val loss: 2.555651903152466\n",
      "Step 797: train loss: 2.596081256866455\n",
      "Step 797: val loss: 2.5512328147888184\n",
      "Step 798: train loss: 2.591597080230713\n",
      "Step 798: val loss: 2.5468227863311768\n",
      "Step 799: train loss: 2.587122917175293\n",
      "Step 799: val loss: 2.54242205619812\n",
      "Step 800: train loss: 2.582658290863037\n",
      "Step 800: val loss: 2.5380303859710693\n",
      "Step 801: train loss: 2.578202724456787\n",
      "Step 801: val loss: 2.533647298812866\n",
      "Step 802: train loss: 2.573756456375122\n",
      "Step 802: val loss: 2.529273509979248\n",
      "Step 803: train loss: 2.569319248199463\n",
      "Step 803: val loss: 2.524909019470215\n",
      "Step 804: train loss: 2.5648910999298096\n",
      "Step 804: val loss: 2.520552396774292\n",
      "Step 805: train loss: 2.5604724884033203\n",
      "Step 805: val loss: 2.5162060260772705\n",
      "Step 806: train loss: 2.556063413619995\n",
      "Step 806: val loss: 2.5118680000305176\n",
      "Step 807: train loss: 2.551663398742676\n",
      "Step 807: val loss: 2.5075387954711914\n",
      "Step 808: train loss: 2.5472726821899414\n",
      "Step 808: val loss: 2.503218412399292\n",
      "Step 809: train loss: 2.5428905487060547\n",
      "Step 809: val loss: 2.4989073276519775\n",
      "Step 810: train loss: 2.5385184288024902\n",
      "Step 810: val loss: 2.4946048259735107\n",
      "Step 811: train loss: 2.5341546535491943\n",
      "Step 811: val loss: 2.4903104305267334\n",
      "Step 812: train loss: 2.529799461364746\n",
      "Step 812: val loss: 2.48602557182312\n",
      "Step 813: train loss: 2.525454044342041\n",
      "Step 813: val loss: 2.4817492961883545\n",
      "Step 814: train loss: 2.521117687225342\n",
      "Step 814: val loss: 2.4774816036224365\n",
      "Step 815: train loss: 2.5167899131774902\n",
      "Step 815: val loss: 2.4732227325439453\n",
      "Step 816: train loss: 2.5124711990356445\n",
      "Step 816: val loss: 2.4689724445343018\n",
      "Step 817: train loss: 2.5081613063812256\n",
      "Step 817: val loss: 2.464730978012085\n",
      "Step 818: train loss: 2.5038609504699707\n",
      "Step 818: val loss: 2.460498809814453\n",
      "Step 819: train loss: 2.4995691776275635\n",
      "Step 819: val loss: 2.4562745094299316\n",
      "Step 820: train loss: 2.4952869415283203\n",
      "Step 820: val loss: 2.452059268951416\n",
      "Step 821: train loss: 2.4910130500793457\n",
      "Step 821: val loss: 2.44785213470459\n",
      "Step 822: train loss: 2.486747980117798\n",
      "Step 822: val loss: 2.4436540603637695\n",
      "Step 823: train loss: 2.4824917316436768\n",
      "Step 823: val loss: 2.439464569091797\n",
      "Step 824: train loss: 2.4782443046569824\n",
      "Step 824: val loss: 2.4352834224700928\n",
      "Step 825: train loss: 2.474006175994873\n",
      "Step 825: val loss: 2.4311106204986572\n",
      "Step 826: train loss: 2.469775915145874\n",
      "Step 826: val loss: 2.426945686340332\n",
      "Step 827: train loss: 2.4655539989471436\n",
      "Step 827: val loss: 2.4227898120880127\n",
      "Step 828: train loss: 2.461341619491577\n",
      "Step 828: val loss: 2.418642282485962\n",
      "Step 829: train loss: 2.457137107849121\n",
      "Step 829: val loss: 2.4145030975341797\n",
      "Step 830: train loss: 2.45294189453125\n",
      "Step 830: val loss: 2.410372495651245\n",
      "Step 831: train loss: 2.4487552642822266\n",
      "Step 831: val loss: 2.40625\n",
      "Step 832: train loss: 2.444577217102051\n",
      "Step 832: val loss: 2.4021363258361816\n",
      "Step 833: train loss: 2.4404077529907227\n",
      "Step 833: val loss: 2.3980305194854736\n",
      "Step 834: train loss: 2.436246871948242\n",
      "Step 834: val loss: 2.3939332962036133\n",
      "Step 835: train loss: 2.4320948123931885\n",
      "Step 835: val loss: 2.3898446559906006\n",
      "Step 836: train loss: 2.427950620651245\n",
      "Step 836: val loss: 2.3857638835906982\n",
      "Step 837: train loss: 2.423815965652466\n",
      "Step 837: val loss: 2.3816914558410645\n",
      "Step 838: train loss: 2.4196889400482178\n",
      "Step 838: val loss: 2.377626895904541\n",
      "Step 839: train loss: 2.4155702590942383\n",
      "Step 839: val loss: 2.3735713958740234\n",
      "Step 840: train loss: 2.4114603996276855\n",
      "Step 840: val loss: 2.369523048400879\n",
      "Step 841: train loss: 2.4073588848114014\n",
      "Step 841: val loss: 2.365483283996582\n",
      "Step 842: train loss: 2.403265953063965\n",
      "Step 842: val loss: 2.361452102661133\n",
      "Step 843: train loss: 2.3991811275482178\n",
      "Step 843: val loss: 2.357429027557373\n",
      "Step 844: train loss: 2.3951051235198975\n",
      "Step 844: val loss: 2.3534131050109863\n",
      "Step 845: train loss: 2.3910372257232666\n",
      "Step 845: val loss: 2.3494064807891846\n",
      "Step 846: train loss: 2.386977434158325\n",
      "Step 846: val loss: 2.345407009124756\n",
      "Step 847: train loss: 2.3829257488250732\n",
      "Step 847: val loss: 2.341416120529175\n",
      "Step 848: train loss: 2.378883123397827\n",
      "Step 848: val loss: 2.337433099746704\n",
      "Step 849: train loss: 2.3748483657836914\n",
      "Step 849: val loss: 2.333458185195923\n",
      "Step 850: train loss: 2.370821714401245\n",
      "Step 850: val loss: 2.3294906616210938\n",
      "Step 851: train loss: 2.3668031692504883\n",
      "Step 851: val loss: 2.3255324363708496\n",
      "Step 852: train loss: 2.36279296875\n",
      "Step 852: val loss: 2.3215813636779785\n",
      "Step 853: train loss: 2.3587913513183594\n",
      "Step 853: val loss: 2.3176381587982178\n",
      "Step 854: train loss: 2.354797124862671\n",
      "Step 854: val loss: 2.3137025833129883\n",
      "Step 855: train loss: 2.350811243057251\n",
      "Step 855: val loss: 2.3097758293151855\n",
      "Step 856: train loss: 2.3468339443206787\n",
      "Step 856: val loss: 2.305856227874756\n",
      "Step 857: train loss: 2.342864513397217\n",
      "Step 857: val loss: 2.3019447326660156\n",
      "Step 858: train loss: 2.3389031887054443\n",
      "Step 858: val loss: 2.2980411052703857\n",
      "Step 859: train loss: 2.3349497318267822\n",
      "Step 859: val loss: 2.2941455841064453\n",
      "Step 860: train loss: 2.3310046195983887\n",
      "Step 860: val loss: 2.290257215499878\n",
      "Step 861: train loss: 2.3270668983459473\n",
      "Step 861: val loss: 2.286376953125\n",
      "Step 862: train loss: 2.3231377601623535\n",
      "Step 862: val loss: 2.2825043201446533\n",
      "Step 863: train loss: 2.319216251373291\n",
      "Step 863: val loss: 2.278639554977417\n",
      "Step 864: train loss: 2.315302848815918\n",
      "Step 864: val loss: 2.274782419204712\n",
      "Step 865: train loss: 2.3113973140716553\n",
      "Step 865: val loss: 2.270933151245117\n",
      "Step 866: train loss: 2.307499885559082\n",
      "Step 866: val loss: 2.267091751098633\n",
      "Step 867: train loss: 2.30361008644104\n",
      "Step 867: val loss: 2.2632579803466797\n",
      "Step 868: train loss: 2.2997281551361084\n",
      "Step 868: val loss: 2.2594313621520996\n",
      "Step 869: train loss: 2.2958545684814453\n",
      "Step 869: val loss: 2.255612850189209\n",
      "Step 870: train loss: 2.2919883728027344\n",
      "Step 870: val loss: 2.2518017292022705\n",
      "Step 871: train loss: 2.288130044937134\n",
      "Step 871: val loss: 2.247997999191284\n",
      "Step 872: train loss: 2.2842793464660645\n",
      "Step 872: val loss: 2.2442023754119873\n",
      "Step 873: train loss: 2.2804367542266846\n",
      "Step 873: val loss: 2.240414619445801\n",
      "Step 874: train loss: 2.276601791381836\n",
      "Step 874: val loss: 2.23663330078125\n",
      "Step 875: train loss: 2.2727746963500977\n",
      "Step 875: val loss: 2.2328603267669678\n",
      "Step 876: train loss: 2.2689549922943115\n",
      "Step 876: val loss: 2.2290945053100586\n",
      "Step 877: train loss: 2.265143632888794\n",
      "Step 877: val loss: 2.2253360748291016\n",
      "Step 878: train loss: 2.2613394260406494\n",
      "Step 878: val loss: 2.221585750579834\n",
      "Step 879: train loss: 2.2575430870056152\n",
      "Step 879: val loss: 2.2178425788879395\n",
      "Step 880: train loss: 2.253754138946533\n",
      "Step 880: val loss: 2.214106798171997\n",
      "Step 881: train loss: 2.2499735355377197\n",
      "Step 881: val loss: 2.210378646850586\n",
      "Step 882: train loss: 2.246199607849121\n",
      "Step 882: val loss: 2.206657648086548\n",
      "Step 883: train loss: 2.242434024810791\n",
      "Step 883: val loss: 2.202944040298462\n",
      "Step 884: train loss: 2.238676071166992\n",
      "Step 884: val loss: 2.1992380619049072\n",
      "Step 885: train loss: 2.2349252700805664\n",
      "Step 885: val loss: 2.1955392360687256\n",
      "Step 886: train loss: 2.231182098388672\n",
      "Step 886: val loss: 2.191847801208496\n",
      "Step 887: train loss: 2.2274465560913086\n",
      "Step 887: val loss: 2.1881635189056396\n",
      "Step 888: train loss: 2.2237184047698975\n",
      "Step 888: val loss: 2.1844871044158936\n",
      "Step 889: train loss: 2.2199978828430176\n",
      "Step 889: val loss: 2.1808173656463623\n",
      "Step 890: train loss: 2.21628475189209\n",
      "Step 890: val loss: 2.1771552562713623\n",
      "Step 891: train loss: 2.212578773498535\n",
      "Step 891: val loss: 2.1735005378723145\n",
      "Step 892: train loss: 2.208880662918091\n",
      "Step 892: val loss: 2.1698529720306396\n",
      "Step 893: train loss: 2.2051901817321777\n",
      "Step 893: val loss: 2.166212320327759\n",
      "Step 894: train loss: 2.2015066146850586\n",
      "Step 894: val loss: 2.162578821182251\n",
      "Step 895: train loss: 2.1978306770324707\n",
      "Step 895: val loss: 2.1589531898498535\n",
      "Step 896: train loss: 2.194161891937256\n",
      "Step 896: val loss: 2.1553337574005127\n",
      "Step 897: train loss: 2.190500259399414\n",
      "Step 897: val loss: 2.1517224311828613\n",
      "Step 898: train loss: 2.1868462562561035\n",
      "Step 898: val loss: 2.1481175422668457\n",
      "Step 899: train loss: 2.183199644088745\n",
      "Step 899: val loss: 2.144519805908203\n",
      "Step 900: train loss: 2.1795599460601807\n",
      "Step 900: val loss: 2.140929698944092\n",
      "Step 901: train loss: 2.1759278774261475\n",
      "Step 901: val loss: 2.137346029281616\n",
      "Step 902: train loss: 2.1723029613494873\n",
      "Step 902: val loss: 2.133769989013672\n",
      "Step 903: train loss: 2.1686854362487793\n",
      "Step 903: val loss: 2.1302006244659424\n",
      "Step 904: train loss: 2.1650753021240234\n",
      "Step 904: val loss: 2.126638650894165\n",
      "Step 905: train loss: 2.1614718437194824\n",
      "Step 905: val loss: 2.1230833530426025\n",
      "Step 906: train loss: 2.1578762531280518\n",
      "Step 906: val loss: 2.119535446166992\n",
      "Step 907: train loss: 2.154287099838257\n",
      "Step 907: val loss: 2.115994453430176\n",
      "Step 908: train loss: 2.150705099105835\n",
      "Step 908: val loss: 2.112460136413574\n",
      "Step 909: train loss: 2.1471309661865234\n",
      "Step 909: val loss: 2.1089329719543457\n",
      "Step 910: train loss: 2.143563747406006\n",
      "Step 910: val loss: 2.105412483215332\n",
      "Step 911: train loss: 2.140002965927124\n",
      "Step 911: val loss: 2.1018993854522705\n",
      "Step 912: train loss: 2.1364500522613525\n",
      "Step 912: val loss: 2.098393201828003\n",
      "Step 913: train loss: 2.132904052734375\n",
      "Step 913: val loss: 2.094893217086792\n",
      "Step 914: train loss: 2.1293647289276123\n",
      "Step 914: val loss: 2.0914011001586914\n",
      "Step 915: train loss: 2.125833034515381\n",
      "Step 915: val loss: 2.0879154205322266\n",
      "Step 916: train loss: 2.122307777404785\n",
      "Step 916: val loss: 2.0844366550445557\n",
      "Step 917: train loss: 2.1187901496887207\n",
      "Step 917: val loss: 2.0809643268585205\n",
      "Step 918: train loss: 2.115278959274292\n",
      "Step 918: val loss: 2.0774996280670166\n",
      "Step 919: train loss: 2.1117753982543945\n",
      "Step 919: val loss: 2.0740411281585693\n",
      "Step 920: train loss: 2.1082780361175537\n",
      "Step 920: val loss: 2.070589303970337\n",
      "Step 921: train loss: 2.104788064956665\n",
      "Step 921: val loss: 2.0671446323394775\n",
      "Step 922: train loss: 2.101304769515991\n",
      "Step 922: val loss: 2.063706636428833\n",
      "Step 923: train loss: 2.0978286266326904\n",
      "Step 923: val loss: 2.0602753162384033\n",
      "Step 924: train loss: 2.0943593978881836\n",
      "Step 924: val loss: 2.0568506717681885\n",
      "Step 925: train loss: 2.0908970832824707\n",
      "Step 925: val loss: 2.0534327030181885\n",
      "Step 926: train loss: 2.0874414443969727\n",
      "Step 926: val loss: 2.050021171569824\n",
      "Step 927: train loss: 2.0839922428131104\n",
      "Step 927: val loss: 2.046617269515991\n",
      "Step 928: train loss: 2.0805506706237793\n",
      "Step 928: val loss: 2.0432193279266357\n",
      "Step 929: train loss: 2.077115774154663\n",
      "Step 929: val loss: 2.039828300476074\n",
      "Step 930: train loss: 2.0736875534057617\n",
      "Step 930: val loss: 2.0364434719085693\n",
      "Step 931: train loss: 2.070265531539917\n",
      "Step 931: val loss: 2.0330655574798584\n",
      "Step 932: train loss: 2.0668511390686035\n",
      "Step 932: val loss: 2.029693841934204\n",
      "Step 933: train loss: 2.0634427070617676\n",
      "Step 933: val loss: 2.026329517364502\n",
      "Step 934: train loss: 2.0600414276123047\n",
      "Step 934: val loss: 2.0229713916778564\n",
      "Step 935: train loss: 2.056647539138794\n",
      "Step 935: val loss: 2.019619941711426\n",
      "Step 936: train loss: 2.05325984954834\n",
      "Step 936: val loss: 2.01627516746521\n",
      "Step 937: train loss: 2.0498785972595215\n",
      "Step 937: val loss: 2.012936592102051\n",
      "Step 938: train loss: 2.046504259109497\n",
      "Step 938: val loss: 2.0096046924591064\n",
      "Step 939: train loss: 2.0431361198425293\n",
      "Step 939: val loss: 2.0062789916992188\n",
      "Step 940: train loss: 2.0397751331329346\n",
      "Step 940: val loss: 2.002959728240967\n",
      "Step 941: train loss: 2.0364203453063965\n",
      "Step 941: val loss: 1.9996477365493774\n",
      "Step 942: train loss: 2.0330727100372314\n",
      "Step 942: val loss: 1.9963418245315552\n",
      "Step 943: train loss: 2.029731512069702\n",
      "Step 943: val loss: 1.9930421113967896\n",
      "Step 944: train loss: 2.0263969898223877\n",
      "Step 944: val loss: 1.9897493124008179\n",
      "Step 945: train loss: 2.023069143295288\n",
      "Step 945: val loss: 1.9864625930786133\n",
      "Step 946: train loss: 2.019747734069824\n",
      "Step 946: val loss: 1.9831821918487549\n",
      "Step 947: train loss: 2.016432762145996\n",
      "Step 947: val loss: 1.9799087047576904\n",
      "Step 948: train loss: 2.0131242275238037\n",
      "Step 948: val loss: 1.9766411781311035\n",
      "Step 949: train loss: 2.0098226070404053\n",
      "Step 949: val loss: 1.9733798503875732\n",
      "Step 950: train loss: 2.0065269470214844\n",
      "Step 950: val loss: 1.9701253175735474\n",
      "Step 951: train loss: 2.003237724304199\n",
      "Step 951: val loss: 1.9668768644332886\n",
      "Step 952: train loss: 1.9999552965164185\n",
      "Step 952: val loss: 1.963634729385376\n",
      "Step 953: train loss: 1.9966791868209839\n",
      "Step 953: val loss: 1.9603990316390991\n",
      "Step 954: train loss: 1.9934098720550537\n",
      "Step 954: val loss: 1.9571692943572998\n",
      "Step 955: train loss: 1.990146517753601\n",
      "Step 955: val loss: 1.9539461135864258\n",
      "Step 956: train loss: 1.9868896007537842\n",
      "Step 956: val loss: 1.9507291316986084\n",
      "Step 957: train loss: 1.9836393594741821\n",
      "Step 957: val loss: 1.9475185871124268\n",
      "Step 958: train loss: 1.9803953170776367\n",
      "Step 958: val loss: 1.9443140029907227\n",
      "Step 959: train loss: 1.9771578311920166\n",
      "Step 959: val loss: 1.9411158561706543\n",
      "Step 960: train loss: 1.9739264249801636\n",
      "Step 960: val loss: 1.9379239082336426\n",
      "Step 961: train loss: 1.9707016944885254\n",
      "Step 961: val loss: 1.9347381591796875\n",
      "Step 962: train loss: 1.9674831628799438\n",
      "Step 962: val loss: 1.93155837059021\n",
      "Step 963: train loss: 1.9642707109451294\n",
      "Step 963: val loss: 1.9283851385116577\n",
      "Step 964: train loss: 1.9610649347305298\n",
      "Step 964: val loss: 1.925218105316162\n",
      "Step 965: train loss: 1.957865595817566\n",
      "Step 965: val loss: 1.9220569133758545\n",
      "Step 966: train loss: 1.95467209815979\n",
      "Step 966: val loss: 1.9189021587371826\n",
      "Step 967: train loss: 1.951485276222229\n",
      "Step 967: val loss: 1.9157531261444092\n",
      "Step 968: train loss: 1.9483044147491455\n",
      "Step 968: val loss: 1.912610650062561\n",
      "Step 969: train loss: 1.9451297521591187\n",
      "Step 969: val loss: 1.9094737768173218\n",
      "Step 970: train loss: 1.9419610500335693\n",
      "Step 970: val loss: 1.9063429832458496\n",
      "Step 971: train loss: 1.9387987852096558\n",
      "Step 971: val loss: 1.9032186269760132\n",
      "Step 972: train loss: 1.9356426000595093\n",
      "Step 972: val loss: 1.9001002311706543\n",
      "Step 973: train loss: 1.932492733001709\n",
      "Step 973: val loss: 1.8969876766204834\n",
      "Step 974: train loss: 1.9293490648269653\n",
      "Step 974: val loss: 1.8938814401626587\n",
      "Step 975: train loss: 1.9262117147445679\n",
      "Step 975: val loss: 1.890781044960022\n",
      "Step 976: train loss: 1.923080325126648\n",
      "Step 976: val loss: 1.8876869678497314\n",
      "Step 977: train loss: 1.9199551343917847\n",
      "Step 977: val loss: 1.884598731994629\n",
      "Step 978: train loss: 1.9168356657028198\n",
      "Step 978: val loss: 1.8815160989761353\n",
      "Step 979: train loss: 1.9137226343154907\n",
      "Step 979: val loss: 1.8784397840499878\n",
      "Step 980: train loss: 1.9106154441833496\n",
      "Step 980: val loss: 1.8753694295883179\n",
      "Step 981: train loss: 1.9075145721435547\n",
      "Step 981: val loss: 1.872305154800415\n",
      "Step 982: train loss: 1.9044198989868164\n",
      "Step 982: val loss: 1.8692466020584106\n",
      "Step 983: train loss: 1.9013311862945557\n",
      "Step 983: val loss: 1.8661938905715942\n",
      "Step 984: train loss: 1.898248314857483\n",
      "Step 984: val loss: 1.8631471395492554\n",
      "Step 985: train loss: 1.8951715230941772\n",
      "Step 985: val loss: 1.8601062297821045\n",
      "Step 986: train loss: 1.8921005725860596\n",
      "Step 986: val loss: 1.8570713996887207\n",
      "Step 987: train loss: 1.8890358209609985\n",
      "Step 987: val loss: 1.8540419340133667\n",
      "Step 988: train loss: 1.8859765529632568\n",
      "Step 988: val loss: 1.8510184288024902\n",
      "Step 989: train loss: 1.8829238414764404\n",
      "Step 989: val loss: 1.84800124168396\n",
      "Step 990: train loss: 1.8798768520355225\n",
      "Step 990: val loss: 1.8449898958206177\n",
      "Step 991: train loss: 1.876835823059082\n",
      "Step 991: val loss: 1.8419839143753052\n",
      "Step 992: train loss: 1.8738008737564087\n",
      "Step 992: val loss: 1.8389836549758911\n",
      "Step 993: train loss: 1.8707716464996338\n",
      "Step 993: val loss: 1.8359897136688232\n",
      "Step 994: train loss: 1.8677483797073364\n",
      "Step 994: val loss: 1.8330013751983643\n",
      "Step 995: train loss: 1.8647310733795166\n",
      "Step 995: val loss: 1.830018401145935\n",
      "Step 996: train loss: 1.8617192506790161\n",
      "Step 996: val loss: 1.827041506767273\n",
      "Step 997: train loss: 1.8587133884429932\n",
      "Step 997: val loss: 1.8240703344345093\n",
      "Step 998: train loss: 1.8557136058807373\n",
      "Step 998: val loss: 1.821104884147644\n",
      "Step 999: train loss: 1.8527199029922485\n",
      "Step 999: val loss: 1.8181450366973877\n",
      "Step 1000: train loss: 1.8497314453125\n",
      "Step 1000: val loss: 1.8151910305023193\n",
      "Step 1001: train loss: 1.846748948097229\n",
      "Step 1001: val loss: 1.8122429847717285\n",
      "Step 1002: train loss: 1.843772292137146\n",
      "Step 1002: val loss: 1.8093000650405884\n",
      "Step 1003: train loss: 1.840801477432251\n",
      "Step 1003: val loss: 1.8063629865646362\n",
      "Step 1004: train loss: 1.8378362655639648\n",
      "Step 1004: val loss: 1.8034319877624512\n",
      "Step 1005: train loss: 1.8348772525787354\n",
      "Step 1005: val loss: 1.8005061149597168\n",
      "Step 1006: train loss: 1.831923484802246\n",
      "Step 1006: val loss: 1.7975857257843018\n",
      "Step 1007: train loss: 1.8289756774902344\n",
      "Step 1007: val loss: 1.794671654701233\n",
      "Step 1008: train loss: 1.8260334730148315\n",
      "Step 1008: val loss: 1.7917625904083252\n",
      "Step 1009: train loss: 1.8230971097946167\n",
      "Step 1009: val loss: 1.7888593673706055\n",
      "Step 1010: train loss: 1.8201664686203003\n",
      "Step 1010: val loss: 1.7859621047973633\n",
      "Step 1011: train loss: 1.8172415494918823\n",
      "Step 1011: val loss: 1.7830697298049927\n",
      "Step 1012: train loss: 1.8143221139907837\n",
      "Step 1012: val loss: 1.78018319606781\n",
      "Step 1013: train loss: 1.811408519744873\n",
      "Step 1013: val loss: 1.7773020267486572\n",
      "Step 1014: train loss: 1.8085001707077026\n",
      "Step 1014: val loss: 1.774427056312561\n",
      "Step 1015: train loss: 1.8055976629257202\n",
      "Step 1015: val loss: 1.7715567350387573\n",
      "Step 1016: train loss: 1.8027008771896362\n",
      "Step 1016: val loss: 1.7686926126480103\n",
      "Step 1017: train loss: 1.7998095750808716\n",
      "Step 1017: val loss: 1.7658333778381348\n",
      "Step 1018: train loss: 1.796924114227295\n",
      "Step 1018: val loss: 1.7629802227020264\n",
      "Step 1019: train loss: 1.794044017791748\n",
      "Step 1019: val loss: 1.760132074356079\n",
      "Step 1020: train loss: 1.79116952419281\n",
      "Step 1020: val loss: 1.7572895288467407\n",
      "Step 1021: train loss: 1.7883005142211914\n",
      "Step 1021: val loss: 1.7544523477554321\n",
      "Step 1022: train loss: 1.7854372262954712\n",
      "Step 1022: val loss: 1.751620888710022\n",
      "Step 1023: train loss: 1.7825795412063599\n",
      "Step 1023: val loss: 1.7487949132919312\n",
      "Step 1024: train loss: 1.7797269821166992\n",
      "Step 1024: val loss: 1.745974063873291\n",
      "Step 1025: train loss: 1.7768803834915161\n",
      "Step 1025: val loss: 1.7431588172912598\n",
      "Step 1026: train loss: 1.7740390300750732\n",
      "Step 1026: val loss: 1.7403490543365479\n",
      "Step 1027: train loss: 1.7712033987045288\n",
      "Step 1027: val loss: 1.7375444173812866\n",
      "Step 1028: train loss: 1.7683727741241455\n",
      "Step 1028: val loss: 1.7347453832626343\n",
      "Step 1029: train loss: 1.7655481100082397\n",
      "Step 1029: val loss: 1.7319514751434326\n",
      "Step 1030: train loss: 1.7627285718917847\n",
      "Step 1030: val loss: 1.7291631698608398\n",
      "Step 1031: train loss: 1.759914755821228\n",
      "Step 1031: val loss: 1.7263799905776978\n",
      "Step 1032: train loss: 1.7571059465408325\n",
      "Step 1032: val loss: 1.723602533340454\n",
      "Step 1033: train loss: 1.754302978515625\n",
      "Step 1033: val loss: 1.7208300828933716\n",
      "Step 1034: train loss: 1.7515051364898682\n",
      "Step 1034: val loss: 1.7180628776550293\n",
      "Step 1035: train loss: 1.7487128973007202\n",
      "Step 1035: val loss: 1.7153010368347168\n",
      "Step 1036: train loss: 1.7459259033203125\n",
      "Step 1036: val loss: 1.712544560432434\n",
      "Step 1037: train loss: 1.7431446313858032\n",
      "Step 1037: val loss: 1.7097934484481812\n",
      "Step 1038: train loss: 1.7403684854507446\n",
      "Step 1038: val loss: 1.7070472240447998\n",
      "Step 1039: train loss: 1.7375969886779785\n",
      "Step 1039: val loss: 1.7043063640594482\n",
      "Step 1040: train loss: 1.734831690788269\n",
      "Step 1040: val loss: 1.701570987701416\n",
      "Step 1041: train loss: 1.7320713996887207\n",
      "Step 1041: val loss: 1.6988409757614136\n",
      "Step 1042: train loss: 1.7293163537979126\n",
      "Step 1042: val loss: 1.696115493774414\n",
      "Step 1043: train loss: 1.7265667915344238\n",
      "Step 1043: val loss: 1.6933958530426025\n",
      "Step 1044: train loss: 1.7238224744796753\n",
      "Step 1044: val loss: 1.6906812191009521\n",
      "Step 1045: train loss: 1.7210835218429565\n",
      "Step 1045: val loss: 1.6879714727401733\n",
      "Step 1046: train loss: 1.7183493375778198\n",
      "Step 1046: val loss: 1.6852673292160034\n",
      "Step 1047: train loss: 1.7156211137771606\n",
      "Step 1047: val loss: 1.682568073272705\n",
      "Step 1048: train loss: 1.712897777557373\n",
      "Step 1048: val loss: 1.679874300956726\n",
      "Step 1049: train loss: 1.7101796865463257\n",
      "Step 1049: val loss: 1.6771855354309082\n",
      "Step 1050: train loss: 1.7074671983718872\n",
      "Step 1050: val loss: 1.674501657485962\n",
      "Step 1051: train loss: 1.7047592401504517\n",
      "Step 1051: val loss: 1.6718231439590454\n",
      "Step 1052: train loss: 1.702057123184204\n",
      "Step 1052: val loss: 1.66914963722229\n",
      "Step 1053: train loss: 1.6993600130081177\n",
      "Step 1053: val loss: 1.6664814949035645\n",
      "Step 1054: train loss: 1.6966675519943237\n",
      "Step 1054: val loss: 1.6638182401657104\n",
      "Step 1055: train loss: 1.6939809322357178\n",
      "Step 1055: val loss: 1.6611597537994385\n",
      "Step 1056: train loss: 1.6912990808486938\n",
      "Step 1056: val loss: 1.6585068702697754\n",
      "Step 1057: train loss: 1.6886224746704102\n",
      "Step 1057: val loss: 1.6558587551116943\n",
      "Step 1058: train loss: 1.6859513521194458\n",
      "Step 1058: val loss: 1.6532158851623535\n",
      "Step 1059: train loss: 1.683284878730774\n",
      "Step 1059: val loss: 1.6505777835845947\n",
      "Step 1060: train loss: 1.6806235313415527\n",
      "Step 1060: val loss: 1.6479450464248657\n",
      "Step 1061: train loss: 1.6779675483703613\n",
      "Step 1061: val loss: 1.6453168392181396\n",
      "Step 1062: train loss: 1.6753166913986206\n",
      "Step 1062: val loss: 1.642694115638733\n",
      "Step 1063: train loss: 1.6726704835891724\n",
      "Step 1063: val loss: 1.6400763988494873\n",
      "Step 1064: train loss: 1.6700299978256226\n",
      "Step 1064: val loss: 1.6374635696411133\n",
      "Step 1065: train loss: 1.6673942804336548\n",
      "Step 1065: val loss: 1.6348555088043213\n",
      "Step 1066: train loss: 1.6647636890411377\n",
      "Step 1066: val loss: 1.6322524547576904\n",
      "Step 1067: train loss: 1.6621376276016235\n",
      "Step 1067: val loss: 1.6296546459197998\n",
      "Step 1068: train loss: 1.6595170497894287\n",
      "Step 1068: val loss: 1.6270616054534912\n",
      "Step 1069: train loss: 1.6569015979766846\n",
      "Step 1069: val loss: 1.6244730949401855\n",
      "Step 1070: train loss: 1.6542909145355225\n",
      "Step 1070: val loss: 1.6218903064727783\n",
      "Step 1071: train loss: 1.6516849994659424\n",
      "Step 1071: val loss: 1.619312047958374\n",
      "Step 1072: train loss: 1.6490846872329712\n",
      "Step 1072: val loss: 1.6167385578155518\n",
      "Step 1073: train loss: 1.646488904953003\n",
      "Step 1073: val loss: 1.6141703128814697\n",
      "Step 1074: train loss: 1.643898367881775\n",
      "Step 1074: val loss: 1.6116067171096802\n",
      "Step 1075: train loss: 1.6413123607635498\n",
      "Step 1075: val loss: 1.6090484857559204\n",
      "Step 1076: train loss: 1.6387317180633545\n",
      "Step 1076: val loss: 1.6064945459365845\n",
      "Step 1077: train loss: 1.6361560821533203\n",
      "Step 1077: val loss: 1.6039457321166992\n",
      "Step 1078: train loss: 1.6335852146148682\n",
      "Step 1078: val loss: 1.6014013290405273\n",
      "Step 1079: train loss: 1.631019115447998\n",
      "Step 1079: val loss: 1.5988620519638062\n",
      "Step 1080: train loss: 1.6284579038619995\n",
      "Step 1080: val loss: 1.5963276624679565\n",
      "Step 1081: train loss: 1.6259018182754517\n",
      "Step 1081: val loss: 1.5937985181808472\n",
      "Step 1082: train loss: 1.6233508586883545\n",
      "Step 1082: val loss: 1.591273546218872\n",
      "Step 1083: train loss: 1.6208041906356812\n",
      "Step 1083: val loss: 1.588753581047058\n",
      "Step 1084: train loss: 1.618262529373169\n",
      "Step 1084: val loss: 1.5862383842468262\n",
      "Step 1085: train loss: 1.6157257556915283\n",
      "Step 1085: val loss: 1.5837278366088867\n",
      "Step 1086: train loss: 1.6131938695907593\n",
      "Step 1086: val loss: 1.5812222957611084\n",
      "Step 1087: train loss: 1.6106669902801514\n",
      "Step 1087: val loss: 1.5787214040756226\n",
      "Step 1088: train loss: 1.608144760131836\n",
      "Step 1088: val loss: 1.5762251615524292\n",
      "Step 1089: train loss: 1.6056275367736816\n",
      "Step 1089: val loss: 1.573733925819397\n",
      "Step 1090: train loss: 1.6031149625778198\n",
      "Step 1090: val loss: 1.5712475776672363\n",
      "Step 1091: train loss: 1.60060715675354\n",
      "Step 1091: val loss: 1.56876540184021\n",
      "Step 1092: train loss: 1.5981038808822632\n",
      "Step 1092: val loss: 1.5662879943847656\n",
      "Step 1093: train loss: 1.5956056118011475\n",
      "Step 1093: val loss: 1.563815712928772\n",
      "Step 1094: train loss: 1.593112587928772\n",
      "Step 1094: val loss: 1.5613481998443604\n",
      "Step 1095: train loss: 1.5906238555908203\n",
      "Step 1095: val loss: 1.5588852167129517\n",
      "Step 1096: train loss: 1.5881398916244507\n",
      "Step 1096: val loss: 1.5564266443252563\n",
      "Step 1097: train loss: 1.5856608152389526\n",
      "Step 1097: val loss: 1.5539731979370117\n",
      "Step 1098: train loss: 1.5831865072250366\n",
      "Step 1098: val loss: 1.551524043083191\n",
      "Step 1099: train loss: 1.580716609954834\n",
      "Step 1099: val loss: 1.549079418182373\n",
      "Step 1100: train loss: 1.578251600265503\n",
      "Step 1100: val loss: 1.5466399192810059\n",
      "Step 1101: train loss: 1.5757910013198853\n",
      "Step 1101: val loss: 1.5442044734954834\n",
      "Step 1102: train loss: 1.5733352899551392\n",
      "Step 1102: val loss: 1.5417739152908325\n",
      "Step 1103: train loss: 1.570884346961975\n",
      "Step 1103: val loss: 1.539347767829895\n",
      "Step 1104: train loss: 1.5684380531311035\n",
      "Step 1104: val loss: 1.536926507949829\n",
      "Step 1105: train loss: 1.5659959316253662\n",
      "Step 1105: val loss: 1.534509539604187\n",
      "Step 1106: train loss: 1.5635589361190796\n",
      "Step 1106: val loss: 1.5320972204208374\n",
      "Step 1107: train loss: 1.561126470565796\n",
      "Step 1107: val loss: 1.5296893119812012\n",
      "Step 1108: train loss: 1.5586985349655151\n",
      "Step 1108: val loss: 1.5272860527038574\n",
      "Step 1109: train loss: 1.5562751293182373\n",
      "Step 1109: val loss: 1.5248876810073853\n",
      "Step 1110: train loss: 1.553856611251831\n",
      "Step 1110: val loss: 1.5224937200546265\n",
      "Step 1111: train loss: 1.5514428615570068\n",
      "Step 1111: val loss: 1.5201047658920288\n",
      "Step 1112: train loss: 1.5490336418151855\n",
      "Step 1112: val loss: 1.5177199840545654\n",
      "Step 1113: train loss: 1.5466290712356567\n",
      "Step 1113: val loss: 1.5153393745422363\n",
      "Step 1114: train loss: 1.5442286729812622\n",
      "Step 1114: val loss: 1.5129634141921997\n",
      "Step 1115: train loss: 1.5418329238891602\n",
      "Step 1115: val loss: 1.5105918645858765\n",
      "Step 1116: train loss: 1.539441704750061\n",
      "Step 1116: val loss: 1.5082249641418457\n",
      "Step 1117: train loss: 1.5370551347732544\n",
      "Step 1117: val loss: 1.5058621168136597\n",
      "Step 1118: train loss: 1.534672737121582\n",
      "Step 1118: val loss: 1.5035043954849243\n",
      "Step 1119: train loss: 1.5322953462600708\n",
      "Step 1119: val loss: 1.5011506080627441\n",
      "Step 1120: train loss: 1.5299222469329834\n",
      "Step 1120: val loss: 1.4988019466400146\n",
      "Step 1121: train loss: 1.527553677558899\n",
      "Step 1121: val loss: 1.4964570999145508\n",
      "Step 1122: train loss: 1.5251898765563965\n",
      "Step 1122: val loss: 1.494117021560669\n",
      "Step 1123: train loss: 1.5228303670883179\n",
      "Step 1123: val loss: 1.4917813539505005\n",
      "Step 1124: train loss: 1.5204753875732422\n",
      "Step 1124: val loss: 1.489450216293335\n",
      "Step 1125: train loss: 1.5181248188018799\n",
      "Step 1125: val loss: 1.4871234893798828\n",
      "Step 1126: train loss: 1.515778660774231\n",
      "Step 1126: val loss: 1.4848010540008545\n",
      "Step 1127: train loss: 1.5134373903274536\n",
      "Step 1127: val loss: 1.482483148574829\n",
      "Step 1128: train loss: 1.5111002922058105\n",
      "Step 1128: val loss: 1.4801697731018066\n",
      "Step 1129: train loss: 1.5087677240371704\n",
      "Step 1129: val loss: 1.477860450744629\n",
      "Step 1130: train loss: 1.5064396858215332\n",
      "Step 1130: val loss: 1.475555419921875\n",
      "Step 1131: train loss: 1.5041155815124512\n",
      "Step 1131: val loss: 1.473254919052124\n",
      "Step 1132: train loss: 1.501796007156372\n",
      "Step 1132: val loss: 1.4709585905075073\n",
      "Step 1133: train loss: 1.499480962753296\n",
      "Step 1133: val loss: 1.468666672706604\n",
      "Step 1134: train loss: 1.4971704483032227\n",
      "Step 1134: val loss: 1.4663794040679932\n",
      "Step 1135: train loss: 1.4948639869689941\n",
      "Step 1135: val loss: 1.4640963077545166\n",
      "Step 1136: train loss: 1.4925622940063477\n",
      "Step 1136: val loss: 1.4618173837661743\n",
      "Step 1137: train loss: 1.490264892578125\n",
      "Step 1137: val loss: 1.459542989730835\n",
      "Step 1138: train loss: 1.4879717826843262\n",
      "Step 1138: val loss: 1.4572726488113403\n",
      "Step 1139: train loss: 1.4856828451156616\n",
      "Step 1139: val loss: 1.455006718635559\n",
      "Step 1140: train loss: 1.483398199081421\n",
      "Step 1140: val loss: 1.4527448415756226\n",
      "Step 1141: train loss: 1.4811182022094727\n",
      "Step 1141: val loss: 1.450487494468689\n",
      "Step 1142: train loss: 1.4788421392440796\n",
      "Step 1142: val loss: 1.4482343196868896\n",
      "Step 1143: train loss: 1.476570963859558\n",
      "Step 1143: val loss: 1.4459857940673828\n",
      "Step 1144: train loss: 1.4743037223815918\n",
      "Step 1144: val loss: 1.4437410831451416\n",
      "Step 1145: train loss: 1.4720406532287598\n",
      "Step 1145: val loss: 1.4415005445480347\n",
      "Step 1146: train loss: 1.4697822332382202\n",
      "Step 1146: val loss: 1.4392644166946411\n",
      "Step 1147: train loss: 1.4675276279449463\n",
      "Step 1147: val loss: 1.4370325803756714\n",
      "Step 1148: train loss: 1.4652774333953857\n",
      "Step 1148: val loss: 1.434804916381836\n",
      "Step 1149: train loss: 1.4630320072174072\n",
      "Step 1149: val loss: 1.4325811862945557\n",
      "Step 1150: train loss: 1.4607902765274048\n",
      "Step 1150: val loss: 1.4303618669509888\n",
      "Step 1151: train loss: 1.4585529565811157\n",
      "Step 1151: val loss: 1.4281469583511353\n",
      "Step 1152: train loss: 1.4563195705413818\n",
      "Step 1152: val loss: 1.425935983657837\n",
      "Step 1153: train loss: 1.4540908336639404\n",
      "Step 1153: val loss: 1.4237293004989624\n",
      "Step 1154: train loss: 1.4518662691116333\n",
      "Step 1154: val loss: 1.4215264320373535\n",
      "Step 1155: train loss: 1.4496455192565918\n",
      "Step 1155: val loss: 1.419328212738037\n",
      "Step 1156: train loss: 1.4474291801452637\n",
      "Step 1156: val loss: 1.4171336889266968\n",
      "Step 1157: train loss: 1.445217251777649\n",
      "Step 1157: val loss: 1.4149435758590698\n",
      "Step 1158: train loss: 1.4430092573165894\n",
      "Step 1158: val loss: 1.412757396697998\n",
      "Step 1159: train loss: 1.440805435180664\n",
      "Step 1159: val loss: 1.4105756282806396\n",
      "Step 1160: train loss: 1.4386056661605835\n",
      "Step 1160: val loss: 1.408397912979126\n",
      "Step 1161: train loss: 1.4364105463027954\n",
      "Step 1161: val loss: 1.4062241315841675\n",
      "Step 1162: train loss: 1.434219241142273\n",
      "Step 1162: val loss: 1.4040544033050537\n",
      "Step 1163: train loss: 1.4320321083068848\n",
      "Step 1163: val loss: 1.4018892049789429\n",
      "Step 1164: train loss: 1.4298491477966309\n",
      "Step 1164: val loss: 1.399727702140808\n",
      "Step 1165: train loss: 1.4276702404022217\n",
      "Step 1165: val loss: 1.3975703716278076\n",
      "Step 1166: train loss: 1.4254953861236572\n",
      "Step 1166: val loss: 1.3954170942306519\n",
      "Step 1167: train loss: 1.423324704170227\n",
      "Step 1167: val loss: 1.3932678699493408\n",
      "Step 1168: train loss: 1.4211584329605103\n",
      "Step 1168: val loss: 1.391122817993164\n",
      "Step 1169: train loss: 1.4189958572387695\n",
      "Step 1169: val loss: 1.388981580734253\n",
      "Step 1170: train loss: 1.4168375730514526\n",
      "Step 1170: val loss: 1.3868447542190552\n",
      "Step 1171: train loss: 1.4146833419799805\n",
      "Step 1171: val loss: 1.384711503982544\n",
      "Step 1172: train loss: 1.4125330448150635\n",
      "Step 1172: val loss: 1.382582664489746\n",
      "Step 1173: train loss: 1.4103868007659912\n",
      "Step 1173: val loss: 1.3804577589035034\n",
      "Step 1174: train loss: 1.4082449674606323\n",
      "Step 1174: val loss: 1.378336787223816\n",
      "Step 1175: train loss: 1.4061068296432495\n",
      "Step 1175: val loss: 1.3762197494506836\n",
      "Step 1176: train loss: 1.4039727449417114\n",
      "Step 1176: val loss: 1.374106526374817\n",
      "Step 1177: train loss: 1.4018425941467285\n",
      "Step 1177: val loss: 1.3719977140426636\n",
      "Step 1178: train loss: 1.3997164964675903\n",
      "Step 1178: val loss: 1.3698925971984863\n",
      "Step 1179: train loss: 1.397594690322876\n",
      "Step 1179: val loss: 1.3677912950515747\n",
      "Step 1180: train loss: 1.3954765796661377\n",
      "Step 1180: val loss: 1.365694284439087\n",
      "Step 1181: train loss: 1.3933625221252441\n",
      "Step 1181: val loss: 1.3636012077331543\n",
      "Step 1182: train loss: 1.3912526369094849\n",
      "Step 1182: val loss: 1.3615119457244873\n",
      "Step 1183: train loss: 1.3891464471817017\n",
      "Step 1183: val loss: 1.3594266176223755\n",
      "Step 1184: train loss: 1.3870447874069214\n",
      "Step 1184: val loss: 1.3573451042175293\n",
      "Step 1185: train loss: 1.384946346282959\n",
      "Step 1185: val loss: 1.3552675247192383\n",
      "Step 1186: train loss: 1.38285231590271\n",
      "Step 1186: val loss: 1.3531943559646606\n",
      "Step 1187: train loss: 1.3807622194290161\n",
      "Step 1187: val loss: 1.3511245250701904\n",
      "Step 1188: train loss: 1.3786758184432983\n",
      "Step 1188: val loss: 1.3490588665008545\n",
      "Step 1189: train loss: 1.3765937089920044\n",
      "Step 1189: val loss: 1.3469969034194946\n",
      "Step 1190: train loss: 1.374515175819397\n",
      "Step 1190: val loss: 1.3449389934539795\n",
      "Step 1191: train loss: 1.3724406957626343\n",
      "Step 1191: val loss: 1.3428847789764404\n",
      "Step 1192: train loss: 1.3703699111938477\n",
      "Step 1192: val loss: 1.3408344984054565\n",
      "Step 1193: train loss: 1.3683032989501953\n",
      "Step 1193: val loss: 1.3387879133224487\n",
      "Step 1194: train loss: 1.366240382194519\n",
      "Step 1194: val loss: 1.3367455005645752\n",
      "Step 1195: train loss: 1.364181399345398\n",
      "Step 1195: val loss: 1.3347067832946777\n",
      "Step 1196: train loss: 1.3621265888214111\n",
      "Step 1196: val loss: 1.332671880722046\n",
      "Step 1197: train loss: 1.3600754737854004\n",
      "Step 1197: val loss: 1.3306410312652588\n",
      "Step 1198: train loss: 1.3580280542373657\n",
      "Step 1198: val loss: 1.3286138772964478\n",
      "Step 1199: train loss: 1.3559844493865967\n",
      "Step 1199: val loss: 1.3265904188156128\n",
      "Step 1200: train loss: 1.3539451360702515\n",
      "Step 1200: val loss: 1.3245710134506226\n",
      "Step 1201: train loss: 1.3519092798233032\n",
      "Step 1201: val loss: 1.3225553035736084\n",
      "Step 1202: train loss: 1.3498774766921997\n",
      "Step 1202: val loss: 1.3205431699752808\n",
      "Step 1203: train loss: 1.3478492498397827\n",
      "Step 1203: val loss: 1.3185352087020874\n",
      "Step 1204: train loss: 1.3458250761032104\n",
      "Step 1204: val loss: 1.3165305852890015\n",
      "Step 1205: train loss: 1.3438045978546143\n",
      "Step 1205: val loss: 1.3145298957824707\n",
      "Step 1206: train loss: 1.3417880535125732\n",
      "Step 1206: val loss: 1.3125331401824951\n",
      "Step 1207: train loss: 1.3397752046585083\n",
      "Step 1207: val loss: 1.3105398416519165\n",
      "Step 1208: train loss: 1.3377655744552612\n",
      "Step 1208: val loss: 1.308550238609314\n",
      "Step 1209: train loss: 1.335760235786438\n",
      "Step 1209: val loss: 1.306564450263977\n",
      "Step 1210: train loss: 1.3337585926055908\n",
      "Step 1210: val loss: 1.3045825958251953\n",
      "Step 1211: train loss: 1.3317608833312988\n",
      "Step 1211: val loss: 1.3026044368743896\n",
      "Step 1212: train loss: 1.329766869544983\n",
      "Step 1212: val loss: 1.3006298542022705\n",
      "Step 1213: train loss: 1.3277764320373535\n",
      "Step 1213: val loss: 1.298659086227417\n",
      "Step 1214: train loss: 1.3257896900177002\n",
      "Step 1214: val loss: 1.296692132949829\n",
      "Step 1215: train loss: 1.3238070011138916\n",
      "Step 1215: val loss: 1.2947286367416382\n",
      "Step 1216: train loss: 1.32182776927948\n",
      "Step 1216: val loss: 1.2927688360214233\n",
      "Step 1217: train loss: 1.3198521137237549\n",
      "Step 1217: val loss: 1.2908127307891846\n",
      "Step 1218: train loss: 1.3178805112838745\n",
      "Step 1218: val loss: 1.2888602018356323\n",
      "Step 1219: train loss: 1.3159123659133911\n",
      "Step 1219: val loss: 1.2869116067886353\n",
      "Step 1220: train loss: 1.3139482736587524\n",
      "Step 1220: val loss: 1.2849665880203247\n",
      "Step 1221: train loss: 1.3119874000549316\n",
      "Step 1221: val loss: 1.2830253839492798\n",
      "Step 1222: train loss: 1.3100306987762451\n",
      "Step 1222: val loss: 1.2810876369476318\n",
      "Step 1223: train loss: 1.3080775737762451\n",
      "Step 1223: val loss: 1.2791534662246704\n",
      "Step 1224: train loss: 1.306127667427063\n",
      "Step 1224: val loss: 1.2772231101989746\n",
      "Step 1225: train loss: 1.3041820526123047\n",
      "Step 1225: val loss: 1.2752962112426758\n",
      "Step 1226: train loss: 1.3022396564483643\n",
      "Step 1226: val loss: 1.273373007774353\n",
      "Step 1227: train loss: 1.3003008365631104\n",
      "Step 1227: val loss: 1.2714534997940063\n",
      "Step 1228: train loss: 1.298365831375122\n",
      "Step 1228: val loss: 1.269537329673767\n",
      "Step 1229: train loss: 1.2964344024658203\n",
      "Step 1229: val loss: 1.2676249742507935\n",
      "Step 1230: train loss: 1.2945066690444946\n",
      "Step 1230: val loss: 1.2657161951065063\n",
      "Step 1231: train loss: 1.2925827503204346\n",
      "Step 1231: val loss: 1.2638109922409058\n",
      "Step 1232: train loss: 1.2906622886657715\n",
      "Step 1232: val loss: 1.2619092464447021\n",
      "Step 1233: train loss: 1.288745403289795\n",
      "Step 1233: val loss: 1.2600115537643433\n",
      "Step 1234: train loss: 1.2868319749832153\n",
      "Step 1234: val loss: 1.2581169605255127\n",
      "Step 1235: train loss: 1.2849222421646118\n",
      "Step 1235: val loss: 1.2562261819839478\n",
      "Step 1236: train loss: 1.2830162048339844\n",
      "Step 1236: val loss: 1.2543386220932007\n",
      "Step 1237: train loss: 1.2811137437820435\n",
      "Step 1237: val loss: 1.2524547576904297\n",
      "Step 1238: train loss: 1.27921462059021\n",
      "Step 1238: val loss: 1.2505744695663452\n",
      "Step 1239: train loss: 1.277319073677063\n",
      "Step 1239: val loss: 1.2486975193023682\n",
      "Step 1240: train loss: 1.2754271030426025\n",
      "Step 1240: val loss: 1.2468243837356567\n",
      "Step 1241: train loss: 1.2735388278961182\n",
      "Step 1241: val loss: 1.2449543476104736\n",
      "Step 1242: train loss: 1.2716537714004517\n",
      "Step 1242: val loss: 1.2430881261825562\n",
      "Step 1243: train loss: 1.2697726488113403\n",
      "Step 1243: val loss: 1.2412254810333252\n",
      "Step 1244: train loss: 1.2678947448730469\n",
      "Step 1244: val loss: 1.239366054534912\n",
      "Step 1245: train loss: 1.2660205364227295\n",
      "Step 1245: val loss: 1.2375102043151855\n",
      "Step 1246: train loss: 1.26414954662323\n",
      "Step 1246: val loss: 1.2356579303741455\n",
      "Step 1247: train loss: 1.2622824907302856\n",
      "Step 1247: val loss: 1.2338091135025024\n",
      "Step 1248: train loss: 1.2604186534881592\n",
      "Step 1248: val loss: 1.2319636344909668\n",
      "Step 1249: train loss: 1.2585583925247192\n",
      "Step 1249: val loss: 1.2301220893859863\n",
      "Step 1250: train loss: 1.2567017078399658\n",
      "Step 1250: val loss: 1.2282835245132446\n",
      "Step 1251: train loss: 1.254848599433899\n",
      "Step 1251: val loss: 1.226448655128479\n",
      "Step 1252: train loss: 1.25299870967865\n",
      "Step 1252: val loss: 1.2246168851852417\n",
      "Step 1253: train loss: 1.2511522769927979\n",
      "Step 1253: val loss: 1.2227885723114014\n",
      "Step 1254: train loss: 1.2493090629577637\n",
      "Step 1254: val loss: 1.2209640741348267\n",
      "Step 1255: train loss: 1.2474700212478638\n",
      "Step 1255: val loss: 1.2191427946090698\n",
      "Step 1256: train loss: 1.245633840560913\n",
      "Step 1256: val loss: 1.2173250913619995\n",
      "Step 1257: train loss: 1.243801236152649\n",
      "Step 1257: val loss: 1.215510368347168\n",
      "Step 1258: train loss: 1.2419719696044922\n",
      "Step 1258: val loss: 1.213699460029602\n",
      "Step 1259: train loss: 1.2401463985443115\n",
      "Step 1259: val loss: 1.211892008781433\n",
      "Step 1260: train loss: 1.2383240461349487\n",
      "Step 1260: val loss: 1.2100874185562134\n",
      "Step 1261: train loss: 1.236505150794983\n",
      "Step 1261: val loss: 1.2082866430282593\n",
      "Step 1262: train loss: 1.2346895933151245\n",
      "Step 1262: val loss: 1.206489086151123\n",
      "Step 1263: train loss: 1.232877492904663\n",
      "Step 1263: val loss: 1.2046951055526733\n",
      "Step 1264: train loss: 1.2310688495635986\n",
      "Step 1264: val loss: 1.202904462814331\n",
      "Step 1265: train loss: 1.2292636632919312\n",
      "Step 1265: val loss: 1.2011170387268066\n",
      "Step 1266: train loss: 1.227461814880371\n",
      "Step 1266: val loss: 1.1993328332901\n",
      "Step 1267: train loss: 1.225663185119629\n",
      "Step 1267: val loss: 1.1975520849227905\n",
      "Step 1268: train loss: 1.2238678932189941\n",
      "Step 1268: val loss: 1.195774793624878\n",
      "Step 1269: train loss: 1.2220759391784668\n",
      "Step 1269: val loss: 1.1940007209777832\n",
      "Step 1270: train loss: 1.2202874422073364\n",
      "Step 1270: val loss: 1.192229986190796\n",
      "Step 1271: train loss: 1.218502402305603\n",
      "Step 1271: val loss: 1.190462589263916\n",
      "Step 1272: train loss: 1.2167205810546875\n",
      "Step 1272: val loss: 1.1886985301971436\n",
      "Step 1273: train loss: 1.214942216873169\n",
      "Step 1273: val loss: 1.186937689781189\n",
      "Step 1274: train loss: 1.2131669521331787\n",
      "Step 1274: val loss: 1.1851801872253418\n",
      "Step 1275: train loss: 1.2113951444625854\n",
      "Step 1275: val loss: 1.1834259033203125\n",
      "Step 1276: train loss: 1.209626317024231\n",
      "Step 1276: val loss: 1.181674838066101\n",
      "Step 1277: train loss: 1.207861065864563\n",
      "Step 1277: val loss: 1.1799273490905762\n",
      "Step 1278: train loss: 1.206099271774292\n",
      "Step 1278: val loss: 1.1781830787658691\n",
      "Step 1279: train loss: 1.2043405771255493\n",
      "Step 1279: val loss: 1.1764421463012695\n",
      "Step 1280: train loss: 1.2025854587554932\n",
      "Step 1280: val loss: 1.1747041940689087\n",
      "Step 1281: train loss: 1.2008334398269653\n",
      "Step 1281: val loss: 1.1729696989059448\n",
      "Step 1282: train loss: 1.1990846395492554\n",
      "Step 1282: val loss: 1.1712384223937988\n",
      "Step 1283: train loss: 1.1973389387130737\n",
      "Step 1283: val loss: 1.1695102453231812\n",
      "Step 1284: train loss: 1.195596694946289\n",
      "Step 1284: val loss: 1.1677852869033813\n",
      "Step 1285: train loss: 1.1938575506210327\n",
      "Step 1285: val loss: 1.1660635471343994\n",
      "Step 1286: train loss: 1.1921218633651733\n",
      "Step 1286: val loss: 1.1643450260162354\n",
      "Step 1287: train loss: 1.1903890371322632\n",
      "Step 1287: val loss: 1.1626296043395996\n",
      "Step 1288: train loss: 1.1886595487594604\n",
      "Step 1288: val loss: 1.1609176397323608\n",
      "Step 1289: train loss: 1.1869333982467651\n",
      "Step 1289: val loss: 1.15920889377594\n",
      "Step 1290: train loss: 1.1852107048034668\n",
      "Step 1290: val loss: 1.1575032472610474\n",
      "Step 1291: train loss: 1.1834912300109863\n",
      "Step 1291: val loss: 1.1558012962341309\n",
      "Step 1292: train loss: 1.1817748546600342\n",
      "Step 1292: val loss: 1.1541019678115845\n",
      "Step 1293: train loss: 1.1800618171691895\n",
      "Step 1293: val loss: 1.152406096458435\n",
      "Step 1294: train loss: 1.178351640701294\n",
      "Step 1294: val loss: 1.150713324546814\n",
      "Step 1295: train loss: 1.176645040512085\n",
      "Step 1295: val loss: 1.1490236520767212\n",
      "Step 1296: train loss: 1.1749413013458252\n",
      "Step 1296: val loss: 1.1473374366760254\n",
      "Step 1297: train loss: 1.1732410192489624\n",
      "Step 1297: val loss: 1.1456540822982788\n",
      "Step 1298: train loss: 1.171543836593628\n",
      "Step 1298: val loss: 1.1439738273620605\n",
      "Step 1299: train loss: 1.1698495149612427\n",
      "Step 1299: val loss: 1.1422969102859497\n",
      "Step 1300: train loss: 1.168158769607544\n",
      "Step 1300: val loss: 1.1406230926513672\n",
      "Step 1301: train loss: 1.166471004486084\n",
      "Step 1301: val loss: 1.1389521360397339\n",
      "Step 1302: train loss: 1.1647863388061523\n",
      "Step 1302: val loss: 1.137284517288208\n",
      "Step 1303: train loss: 1.163104772567749\n",
      "Step 1303: val loss: 1.1356199979782104\n",
      "Step 1304: train loss: 1.1614265441894531\n",
      "Step 1304: val loss: 1.1339586973190308\n",
      "Step 1305: train loss: 1.1597514152526855\n",
      "Step 1305: val loss: 1.132300615310669\n",
      "Step 1306: train loss: 1.1580791473388672\n",
      "Step 1306: val loss: 1.130645513534546\n",
      "Step 1307: train loss: 1.1564103364944458\n",
      "Step 1307: val loss: 1.1289933919906616\n",
      "Step 1308: train loss: 1.1547443866729736\n",
      "Step 1308: val loss: 1.1273443698883057\n",
      "Step 1309: train loss: 1.1530816555023193\n",
      "Step 1309: val loss: 1.1256985664367676\n",
      "Step 1310: train loss: 1.151422142982483\n",
      "Step 1310: val loss: 1.1240558624267578\n",
      "Step 1311: train loss: 1.1497656106948853\n",
      "Step 1311: val loss: 1.1224159002304077\n",
      "Step 1312: train loss: 1.1481119394302368\n",
      "Step 1312: val loss: 1.120779037475586\n",
      "Step 1313: train loss: 1.1464612483978271\n",
      "Step 1313: val loss: 1.1191452741622925\n",
      "Step 1314: train loss: 1.144813895225525\n",
      "Step 1314: val loss: 1.1175146102905273\n",
      "Step 1315: train loss: 1.1431694030761719\n",
      "Step 1315: val loss: 1.115886926651001\n",
      "Step 1316: train loss: 1.1415281295776367\n",
      "Step 1316: val loss: 1.1142624616622925\n",
      "Step 1317: train loss: 1.1398898363113403\n",
      "Step 1317: val loss: 1.1126408576965332\n",
      "Step 1318: train loss: 1.1382547616958618\n",
      "Step 1318: val loss: 1.1110223531723022\n",
      "Step 1319: train loss: 1.136622667312622\n",
      "Step 1319: val loss: 1.1094070672988892\n",
      "Step 1320: train loss: 1.1349936723709106\n",
      "Step 1320: val loss: 1.1077946424484253\n",
      "Step 1321: train loss: 1.1333675384521484\n",
      "Step 1321: val loss: 1.1061855554580688\n",
      "Step 1322: train loss: 1.1317447423934937\n",
      "Step 1322: val loss: 1.104579210281372\n",
      "Step 1323: train loss: 1.1301250457763672\n",
      "Step 1323: val loss: 1.1029757261276245\n",
      "Step 1324: train loss: 1.1285078525543213\n",
      "Step 1324: val loss: 1.1013752222061157\n",
      "Step 1325: train loss: 1.1268937587738037\n",
      "Step 1325: val loss: 1.0997775793075562\n",
      "Step 1326: train loss: 1.1252827644348145\n",
      "Step 1326: val loss: 1.098183274269104\n",
      "Step 1327: train loss: 1.123674750328064\n",
      "Step 1327: val loss: 1.0965919494628906\n",
      "Step 1328: train loss: 1.1220698356628418\n",
      "Step 1328: val loss: 1.0950034856796265\n",
      "Step 1329: train loss: 1.120468020439148\n",
      "Step 1329: val loss: 1.0934181213378906\n",
      "Step 1330: train loss: 1.1188690662384033\n",
      "Step 1330: val loss: 1.0918357372283936\n",
      "Step 1331: train loss: 1.117273211479187\n",
      "Step 1331: val loss: 1.090255856513977\n",
      "Step 1332: train loss: 1.1156799793243408\n",
      "Step 1332: val loss: 1.0886794328689575\n",
      "Step 1333: train loss: 1.1140899658203125\n",
      "Step 1333: val loss: 1.0871057510375977\n",
      "Step 1334: train loss: 1.1125026941299438\n",
      "Step 1334: val loss: 1.085534930229187\n",
      "Step 1335: train loss: 1.1109185218811035\n",
      "Step 1335: val loss: 1.0839672088623047\n",
      "Step 1336: train loss: 1.1093374490737915\n",
      "Step 1336: val loss: 1.0824024677276611\n",
      "Step 1337: train loss: 1.1077593564987183\n",
      "Step 1337: val loss: 1.0808404684066772\n",
      "Step 1338: train loss: 1.1061840057373047\n",
      "Step 1338: val loss: 1.0792814493179321\n",
      "Step 1339: train loss: 1.1046113967895508\n",
      "Step 1339: val loss: 1.0777252912521362\n",
      "Step 1340: train loss: 1.1030418872833252\n",
      "Step 1340: val loss: 1.076172113418579\n",
      "Step 1341: train loss: 1.1014753580093384\n",
      "Step 1341: val loss: 1.0746219158172607\n",
      "Step 1342: train loss: 1.0999118089675903\n",
      "Step 1342: val loss: 1.0730745792388916\n",
      "Step 1343: train loss: 1.0983511209487915\n",
      "Step 1343: val loss: 1.0715302228927612\n",
      "Step 1344: train loss: 1.0967931747436523\n",
      "Step 1344: val loss: 1.0699883699417114\n",
      "Step 1345: train loss: 1.0952383279800415\n",
      "Step 1345: val loss: 1.0684497356414795\n",
      "Step 1346: train loss: 1.0936863422393799\n",
      "Step 1346: val loss: 1.0669139623641968\n",
      "Step 1347: train loss: 1.0921372175216675\n",
      "Step 1347: val loss: 1.0653810501098633\n",
      "Step 1348: train loss: 1.0905911922454834\n",
      "Step 1348: val loss: 1.063850998878479\n",
      "Step 1349: train loss: 1.0890475511550903\n",
      "Step 1349: val loss: 1.0623236894607544\n",
      "Step 1350: train loss: 1.0875072479248047\n",
      "Step 1350: val loss: 1.0607993602752686\n",
      "Step 1351: train loss: 1.0859694480895996\n",
      "Step 1351: val loss: 1.0592776536941528\n",
      "Step 1352: train loss: 1.084434986114502\n",
      "Step 1352: val loss: 1.0577589273452759\n",
      "Step 1353: train loss: 1.0829027891159058\n",
      "Step 1353: val loss: 1.0562430620193481\n",
      "Step 1354: train loss: 1.081373691558838\n",
      "Step 1354: val loss: 1.0547301769256592\n",
      "Step 1355: train loss: 1.0798475742340088\n",
      "Step 1355: val loss: 1.0532200336456299\n",
      "Step 1356: train loss: 1.0783241987228394\n",
      "Step 1356: val loss: 1.0517126321792603\n",
      "Step 1357: train loss: 1.07680344581604\n",
      "Step 1357: val loss: 1.0502077341079712\n",
      "Step 1358: train loss: 1.0752859115600586\n",
      "Step 1358: val loss: 1.048706293106079\n",
      "Step 1359: train loss: 1.0737708806991577\n",
      "Step 1359: val loss: 1.0472071170806885\n",
      "Step 1360: train loss: 1.0722585916519165\n",
      "Step 1360: val loss: 1.045710802078247\n",
      "Step 1361: train loss: 1.0707491636276245\n",
      "Step 1361: val loss: 1.0442177057266235\n",
      "Step 1362: train loss: 1.0692427158355713\n",
      "Step 1362: val loss: 1.0427272319793701\n",
      "Step 1363: train loss: 1.0677391290664673\n",
      "Step 1363: val loss: 1.0412391424179077\n",
      "Step 1364: train loss: 1.0662381649017334\n",
      "Step 1364: val loss: 1.0397541522979736\n",
      "Step 1365: train loss: 1.0647399425506592\n",
      "Step 1365: val loss: 1.0382720232009888\n",
      "Step 1366: train loss: 1.0632447004318237\n",
      "Step 1366: val loss: 1.036792278289795\n",
      "Step 1367: train loss: 1.0617520809173584\n",
      "Step 1367: val loss: 1.0353156328201294\n",
      "Step 1368: train loss: 1.0602622032165527\n",
      "Step 1368: val loss: 1.033841848373413\n",
      "Step 1369: train loss: 1.0587751865386963\n",
      "Step 1369: val loss: 1.0323705673217773\n",
      "Step 1370: train loss: 1.057291030883789\n",
      "Step 1370: val loss: 1.0309021472930908\n",
      "Step 1371: train loss: 1.055809497833252\n",
      "Step 1371: val loss: 1.0294363498687744\n",
      "Step 1372: train loss: 1.0543307065963745\n",
      "Step 1372: val loss: 1.0279735326766968\n",
      "Step 1373: train loss: 1.0528548955917358\n",
      "Step 1373: val loss: 1.0265133380889893\n",
      "Step 1374: train loss: 1.0513814687728882\n",
      "Step 1374: val loss: 1.0250557661056519\n",
      "Step 1375: train loss: 1.0499110221862793\n",
      "Step 1375: val loss: 1.0236009359359741\n",
      "Step 1376: train loss: 1.048443078994751\n",
      "Step 1376: val loss: 1.0221487283706665\n",
      "Step 1377: train loss: 1.0469778776168823\n",
      "Step 1377: val loss: 1.0206997394561768\n",
      "Step 1378: train loss: 1.0455156564712524\n",
      "Step 1378: val loss: 1.0192527770996094\n",
      "Step 1379: train loss: 1.0440558195114136\n",
      "Step 1379: val loss: 1.0178087949752808\n",
      "Step 1380: train loss: 1.0425989627838135\n",
      "Step 1380: val loss: 1.0163676738739014\n",
      "Step 1381: train loss: 1.041144609451294\n",
      "Step 1381: val loss: 1.0149290561676025\n",
      "Step 1382: train loss: 1.039692997932434\n",
      "Step 1382: val loss: 1.0134930610656738\n",
      "Step 1383: train loss: 1.0382442474365234\n",
      "Step 1383: val loss: 1.0120600461959839\n",
      "Step 1384: train loss: 1.0367982387542725\n",
      "Step 1384: val loss: 1.010629415512085\n",
      "Step 1385: train loss: 1.0353546142578125\n",
      "Step 1385: val loss: 1.0092018842697144\n",
      "Step 1386: train loss: 1.0339139699935913\n",
      "Step 1386: val loss: 1.0077762603759766\n",
      "Step 1387: train loss: 1.0324757099151611\n",
      "Step 1387: val loss: 1.006353735923767\n",
      "Step 1388: train loss: 1.0310403108596802\n",
      "Step 1388: val loss: 1.0049340724945068\n",
      "Step 1389: train loss: 1.0296075344085693\n",
      "Step 1389: val loss: 1.0035167932510376\n",
      "Step 1390: train loss: 1.0281773805618286\n",
      "Step 1390: val loss: 1.002102255821228\n",
      "Step 1391: train loss: 1.0267499685287476\n",
      "Step 1391: val loss: 1.0006903409957886\n",
      "Step 1392: train loss: 1.025325059890747\n",
      "Step 1392: val loss: 0.9992810487747192\n",
      "Step 1393: train loss: 1.0239031314849854\n",
      "Step 1393: val loss: 0.99787437915802\n",
      "Step 1394: train loss: 1.022483468055725\n",
      "Step 1394: val loss: 0.9964702725410461\n",
      "Step 1395: train loss: 1.0210665464401245\n",
      "Step 1395: val loss: 0.9950690269470215\n",
      "Step 1396: train loss: 1.0196524858474731\n",
      "Step 1396: val loss: 0.9936701655387878\n",
      "Step 1397: train loss: 1.0182406902313232\n",
      "Step 1397: val loss: 0.9922740459442139\n",
      "Step 1398: train loss: 1.0168317556381226\n",
      "Step 1398: val loss: 0.9908803701400757\n",
      "Step 1399: train loss: 1.015425205230713\n",
      "Step 1399: val loss: 0.9894893765449524\n",
      "Step 1400: train loss: 1.0140215158462524\n",
      "Step 1400: val loss: 0.9881010055541992\n",
      "Step 1401: train loss: 1.012620210647583\n",
      "Step 1401: val loss: 0.9867150187492371\n",
      "Step 1402: train loss: 1.0112214088439941\n",
      "Step 1402: val loss: 0.9853318929672241\n",
      "Step 1403: train loss: 1.0098252296447754\n",
      "Step 1403: val loss: 0.9839511513710022\n",
      "Step 1404: train loss: 1.0084317922592163\n",
      "Step 1404: val loss: 0.9825733304023743\n",
      "Step 1405: train loss: 1.007041096687317\n",
      "Step 1405: val loss: 0.9811975955963135\n",
      "Step 1406: train loss: 1.0056527853012085\n",
      "Step 1406: val loss: 0.979824960231781\n",
      "Step 1407: train loss: 1.0042670965194702\n",
      "Step 1407: val loss: 0.9784543514251709\n",
      "Step 1408: train loss: 1.002884030342102\n",
      "Step 1408: val loss: 0.9770867824554443\n",
      "Step 1409: train loss: 1.0015034675598145\n",
      "Step 1409: val loss: 0.9757214188575745\n",
      "Step 1410: train loss: 1.000125527381897\n",
      "Step 1410: val loss: 0.9743589758872986\n",
      "Step 1411: train loss: 0.9987501502037048\n",
      "Step 1411: val loss: 0.9729987978935242\n",
      "Step 1412: train loss: 0.9973772168159485\n",
      "Step 1412: val loss: 0.9716412425041199\n",
      "Step 1413: train loss: 0.9960068464279175\n",
      "Step 1413: val loss: 0.9702861905097961\n",
      "Step 1414: train loss: 0.9946390390396118\n",
      "Step 1414: val loss: 0.968933641910553\n",
      "Step 1415: train loss: 0.9932738542556763\n",
      "Step 1415: val loss: 0.9675835371017456\n",
      "Step 1416: train loss: 0.9919108748435974\n",
      "Step 1416: val loss: 0.9662363529205322\n",
      "Step 1417: train loss: 0.9905508756637573\n",
      "Step 1417: val loss: 0.9648913145065308\n",
      "Step 1418: train loss: 0.9891932010650635\n",
      "Step 1418: val loss: 0.9635486006736755\n",
      "Step 1419: train loss: 0.9878377914428711\n",
      "Step 1419: val loss: 0.9622087478637695\n",
      "Step 1420: train loss: 0.9864852428436279\n",
      "Step 1420: val loss: 0.9608711004257202\n",
      "Step 1421: train loss: 0.9851349592208862\n",
      "Step 1421: val loss: 0.9595359563827515\n",
      "Step 1422: train loss: 0.9837871789932251\n",
      "Step 1422: val loss: 0.9582034945487976\n",
      "Step 1423: train loss: 0.9824419617652893\n",
      "Step 1423: val loss: 0.9568734765052795\n",
      "Step 1424: train loss: 0.9810993671417236\n",
      "Step 1424: val loss: 0.9555461406707764\n",
      "Step 1425: train loss: 0.9797590374946594\n",
      "Step 1425: val loss: 0.9542210102081299\n",
      "Step 1426: train loss: 0.9784213304519653\n",
      "Step 1426: val loss: 0.9528984427452087\n",
      "Step 1427: train loss: 0.977086067199707\n",
      "Step 1427: val loss: 0.9515781998634338\n",
      "Step 1428: train loss: 0.975753128528595\n",
      "Step 1428: val loss: 0.950260579586029\n",
      "Step 1429: train loss: 0.9744228720664978\n",
      "Step 1429: val loss: 0.9489455223083496\n",
      "Step 1430: train loss: 0.9730950593948364\n",
      "Step 1430: val loss: 0.9476326704025269\n",
      "Step 1431: train loss: 0.9717695713043213\n",
      "Step 1431: val loss: 0.9463223814964294\n",
      "Step 1432: train loss: 0.9704465866088867\n",
      "Step 1432: val loss: 0.9450145959854126\n",
      "Step 1433: train loss: 0.9691263437271118\n",
      "Step 1433: val loss: 0.9437092542648315\n",
      "Step 1434: train loss: 0.967808187007904\n",
      "Step 1434: val loss: 0.9424059391021729\n",
      "Step 1435: train loss: 0.9664924740791321\n",
      "Step 1435: val loss: 0.9411056041717529\n",
      "Step 1436: train loss: 0.9651793837547302\n",
      "Step 1436: val loss: 0.9398074150085449\n",
      "Step 1437: train loss: 0.9638686776161194\n",
      "Step 1437: val loss: 0.9385119676589966\n",
      "Step 1438: train loss: 0.9625604152679443\n",
      "Step 1438: val loss: 0.9372184872627258\n",
      "Step 1439: train loss: 0.9612544775009155\n",
      "Step 1439: val loss: 0.9359275102615356\n",
      "Step 1440: train loss: 0.9599509835243225\n",
      "Step 1440: val loss: 0.9346393346786499\n",
      "Step 1441: train loss: 0.9586500525474548\n",
      "Step 1441: val loss: 0.9333531856536865\n",
      "Step 1442: train loss: 0.9573513865470886\n",
      "Step 1442: val loss: 0.9320694208145142\n",
      "Step 1443: train loss: 0.9560548663139343\n",
      "Step 1443: val loss: 0.9307882785797119\n",
      "Step 1444: train loss: 0.9547612071037292\n",
      "Step 1444: val loss: 0.9295095205307007\n",
      "Step 1445: train loss: 0.9534696936607361\n",
      "Step 1445: val loss: 0.9282327890396118\n",
      "Step 1446: train loss: 0.9521803855895996\n",
      "Step 1446: val loss: 0.9269587993621826\n",
      "Step 1447: train loss: 0.9508938193321228\n",
      "Step 1447: val loss: 0.9256868958473206\n",
      "Step 1448: train loss: 0.9496094584465027\n",
      "Step 1448: val loss: 0.9244178533554077\n",
      "Step 1449: train loss: 0.9483276009559631\n",
      "Step 1449: val loss: 0.9231507778167725\n",
      "Step 1450: train loss: 0.9470480680465698\n",
      "Step 1450: val loss: 0.9218859672546387\n",
      "Step 1451: train loss: 0.9457708597183228\n",
      "Step 1451: val loss: 0.9206239581108093\n",
      "Step 1452: train loss: 0.9444961547851562\n",
      "Step 1452: val loss: 0.9193640351295471\n",
      "Step 1453: train loss: 0.9432237148284912\n",
      "Step 1453: val loss: 0.9181065559387207\n",
      "Step 1454: train loss: 0.9419535398483276\n",
      "Step 1454: val loss: 0.9168511033058167\n",
      "Step 1455: train loss: 0.9406857490539551\n",
      "Step 1455: val loss: 0.9155982732772827\n",
      "Step 1456: train loss: 0.939420223236084\n",
      "Step 1456: val loss: 0.914347767829895\n",
      "Step 1457: train loss: 0.9381570816040039\n",
      "Step 1457: val loss: 0.9130993485450745\n",
      "Step 1458: train loss: 0.9368963241577148\n",
      "Step 1458: val loss: 0.9118536114692688\n",
      "Step 1459: train loss: 0.9356379508972168\n",
      "Step 1459: val loss: 0.910610020160675\n",
      "Step 1460: train loss: 0.934381902217865\n",
      "Step 1460: val loss: 0.9093686938285828\n",
      "Step 1461: train loss: 0.9331279993057251\n",
      "Step 1461: val loss: 0.908129870891571\n",
      "Step 1462: train loss: 0.9318767189979553\n",
      "Step 1462: val loss: 0.9068933129310608\n",
      "Step 1463: train loss: 0.9306275248527527\n",
      "Step 1463: val loss: 0.905659019947052\n",
      "Step 1464: train loss: 0.9293806552886963\n",
      "Step 1464: val loss: 0.9044268727302551\n",
      "Step 1465: train loss: 0.9281361103057861\n",
      "Step 1465: val loss: 0.903197169303894\n",
      "Step 1466: train loss: 0.9268937706947327\n",
      "Step 1466: val loss: 0.901969850063324\n",
      "Step 1467: train loss: 0.9256540536880493\n",
      "Step 1467: val loss: 0.9007444977760315\n",
      "Step 1468: train loss: 0.9244162440299988\n",
      "Step 1468: val loss: 0.8995216488838196\n",
      "Step 1469: train loss: 0.9231808185577393\n",
      "Step 1469: val loss: 0.8983010053634644\n",
      "Step 1470: train loss: 0.9219478368759155\n",
      "Step 1470: val loss: 0.8970829248428345\n",
      "Step 1471: train loss: 0.9207170605659485\n",
      "Step 1471: val loss: 0.8958668112754822\n",
      "Step 1472: train loss: 0.9194884300231934\n",
      "Step 1472: val loss: 0.8946530222892761\n",
      "Step 1473: train loss: 0.9182621836662292\n",
      "Step 1473: val loss: 0.8934415578842163\n",
      "Step 1474: train loss: 0.9170382618904114\n",
      "Step 1474: val loss: 0.892232358455658\n",
      "Step 1475: train loss: 0.9158164262771606\n",
      "Step 1475: val loss: 0.8910252451896667\n",
      "Step 1476: train loss: 0.9145969152450562\n",
      "Step 1476: val loss: 0.8898206353187561\n",
      "Step 1477: train loss: 0.9133795499801636\n",
      "Step 1477: val loss: 0.8886180520057678\n",
      "Step 1478: train loss: 0.9121649265289307\n",
      "Step 1478: val loss: 0.8874179720878601\n",
      "Step 1479: train loss: 0.9109521508216858\n",
      "Step 1479: val loss: 0.8862200379371643\n",
      "Step 1480: train loss: 0.9097415804862976\n",
      "Step 1480: val loss: 0.8850241899490356\n",
      "Step 1481: train loss: 0.9085333347320557\n",
      "Step 1481: val loss: 0.8838304877281189\n",
      "Step 1482: train loss: 0.9073272943496704\n",
      "Step 1482: val loss: 0.8826391696929932\n",
      "Step 1483: train loss: 0.9061235785484314\n",
      "Step 1483: val loss: 0.8814502358436584\n",
      "Step 1484: train loss: 0.9049220681190491\n",
      "Step 1484: val loss: 0.8802632093429565\n",
      "Step 1485: train loss: 0.9037227034568787\n",
      "Step 1485: val loss: 0.8790785074234009\n",
      "Step 1486: train loss: 0.9025253653526306\n",
      "Step 1486: val loss: 0.8778960108757019\n",
      "Step 1487: train loss: 0.9013305306434631\n",
      "Step 1487: val loss: 0.8767159581184387\n",
      "Step 1488: train loss: 0.9001379013061523\n",
      "Step 1488: val loss: 0.8755378127098083\n",
      "Step 1489: train loss: 0.8989473581314087\n",
      "Step 1489: val loss: 0.874362051486969\n",
      "Step 1490: train loss: 0.8977591395378113\n",
      "Step 1490: val loss: 0.8731884360313416\n",
      "Step 1491: train loss: 0.8965731263160706\n",
      "Step 1491: val loss: 0.8720168471336365\n",
      "Step 1492: train loss: 0.8953890204429626\n",
      "Step 1492: val loss: 0.8708473443984985\n",
      "Step 1493: train loss: 0.8942071795463562\n",
      "Step 1493: val loss: 0.8696803450584412\n",
      "Step 1494: train loss: 0.893027663230896\n",
      "Step 1494: val loss: 0.8685154914855957\n",
      "Step 1495: train loss: 0.8918503522872925\n",
      "Step 1495: val loss: 0.8673528432846069\n",
      "Step 1496: train loss: 0.8906752467155457\n",
      "Step 1496: val loss: 0.866192102432251\n",
      "Step 1497: train loss: 0.8895020484924316\n",
      "Step 1497: val loss: 0.8650336861610413\n",
      "Step 1498: train loss: 0.8883312940597534\n",
      "Step 1498: val loss: 0.8638771772384644\n",
      "Step 1499: train loss: 0.8871625661849976\n",
      "Step 1499: val loss: 0.8627233505249023\n",
      "Step 1500: train loss: 0.8859960436820984\n",
      "Step 1500: val loss: 0.8615714311599731\n",
      "Step 1501: train loss: 0.8848316073417664\n",
      "Step 1501: val loss: 0.8604217767715454\n",
      "Step 1502: train loss: 0.8836694359779358\n",
      "Step 1502: val loss: 0.8592739105224609\n",
      "Step 1503: train loss: 0.8825094699859619\n",
      "Step 1503: val loss: 0.8581284880638123\n",
      "Step 1504: train loss: 0.8813516497612\n",
      "Step 1504: val loss: 0.8569848537445068\n",
      "Step 1505: train loss: 0.880195677280426\n",
      "Step 1505: val loss: 0.855843722820282\n",
      "Step 1506: train loss: 0.879041850566864\n",
      "Step 1506: val loss: 0.8547044992446899\n",
      "Step 1507: train loss: 0.8778902292251587\n",
      "Step 1507: val loss: 0.8535674810409546\n",
      "Step 1508: train loss: 0.8767406344413757\n",
      "Step 1508: val loss: 0.8524326682090759\n",
      "Step 1509: train loss: 0.8755935430526733\n",
      "Step 1509: val loss: 0.8512998223304749\n",
      "Step 1510: train loss: 0.8744484782218933\n",
      "Step 1510: val loss: 0.85016930103302\n",
      "Step 1511: train loss: 0.8733055591583252\n",
      "Step 1511: val loss: 0.8490407466888428\n",
      "Step 1512: train loss: 0.8721644282341003\n",
      "Step 1512: val loss: 0.8479143381118774\n",
      "Step 1513: train loss: 0.8710256814956665\n",
      "Step 1513: val loss: 0.8467898368835449\n",
      "Step 1514: train loss: 0.869888961315155\n",
      "Step 1514: val loss: 0.8456677198410034\n",
      "Step 1515: train loss: 0.8687542080879211\n",
      "Step 1515: val loss: 0.8445476293563843\n",
      "Step 1516: train loss: 0.8676217198371887\n",
      "Step 1516: val loss: 0.843429446220398\n",
      "Step 1517: train loss: 0.8664910793304443\n",
      "Step 1517: val loss: 0.8423133492469788\n",
      "Step 1518: train loss: 0.8653626441955566\n",
      "Step 1518: val loss: 0.8411992192268372\n",
      "Step 1519: train loss: 0.8642362952232361\n",
      "Step 1519: val loss: 0.8400874733924866\n",
      "Step 1520: train loss: 0.8631120324134827\n",
      "Step 1520: val loss: 0.8389775156974792\n",
      "Step 1521: train loss: 0.8619896769523621\n",
      "Step 1521: val loss: 0.8378698229789734\n",
      "Step 1522: train loss: 0.8608695268630981\n",
      "Step 1522: val loss: 0.8367642164230347\n",
      "Step 1523: train loss: 0.8597515821456909\n",
      "Step 1523: val loss: 0.8356606364250183\n",
      "Step 1524: train loss: 0.858635663986206\n",
      "Step 1524: val loss: 0.8345592021942139\n",
      "Step 1525: train loss: 0.8575218319892883\n",
      "Step 1525: val loss: 0.8334598541259766\n",
      "Step 1526: train loss: 0.8564101457595825\n",
      "Step 1526: val loss: 0.8323625922203064\n",
      "Step 1527: train loss: 0.8553003668785095\n",
      "Step 1527: val loss: 0.8312671780586243\n",
      "Step 1528: train loss: 0.8541927337646484\n",
      "Step 1528: val loss: 0.8301739692687988\n",
      "Step 1529: train loss: 0.8530870676040649\n",
      "Step 1529: val loss: 0.8290829062461853\n",
      "Step 1530: train loss: 0.8519834876060486\n",
      "Step 1530: val loss: 0.8279935121536255\n",
      "Step 1531: train loss: 0.8508818745613098\n",
      "Step 1531: val loss: 0.8269064426422119\n",
      "Step 1532: train loss: 0.8497823476791382\n",
      "Step 1532: val loss: 0.8258211612701416\n",
      "Step 1533: train loss: 0.8486848473548889\n",
      "Step 1533: val loss: 0.824738085269928\n",
      "Step 1534: train loss: 0.8475892543792725\n",
      "Step 1534: val loss: 0.8236569166183472\n",
      "Step 1535: train loss: 0.8464958071708679\n",
      "Step 1535: val loss: 0.8225778341293335\n",
      "Step 1536: train loss: 0.8454045057296753\n",
      "Step 1536: val loss: 0.8215008974075317\n",
      "Step 1537: train loss: 0.8443148136138916\n",
      "Step 1537: val loss: 0.8204257488250732\n",
      "Step 1538: train loss: 0.8432275652885437\n",
      "Step 1538: val loss: 0.8193526864051819\n",
      "Step 1539: train loss: 0.8421421051025391\n",
      "Step 1539: val loss: 0.8182814717292786\n",
      "Step 1540: train loss: 0.8410586714744568\n",
      "Step 1540: val loss: 0.8172125220298767\n",
      "Step 1541: train loss: 0.8399771451950073\n",
      "Step 1541: val loss: 0.8161454200744629\n",
      "Step 1542: train loss: 0.8388978242874146\n",
      "Step 1542: val loss: 0.815080463886261\n",
      "Step 1543: train loss: 0.8378202319145203\n",
      "Step 1543: val loss: 0.8140173554420471\n",
      "Step 1544: train loss: 0.8367449045181274\n",
      "Step 1544: val loss: 0.8129563331604004\n",
      "Step 1545: train loss: 0.8356714844703674\n",
      "Step 1545: val loss: 0.8118972182273865\n",
      "Step 1546: train loss: 0.8345999717712402\n",
      "Step 1546: val loss: 0.8108398914337158\n",
      "Step 1547: train loss: 0.8335302472114563\n",
      "Step 1547: val loss: 0.8097846508026123\n",
      "Step 1548: train loss: 0.8324625492095947\n",
      "Step 1548: val loss: 0.8087313175201416\n",
      "Step 1549: train loss: 0.8313966989517212\n",
      "Step 1549: val loss: 0.8076798319816589\n",
      "Step 1550: train loss: 0.8303329944610596\n",
      "Step 1550: val loss: 0.8066304326057434\n",
      "Step 1551: train loss: 0.8292713761329651\n",
      "Step 1551: val loss: 0.8055828809738159\n",
      "Step 1552: train loss: 0.8282115459442139\n",
      "Step 1552: val loss: 0.8045374751091003\n",
      "Step 1553: train loss: 0.8271535634994507\n",
      "Step 1553: val loss: 0.8034940361976624\n",
      "Step 1554: train loss: 0.8260977864265442\n",
      "Step 1554: val loss: 0.8024523854255676\n",
      "Step 1555: train loss: 0.825043797492981\n",
      "Step 1555: val loss: 0.8014125823974609\n",
      "Step 1556: train loss: 0.8239918351173401\n",
      "Step 1556: val loss: 0.8003751039505005\n",
      "Step 1557: train loss: 0.8229418396949768\n",
      "Step 1557: val loss: 0.7993393540382385\n",
      "Step 1558: train loss: 0.8218937516212463\n",
      "Step 1558: val loss: 0.7983056902885437\n",
      "Step 1559: train loss: 0.8208476305007935\n",
      "Step 1559: val loss: 0.7972738146781921\n",
      "Step 1560: train loss: 0.8198035955429077\n",
      "Step 1560: val loss: 0.7962437868118286\n",
      "Step 1561: train loss: 0.8187609910964966\n",
      "Step 1561: val loss: 0.7952157258987427\n",
      "Step 1562: train loss: 0.8177204728126526\n",
      "Step 1562: val loss: 0.7941895723342896\n",
      "Step 1563: train loss: 0.8166819214820862\n",
      "Step 1563: val loss: 0.7931651473045349\n",
      "Step 1564: train loss: 0.8156453371047974\n",
      "Step 1564: val loss: 0.7921428680419922\n",
      "Step 1565: train loss: 0.8146106600761414\n",
      "Step 1565: val loss: 0.7911223769187927\n",
      "Step 1566: train loss: 0.8135779500007629\n",
      "Step 1566: val loss: 0.7901038527488708\n",
      "Step 1567: train loss: 0.8125471472740173\n",
      "Step 1567: val loss: 0.7890873551368713\n",
      "Step 1568: train loss: 0.8115183115005493\n",
      "Step 1568: val loss: 0.7880727648735046\n",
      "Step 1569: train loss: 0.8104912638664246\n",
      "Step 1569: val loss: 0.7870599627494812\n",
      "Step 1570: train loss: 0.8094660639762878\n",
      "Step 1570: val loss: 0.7860490083694458\n",
      "Step 1571: train loss: 0.8084425926208496\n",
      "Step 1571: val loss: 0.7850398421287537\n",
      "Step 1572: train loss: 0.8074212074279785\n",
      "Step 1572: val loss: 0.7840326428413391\n",
      "Step 1573: train loss: 0.8064016699790955\n",
      "Step 1573: val loss: 0.7830274105072021\n",
      "Step 1574: train loss: 0.8053840398788452\n",
      "Step 1574: val loss: 0.7820240259170532\n",
      "Step 1575: train loss: 0.8043683767318726\n",
      "Step 1575: val loss: 0.7810226678848267\n",
      "Step 1576: train loss: 0.8033545613288879\n",
      "Step 1576: val loss: 0.7800227999687195\n",
      "Step 1577: train loss: 0.8023424744606018\n",
      "Step 1577: val loss: 0.7790250182151794\n",
      "Step 1578: train loss: 0.8013322949409485\n",
      "Step 1578: val loss: 0.7780289649963379\n",
      "Step 1579: train loss: 0.8003238439559937\n",
      "Step 1579: val loss: 0.7770349383354187\n",
      "Step 1580: train loss: 0.7993174195289612\n",
      "Step 1580: val loss: 0.776042640209198\n",
      "Step 1581: train loss: 0.7983128428459167\n",
      "Step 1581: val loss: 0.7750523090362549\n",
      "Step 1582: train loss: 0.7973101735115051\n",
      "Step 1582: val loss: 0.7740638256072998\n",
      "Step 1583: train loss: 0.7963091135025024\n",
      "Step 1583: val loss: 0.7730768918991089\n",
      "Step 1584: train loss: 0.7953101396560669\n",
      "Step 1584: val loss: 0.7720922231674194\n",
      "Step 1585: train loss: 0.7943128347396851\n",
      "Step 1585: val loss: 0.7711091041564941\n",
      "Step 1586: train loss: 0.7933175563812256\n",
      "Step 1586: val loss: 0.7701278328895569\n",
      "Step 1587: train loss: 0.7923239469528198\n",
      "Step 1587: val loss: 0.7691483497619629\n",
      "Step 1588: train loss: 0.7913321852684021\n",
      "Step 1588: val loss: 0.7681708931922913\n",
      "Step 1589: train loss: 0.7903422713279724\n",
      "Step 1589: val loss: 0.7671949863433838\n",
      "Step 1590: train loss: 0.7893539667129517\n",
      "Step 1590: val loss: 0.7662211060523987\n",
      "Step 1591: train loss: 0.7883678674697876\n",
      "Step 1591: val loss: 0.7652491927146912\n",
      "Step 1592: train loss: 0.787383496761322\n",
      "Step 1592: val loss: 0.7642788887023926\n",
      "Step 1593: train loss: 0.7864006757736206\n",
      "Step 1593: val loss: 0.7633103728294373\n",
      "Step 1594: train loss: 0.785419762134552\n",
      "Step 1594: val loss: 0.7623436450958252\n",
      "Step 1595: train loss: 0.784440815448761\n",
      "Step 1595: val loss: 0.7613787055015564\n",
      "Step 1596: train loss: 0.7834635972976685\n",
      "Step 1596: val loss: 0.76041579246521\n",
      "Step 1597: train loss: 0.7824880480766296\n",
      "Step 1597: val loss: 0.7594542503356934\n",
      "Step 1598: train loss: 0.7815142273902893\n",
      "Step 1598: val loss: 0.7584947347640991\n",
      "Step 1599: train loss: 0.7805424928665161\n",
      "Step 1599: val loss: 0.7575369477272034\n",
      "Step 1600: train loss: 0.7795724868774414\n",
      "Step 1600: val loss: 0.75658118724823\n",
      "Step 1601: train loss: 0.7786043882369995\n",
      "Step 1601: val loss: 0.755626916885376\n",
      "Step 1602: train loss: 0.7776377201080322\n",
      "Step 1602: val loss: 0.7546746730804443\n",
      "Step 1603: train loss: 0.7766729593276978\n",
      "Step 1603: val loss: 0.7537240386009216\n",
      "Step 1604: train loss: 0.7757100462913513\n",
      "Step 1604: val loss: 0.7527753710746765\n",
      "Step 1605: train loss: 0.7747489809989929\n",
      "Step 1605: val loss: 0.7518283128738403\n",
      "Step 1606: train loss: 0.7737895250320435\n",
      "Step 1606: val loss: 0.7508829236030579\n",
      "Step 1607: train loss: 0.772831916809082\n",
      "Step 1607: val loss: 0.749939501285553\n",
      "Step 1608: train loss: 0.7718760371208191\n",
      "Step 1608: val loss: 0.748997688293457\n",
      "Step 1609: train loss: 0.7709218263626099\n",
      "Step 1609: val loss: 0.74805748462677\n",
      "Step 1610: train loss: 0.7699693441390991\n",
      "Step 1610: val loss: 0.7471193075180054\n",
      "Step 1611: train loss: 0.7690186500549316\n",
      "Step 1611: val loss: 0.7461828589439392\n",
      "Step 1612: train loss: 0.7680699229240417\n",
      "Step 1612: val loss: 0.7452479004859924\n",
      "Step 1613: train loss: 0.7671229243278503\n",
      "Step 1613: val loss: 0.7443150281906128\n",
      "Step 1614: train loss: 0.7661774754524231\n",
      "Step 1614: val loss: 0.7433836460113525\n",
      "Step 1615: train loss: 0.7652337551116943\n",
      "Step 1615: val loss: 0.7424542903900146\n",
      "Step 1616: train loss: 0.7642920017242432\n",
      "Step 1616: val loss: 0.7415264844894409\n",
      "Step 1617: train loss: 0.7633517384529114\n",
      "Step 1617: val loss: 0.7406003475189209\n",
      "Step 1618: train loss: 0.7624133229255676\n",
      "Step 1618: val loss: 0.7396762371063232\n",
      "Step 1619: train loss: 0.7614766955375671\n",
      "Step 1619: val loss: 0.7387535572052002\n",
      "Step 1620: train loss: 0.7605418562889099\n",
      "Step 1620: val loss: 0.7378326058387756\n",
      "Step 1621: train loss: 0.759608268737793\n",
      "Step 1621: val loss: 0.7369132041931152\n",
      "Step 1622: train loss: 0.7586766481399536\n",
      "Step 1622: val loss: 0.7359956502914429\n",
      "Step 1623: train loss: 0.757746696472168\n",
      "Step 1623: val loss: 0.735079824924469\n",
      "Step 1624: train loss: 0.7568185925483704\n",
      "Step 1624: val loss: 0.7341657876968384\n",
      "Step 1625: train loss: 0.755892276763916\n",
      "Step 1625: val loss: 0.7332535982131958\n",
      "Step 1626: train loss: 0.7549676299095154\n",
      "Step 1626: val loss: 0.7323426604270935\n",
      "Step 1627: train loss: 0.7540444135665894\n",
      "Step 1627: val loss: 0.731433629989624\n",
      "Step 1628: train loss: 0.753122866153717\n",
      "Step 1628: val loss: 0.7305263876914978\n",
      "Step 1629: train loss: 0.752203106880188\n",
      "Step 1629: val loss: 0.7296205759048462\n",
      "Step 1630: train loss: 0.751285195350647\n",
      "Step 1630: val loss: 0.7287169098854065\n",
      "Step 1631: train loss: 0.7503690123558044\n",
      "Step 1631: val loss: 0.7278146743774414\n",
      "Step 1632: train loss: 0.7494544386863708\n",
      "Step 1632: val loss: 0.7269140481948853\n",
      "Step 1633: train loss: 0.7485413551330566\n",
      "Step 1633: val loss: 0.7260150909423828\n",
      "Step 1634: train loss: 0.7476301193237305\n",
      "Step 1634: val loss: 0.7251178622245789\n",
      "Step 1635: train loss: 0.7467204332351685\n",
      "Step 1635: val loss: 0.7242224216461182\n",
      "Step 1636: train loss: 0.7458125352859497\n",
      "Step 1636: val loss: 0.7233285903930664\n",
      "Step 1637: train loss: 0.7449065446853638\n",
      "Step 1637: val loss: 0.7224366664886475\n",
      "Step 1638: train loss: 0.7440022826194763\n",
      "Step 1638: val loss: 0.721545934677124\n",
      "Step 1639: train loss: 0.74309903383255\n",
      "Step 1639: val loss: 0.7206570506095886\n",
      "Step 1640: train loss: 0.7421977519989014\n",
      "Step 1640: val loss: 0.7197698354721069\n",
      "Step 1641: train loss: 0.7412981986999512\n",
      "Step 1641: val loss: 0.718884289264679\n",
      "Step 1642: train loss: 0.7404004335403442\n",
      "Step 1642: val loss: 0.7180006504058838\n",
      "Step 1643: train loss: 0.739504337310791\n",
      "Step 1643: val loss: 0.7171183824539185\n",
      "Step 1644: train loss: 0.7386094927787781\n",
      "Step 1644: val loss: 0.7162376046180725\n",
      "Step 1645: train loss: 0.7377163767814636\n",
      "Step 1645: val loss: 0.715358555316925\n",
      "Step 1646: train loss: 0.7368251085281372\n",
      "Step 1646: val loss: 0.7144814133644104\n",
      "Step 1647: train loss: 0.7359355092048645\n",
      "Step 1647: val loss: 0.7136058211326599\n",
      "Step 1648: train loss: 0.7350475788116455\n",
      "Step 1648: val loss: 0.7127317190170288\n",
      "Step 1649: train loss: 0.7341610193252563\n",
      "Step 1649: val loss: 0.7118591666221619\n",
      "Step 1650: train loss: 0.7332760691642761\n",
      "Step 1650: val loss: 0.7109883427619934\n",
      "Step 1651: train loss: 0.7323929667472839\n",
      "Step 1651: val loss: 0.7101192474365234\n",
      "Step 1652: train loss: 0.7315114736557007\n",
      "Step 1652: val loss: 0.7092518210411072\n",
      "Step 1653: train loss: 0.7306317687034607\n",
      "Step 1653: val loss: 0.7083858251571655\n",
      "Step 1654: train loss: 0.7297531962394714\n",
      "Step 1654: val loss: 0.7075214385986328\n",
      "Step 1655: train loss: 0.728876531124115\n",
      "Step 1655: val loss: 0.7066587209701538\n",
      "Step 1656: train loss: 0.7280014753341675\n",
      "Step 1656: val loss: 0.7057978510856628\n",
      "Step 1657: train loss: 0.7271280288696289\n",
      "Step 1657: val loss: 0.7049382328987122\n",
      "Step 1658: train loss: 0.7262558937072754\n",
      "Step 1658: val loss: 0.7040800452232361\n",
      "Step 1659: train loss: 0.7253856062889099\n",
      "Step 1659: val loss: 0.7032238245010376\n",
      "Step 1660: train loss: 0.7245168089866638\n",
      "Step 1660: val loss: 0.7023690938949585\n",
      "Step 1661: train loss: 0.7236499190330505\n",
      "Step 1661: val loss: 0.7015160918235779\n",
      "Step 1662: train loss: 0.7227844595909119\n",
      "Step 1662: val loss: 0.7006646394729614\n",
      "Step 1663: train loss: 0.7219204902648926\n",
      "Step 1663: val loss: 0.6998146176338196\n",
      "Step 1664: train loss: 0.7210580706596375\n",
      "Step 1664: val loss: 0.6989662647247314\n",
      "Step 1665: train loss: 0.720197319984436\n",
      "Step 1665: val loss: 0.6981195211410522\n",
      "Step 1666: train loss: 0.7193382978439331\n",
      "Step 1666: val loss: 0.6972742080688477\n",
      "Step 1667: train loss: 0.71848064661026\n",
      "Step 1667: val loss: 0.6964305639266968\n",
      "Step 1668: train loss: 0.7176245450973511\n",
      "Step 1668: val loss: 0.6955885887145996\n",
      "Step 1669: train loss: 0.7167700529098511\n",
      "Step 1669: val loss: 0.6947482228279114\n",
      "Step 1670: train loss: 0.7159172892570496\n",
      "Step 1670: val loss: 0.6939090490341187\n",
      "Step 1671: train loss: 0.7150658369064331\n",
      "Step 1671: val loss: 0.6930716633796692\n",
      "Step 1672: train loss: 0.7142160534858704\n",
      "Step 1672: val loss: 0.6922358274459839\n",
      "Step 1673: train loss: 0.713367760181427\n",
      "Step 1673: val loss: 0.6914017200469971\n",
      "Step 1674: train loss: 0.7125213146209717\n",
      "Step 1674: val loss: 0.6905689239501953\n",
      "Step 1675: train loss: 0.7116759419441223\n",
      "Step 1675: val loss: 0.6897376179695129\n",
      "Step 1676: train loss: 0.7108323574066162\n",
      "Step 1676: val loss: 0.6889081597328186\n",
      "Step 1677: train loss: 0.709990382194519\n",
      "Step 1677: val loss: 0.6880800724029541\n",
      "Step 1678: train loss: 0.7091500163078308\n",
      "Step 1678: val loss: 0.6872534155845642\n",
      "Step 1679: train loss: 0.7083109021186829\n",
      "Step 1679: val loss: 0.6864284873008728\n",
      "Step 1680: train loss: 0.7074735760688782\n",
      "Step 1680: val loss: 0.685604989528656\n",
      "Step 1681: train loss: 0.7066377401351929\n",
      "Step 1681: val loss: 0.6847833395004272\n",
      "Step 1682: train loss: 0.7058035135269165\n",
      "Step 1682: val loss: 0.6839627623558044\n",
      "Step 1683: train loss: 0.7049705982208252\n",
      "Step 1683: val loss: 0.6831439733505249\n",
      "Step 1684: train loss: 0.7041394114494324\n",
      "Step 1684: val loss: 0.6823266744613647\n",
      "Step 1685: train loss: 0.7033097147941589\n",
      "Step 1685: val loss: 0.6815111637115479\n",
      "Step 1686: train loss: 0.702481746673584\n",
      "Step 1686: val loss: 0.6806966662406921\n",
      "Step 1687: train loss: 0.701654851436615\n",
      "Step 1687: val loss: 0.6798840165138245\n",
      "Step 1688: train loss: 0.7008297443389893\n",
      "Step 1688: val loss: 0.6790727972984314\n",
      "Step 1689: train loss: 0.7000062465667725\n",
      "Step 1689: val loss: 0.6782633066177368\n",
      "Step 1690: train loss: 0.699184238910675\n",
      "Step 1690: val loss: 0.6774550080299377\n",
      "Step 1691: train loss: 0.6983634233474731\n",
      "Step 1691: val loss: 0.6766483187675476\n",
      "Step 1692: train loss: 0.6975443363189697\n",
      "Step 1692: val loss: 0.6758431196212769\n",
      "Step 1693: train loss: 0.6967268586158752\n",
      "Step 1693: val loss: 0.6750392913818359\n",
      "Step 1694: train loss: 0.6959106922149658\n",
      "Step 1694: val loss: 0.6742372512817383\n",
      "Step 1695: train loss: 0.6950958967208862\n",
      "Step 1695: val loss: 0.6734365224838257\n",
      "Step 1696: train loss: 0.6942829489707947\n",
      "Step 1696: val loss: 0.6726375222206116\n",
      "Step 1697: train loss: 0.693471372127533\n",
      "Step 1697: val loss: 0.671839714050293\n",
      "Step 1698: train loss: 0.6926612257957458\n",
      "Step 1698: val loss: 0.6710433959960938\n",
      "Step 1699: train loss: 0.6918525099754333\n",
      "Step 1699: val loss: 0.6702487468719482\n",
      "Step 1700: train loss: 0.6910455226898193\n",
      "Step 1700: val loss: 0.669455349445343\n",
      "Step 1701: train loss: 0.6902396082878113\n",
      "Step 1701: val loss: 0.668663501739502\n",
      "Step 1702: train loss: 0.6894353032112122\n",
      "Step 1702: val loss: 0.6678732633590698\n",
      "Step 1703: train loss: 0.6886326670646667\n",
      "Step 1703: val loss: 0.6670846939086914\n",
      "Step 1704: train loss: 0.6878315806388855\n",
      "Step 1704: val loss: 0.6662971377372742\n",
      "Step 1705: train loss: 0.6870318055152893\n",
      "Step 1705: val loss: 0.6655112504959106\n",
      "Step 1706: train loss: 0.6862334609031677\n",
      "Step 1706: val loss: 0.664726972579956\n",
      "Step 1707: train loss: 0.6854368448257446\n",
      "Step 1707: val loss: 0.6639439463615417\n",
      "Step 1708: train loss: 0.6846411824226379\n",
      "Step 1708: val loss: 0.6631622910499573\n",
      "Step 1709: train loss: 0.6838473081588745\n",
      "Step 1709: val loss: 0.6623824238777161\n",
      "Step 1710: train loss: 0.6830549240112305\n",
      "Step 1710: val loss: 0.6616036891937256\n",
      "Step 1711: train loss: 0.6822636127471924\n",
      "Step 1711: val loss: 0.660826563835144\n",
      "Step 1712: train loss: 0.6814740896224976\n",
      "Step 1712: val loss: 0.6600509285926819\n",
      "Step 1713: train loss: 0.6806860566139221\n",
      "Step 1713: val loss: 0.6592764854431152\n",
      "Step 1714: train loss: 0.6798990964889526\n",
      "Step 1714: val loss: 0.6585036516189575\n",
      "Step 1715: train loss: 0.6791138052940369\n",
      "Step 1715: val loss: 0.6577323079109192\n",
      "Step 1716: train loss: 0.6783300042152405\n",
      "Step 1716: val loss: 0.6569626331329346\n",
      "Step 1717: train loss: 0.6775479316711426\n",
      "Step 1717: val loss: 0.6561939120292664\n",
      "Step 1718: train loss: 0.6767667531967163\n",
      "Step 1718: val loss: 0.6554268598556519\n",
      "Step 1719: train loss: 0.6759873628616333\n",
      "Step 1719: val loss: 0.6546614170074463\n",
      "Step 1720: train loss: 0.6752094030380249\n",
      "Step 1720: val loss: 0.6538971662521362\n",
      "Step 1721: train loss: 0.674432635307312\n",
      "Step 1721: val loss: 0.653134286403656\n",
      "Step 1722: train loss: 0.6736574172973633\n",
      "Step 1722: val loss: 0.6523729562759399\n",
      "Step 1723: train loss: 0.6728838086128235\n",
      "Step 1723: val loss: 0.651613175868988\n",
      "Step 1724: train loss: 0.6721112728118896\n",
      "Step 1724: val loss: 0.6508545875549316\n",
      "Step 1725: train loss: 0.67134028673172\n",
      "Step 1725: val loss: 0.6500975489616394\n",
      "Step 1726: train loss: 0.6705709099769592\n",
      "Step 1726: val loss: 0.6493418216705322\n",
      "Step 1727: train loss: 0.6698026657104492\n",
      "Step 1727: val loss: 0.6485875844955444\n",
      "Step 1728: train loss: 0.6690359115600586\n",
      "Step 1728: val loss: 0.647834837436676\n",
      "Step 1729: train loss: 0.6682707667350769\n",
      "Step 1729: val loss: 0.6470832228660583\n",
      "Step 1730: train loss: 0.6675066351890564\n",
      "Step 1730: val loss: 0.6463332772254944\n",
      "Step 1731: train loss: 0.6667441725730896\n",
      "Step 1731: val loss: 0.6455846428871155\n",
      "Step 1732: train loss: 0.6659831404685974\n",
      "Step 1732: val loss: 0.6448373198509216\n",
      "Step 1733: train loss: 0.6652233600616455\n",
      "Step 1733: val loss: 0.6440915465354919\n",
      "Step 1734: train loss: 0.6644651293754578\n",
      "Step 1734: val loss: 0.6433470845222473\n",
      "Step 1735: train loss: 0.6637083292007446\n",
      "Step 1735: val loss: 0.6426039338111877\n",
      "Step 1736: train loss: 0.6629526019096375\n",
      "Step 1736: val loss: 0.6418622732162476\n",
      "Step 1737: train loss: 0.6621983647346497\n",
      "Step 1737: val loss: 0.6411215662956238\n",
      "Step 1738: train loss: 0.6614454388618469\n",
      "Step 1738: val loss: 0.6403826475143433\n",
      "Step 1739: train loss: 0.6606939435005188\n",
      "Step 1739: val loss: 0.6396451592445374\n",
      "Step 1740: train loss: 0.6599439382553101\n",
      "Step 1740: val loss: 0.6389086842536926\n",
      "Step 1741: train loss: 0.659195065498352\n",
      "Step 1741: val loss: 0.6381738185882568\n",
      "Step 1742: train loss: 0.6584476232528687\n",
      "Step 1742: val loss: 0.6374405026435852\n",
      "Step 1743: train loss: 0.6577017903327942\n",
      "Step 1743: val loss: 0.63670814037323\n",
      "Step 1744: train loss: 0.6569570899009705\n",
      "Step 1744: val loss: 0.6359773278236389\n",
      "Step 1745: train loss: 0.6562139987945557\n",
      "Step 1745: val loss: 0.6352481842041016\n",
      "Step 1746: train loss: 0.6554721593856812\n",
      "Step 1746: val loss: 0.6345200538635254\n",
      "Step 1747: train loss: 0.6547315120697021\n",
      "Step 1747: val loss: 0.633793294429779\n",
      "Step 1748: train loss: 0.6539923548698425\n",
      "Step 1748: val loss: 0.6330682635307312\n",
      "Step 1749: train loss: 0.6532548666000366\n",
      "Step 1749: val loss: 0.6323442459106445\n",
      "Step 1750: train loss: 0.6525183320045471\n",
      "Step 1750: val loss: 0.6316217184066772\n",
      "Step 1751: train loss: 0.6517833471298218\n",
      "Step 1751: val loss: 0.6309001445770264\n",
      "Step 1752: train loss: 0.6510493755340576\n",
      "Step 1752: val loss: 0.6301802396774292\n",
      "Step 1753: train loss: 0.6503168940544128\n",
      "Step 1753: val loss: 0.6294617652893066\n",
      "Step 1754: train loss: 0.649586021900177\n",
      "Step 1754: val loss: 0.6287444829940796\n",
      "Step 1755: train loss: 0.6488562226295471\n",
      "Step 1755: val loss: 0.6280285716056824\n",
      "Step 1756: train loss: 0.6481276750564575\n",
      "Step 1756: val loss: 0.6273141503334045\n",
      "Step 1757: train loss: 0.6474008560180664\n",
      "Step 1757: val loss: 0.626600980758667\n",
      "Step 1758: train loss: 0.646675169467926\n",
      "Step 1758: val loss: 0.625889003276825\n",
      "Step 1759: train loss: 0.6459506750106812\n",
      "Step 1759: val loss: 0.6251782774925232\n",
      "Step 1760: train loss: 0.6452275514602661\n",
      "Step 1760: val loss: 0.6244690418243408\n",
      "Step 1761: train loss: 0.6445057988166809\n",
      "Step 1761: val loss: 0.6237611174583435\n",
      "Step 1762: train loss: 0.6437855362892151\n",
      "Step 1762: val loss: 0.623054563999176\n",
      "Step 1763: train loss: 0.6430662870407104\n",
      "Step 1763: val loss: 0.6223492622375488\n",
      "Step 1764: train loss: 0.6423485279083252\n",
      "Step 1764: val loss: 0.6216452121734619\n",
      "Step 1765: train loss: 0.6416319608688354\n",
      "Step 1765: val loss: 0.6209425330162048\n",
      "Step 1766: train loss: 0.640916645526886\n",
      "Step 1766: val loss: 0.620241105556488\n",
      "Step 1767: train loss: 0.6402029991149902\n",
      "Step 1767: val loss: 0.6195409893989563\n",
      "Step 1768: train loss: 0.6394903063774109\n",
      "Step 1768: val loss: 0.6188422441482544\n",
      "Step 1769: train loss: 0.6387791037559509\n",
      "Step 1769: val loss: 0.6181446313858032\n",
      "Step 1770: train loss: 0.6380689740180969\n",
      "Step 1770: val loss: 0.6174485087394714\n",
      "Step 1771: train loss: 0.6373602747917175\n",
      "Step 1771: val loss: 0.6167536377906799\n",
      "Step 1772: train loss: 0.6366530656814575\n",
      "Step 1772: val loss: 0.6160601377487183\n",
      "Step 1773: train loss: 0.6359469890594482\n",
      "Step 1773: val loss: 0.6153680086135864\n",
      "Step 1774: train loss: 0.6352422833442688\n",
      "Step 1774: val loss: 0.6146767735481262\n",
      "Step 1775: train loss: 0.6345385909080505\n",
      "Step 1775: val loss: 0.613987147808075\n",
      "Step 1776: train loss: 0.6338363885879517\n",
      "Step 1776: val loss: 0.6132987141609192\n",
      "Step 1777: train loss: 0.6331355571746826\n",
      "Step 1777: val loss: 0.6126115322113037\n",
      "Step 1778: train loss: 0.6324358582496643\n",
      "Step 1778: val loss: 0.6119257807731628\n",
      "Step 1779: train loss: 0.6317375302314758\n",
      "Step 1779: val loss: 0.6112411022186279\n",
      "Step 1780: train loss: 0.6310403943061829\n",
      "Step 1780: val loss: 0.6105577945709229\n",
      "Step 1781: train loss: 0.6303446292877197\n",
      "Step 1781: val loss: 0.6098759770393372\n",
      "Step 1782: train loss: 0.6296502947807312\n",
      "Step 1782: val loss: 0.6091952919960022\n",
      "Step 1783: train loss: 0.6289569735527039\n",
      "Step 1783: val loss: 0.6085159182548523\n",
      "Step 1784: train loss: 0.6282650828361511\n",
      "Step 1784: val loss: 0.6078376173973083\n",
      "Step 1785: train loss: 0.6275743246078491\n",
      "Step 1785: val loss: 0.607160747051239\n",
      "Step 1786: train loss: 0.6268848776817322\n",
      "Step 1786: val loss: 0.6064848303794861\n",
      "Step 1787: train loss: 0.6261965036392212\n",
      "Step 1787: val loss: 0.6058104038238525\n",
      "Step 1788: train loss: 0.6255096197128296\n",
      "Step 1788: val loss: 0.6051373481750488\n",
      "Step 1789: train loss: 0.624824047088623\n",
      "Step 1789: val loss: 0.6044653654098511\n",
      "Step 1790: train loss: 0.624139666557312\n",
      "Step 1790: val loss: 0.6037948727607727\n",
      "Step 1791: train loss: 0.6234564781188965\n",
      "Step 1791: val loss: 0.6031253933906555\n",
      "Step 1792: train loss: 0.6227743625640869\n",
      "Step 1792: val loss: 0.6024572253227234\n",
      "Step 1793: train loss: 0.6220938563346863\n",
      "Step 1793: val loss: 0.6017900705337524\n",
      "Step 1794: train loss: 0.6214142441749573\n",
      "Step 1794: val loss: 0.60112464427948\n",
      "Step 1795: train loss: 0.6207360625267029\n",
      "Step 1795: val loss: 0.6004602313041687\n",
      "Step 1796: train loss: 0.6200593113899231\n",
      "Step 1796: val loss: 0.5997970700263977\n",
      "Step 1797: train loss: 0.6193835735321045\n",
      "Step 1797: val loss: 0.5991353392601013\n",
      "Step 1798: train loss: 0.6187092065811157\n",
      "Step 1798: val loss: 0.598474383354187\n",
      "Step 1799: train loss: 0.6180359721183777\n",
      "Step 1799: val loss: 0.5978150963783264\n",
      "Step 1800: train loss: 0.6173639297485352\n",
      "Step 1800: val loss: 0.5971567630767822\n",
      "Step 1801: train loss: 0.6166930794715881\n",
      "Step 1801: val loss: 0.5964997410774231\n",
      "Step 1802: train loss: 0.616023600101471\n",
      "Step 1802: val loss: 0.5958437323570251\n",
      "Step 1803: train loss: 0.6153550744056702\n",
      "Step 1803: val loss: 0.5951892137527466\n",
      "Step 1804: train loss: 0.614687979221344\n",
      "Step 1804: val loss: 0.5945359468460083\n",
      "Step 1805: train loss: 0.6140221953392029\n",
      "Step 1805: val loss: 0.5938838124275208\n",
      "Step 1806: train loss: 0.6133575439453125\n",
      "Step 1806: val loss: 0.5932329893112183\n",
      "Step 1807: train loss: 0.6126942038536072\n",
      "Step 1807: val loss: 0.592583179473877\n",
      "Step 1808: train loss: 0.6120319366455078\n",
      "Step 1808: val loss: 0.5919348001480103\n",
      "Step 1809: train loss: 0.6113709211349487\n",
      "Step 1809: val loss: 0.5912874937057495\n",
      "Step 1810: train loss: 0.6107110977172852\n",
      "Step 1810: val loss: 0.5906414985656738\n",
      "Step 1811: train loss: 0.6100525856018066\n",
      "Step 1811: val loss: 0.5899964570999146\n",
      "Step 1812: train loss: 0.6093950271606445\n",
      "Step 1812: val loss: 0.5893528461456299\n",
      "Step 1813: train loss: 0.6087388396263123\n",
      "Step 1813: val loss: 0.5887101888656616\n",
      "Step 1814: train loss: 0.6080836057662964\n",
      "Step 1814: val loss: 0.5880689024925232\n",
      "Step 1815: train loss: 0.6074297428131104\n",
      "Step 1815: val loss: 0.5874289870262146\n",
      "Step 1816: train loss: 0.6067774295806885\n",
      "Step 1816: val loss: 0.586790144443512\n",
      "Step 1817: train loss: 0.6061259508132935\n",
      "Step 1817: val loss: 0.5861526131629944\n",
      "Step 1818: train loss: 0.605475902557373\n",
      "Step 1818: val loss: 0.5855159759521484\n",
      "Step 1819: train loss: 0.6048269867897034\n",
      "Step 1819: val loss: 0.5848809480667114\n",
      "Step 1820: train loss: 0.6041792035102844\n",
      "Step 1820: val loss: 0.5842466950416565\n",
      "Step 1821: train loss: 0.6035324335098267\n",
      "Step 1821: val loss: 0.583613932132721\n",
      "Step 1822: train loss: 0.6028870344161987\n",
      "Step 1822: val loss: 0.5829821228981018\n",
      "Step 1823: train loss: 0.602242648601532\n",
      "Step 1823: val loss: 0.5823516249656677\n",
      "Step 1824: train loss: 0.6015996932983398\n",
      "Step 1824: val loss: 0.5817221403121948\n",
      "Step 1825: train loss: 0.6009576320648193\n",
      "Step 1825: val loss: 0.5810939073562622\n",
      "Step 1826: train loss: 0.6003170013427734\n",
      "Step 1826: val loss: 0.5804668068885803\n",
      "Step 1827: train loss: 0.5996772646903992\n",
      "Step 1827: val loss: 0.5798410773277283\n",
      "Step 1828: train loss: 0.5990389585494995\n",
      "Step 1828: val loss: 0.5792162418365479\n",
      "Step 1829: train loss: 0.5984014272689819\n",
      "Step 1829: val loss: 0.5785927176475525\n",
      "Step 1830: train loss: 0.5977653861045837\n",
      "Step 1830: val loss: 0.5779703259468079\n",
      "Step 1831: train loss: 0.5971303582191467\n",
      "Step 1831: val loss: 0.577349066734314\n",
      "Step 1832: train loss: 0.59649658203125\n",
      "Step 1832: val loss: 0.576728880405426\n",
      "Step 1833: train loss: 0.5958638787269592\n",
      "Step 1833: val loss: 0.5761099457740784\n",
      "Step 1834: train loss: 0.5952324271202087\n",
      "Step 1834: val loss: 0.5754921436309814\n",
      "Step 1835: train loss: 0.5946020483970642\n",
      "Step 1835: val loss: 0.5748755931854248\n",
      "Step 1836: train loss: 0.59397292137146\n",
      "Step 1836: val loss: 0.5742599964141846\n",
      "Step 1837: train loss: 0.5933447480201721\n",
      "Step 1837: val loss: 0.5736457109451294\n",
      "Step 1838: train loss: 0.5927179455757141\n",
      "Step 1838: val loss: 0.5730324983596802\n",
      "Step 1839: train loss: 0.5920920968055725\n",
      "Step 1839: val loss: 0.5724204182624817\n",
      "Step 1840: train loss: 0.5914674997329712\n",
      "Step 1840: val loss: 0.5718094110488892\n",
      "Step 1841: train loss: 0.590843915939331\n",
      "Step 1841: val loss: 0.5711997151374817\n",
      "Step 1842: train loss: 0.5902217030525208\n",
      "Step 1842: val loss: 0.5705910325050354\n",
      "Step 1843: train loss: 0.5896004438400269\n",
      "Step 1843: val loss: 0.5699836611747742\n",
      "Step 1844: train loss: 0.5889804363250732\n",
      "Step 1844: val loss: 0.569377064704895\n",
      "Step 1845: train loss: 0.588361382484436\n",
      "Step 1845: val loss: 0.5687718987464905\n",
      "Step 1846: train loss: 0.5877436995506287\n",
      "Step 1846: val loss: 0.5681678056716919\n",
      "Step 1847: train loss: 0.5871268510818481\n",
      "Step 1847: val loss: 0.567564845085144\n",
      "Step 1848: train loss: 0.5865114331245422\n",
      "Step 1848: val loss: 0.5669630169868469\n",
      "Step 1849: train loss: 0.5858969688415527\n",
      "Step 1849: val loss: 0.5663623809814453\n",
      "Step 1850: train loss: 0.5852837562561035\n",
      "Step 1850: val loss: 0.5657628178596497\n",
      "Step 1851: train loss: 0.5846714973449707\n",
      "Step 1851: val loss: 0.5651642680168152\n",
      "Step 1852: train loss: 0.584060549736023\n",
      "Step 1852: val loss: 0.5645669102668762\n",
      "Step 1853: train loss: 0.5834505558013916\n",
      "Step 1853: val loss: 0.5639706254005432\n",
      "Step 1854: train loss: 0.5828419923782349\n",
      "Step 1854: val loss: 0.5633755922317505\n",
      "Step 1855: train loss: 0.5822341442108154\n",
      "Step 1855: val loss: 0.5627816319465637\n",
      "Step 1856: train loss: 0.5816276669502258\n",
      "Step 1856: val loss: 0.5621888041496277\n",
      "Step 1857: train loss: 0.5810220837593079\n",
      "Step 1857: val loss: 0.5615970492362976\n",
      "Step 1858: train loss: 0.5804179906845093\n",
      "Step 1858: val loss: 0.5610064268112183\n",
      "Step 1859: train loss: 0.5798146724700928\n",
      "Step 1859: val loss: 0.5604168772697449\n",
      "Step 1860: train loss: 0.5792126059532166\n",
      "Step 1860: val loss: 0.559828519821167\n",
      "Step 1861: train loss: 0.5786115527153015\n",
      "Step 1861: val loss: 0.5592408180236816\n",
      "Step 1862: train loss: 0.5780114531517029\n",
      "Step 1862: val loss: 0.5586546659469604\n",
      "Step 1863: train loss: 0.5774126052856445\n",
      "Step 1863: val loss: 0.5580692887306213\n",
      "Step 1864: train loss: 0.5768146514892578\n",
      "Step 1864: val loss: 0.5574852228164673\n",
      "Step 1865: train loss: 0.5762180089950562\n",
      "Step 1865: val loss: 0.5569020509719849\n",
      "Step 1866: train loss: 0.5756223201751709\n",
      "Step 1866: val loss: 0.5563202500343323\n",
      "Step 1867: train loss: 0.5750277638435364\n",
      "Step 1867: val loss: 0.5557393431663513\n",
      "Step 1868: train loss: 0.5744343400001526\n",
      "Step 1868: val loss: 0.5551596879959106\n",
      "Step 1869: train loss: 0.5738420486450195\n",
      "Step 1869: val loss: 0.554580807685852\n",
      "Step 1870: train loss: 0.5732507109642029\n",
      "Step 1870: val loss: 0.5540032982826233\n",
      "Step 1871: train loss: 0.5726606249809265\n",
      "Step 1871: val loss: 0.5534266829490662\n",
      "Step 1872: train loss: 0.5720716118812561\n",
      "Step 1872: val loss: 0.5528514981269836\n",
      "Step 1873: train loss: 0.5714837312698364\n",
      "Step 1873: val loss: 0.5522770881652832\n",
      "Step 1874: train loss: 0.5708967447280884\n",
      "Step 1874: val loss: 0.5517037510871887\n",
      "Step 1875: train loss: 0.570310652256012\n",
      "Step 1875: val loss: 0.5511314868927002\n",
      "Step 1876: train loss: 0.5697258114814758\n",
      "Step 1876: val loss: 0.5505602359771729\n",
      "Step 1877: train loss: 0.5691421031951904\n",
      "Step 1877: val loss: 0.5499901175498962\n",
      "Step 1878: train loss: 0.568559467792511\n",
      "Step 1878: val loss: 0.5494211316108704\n",
      "Step 1879: train loss: 0.567977786064148\n",
      "Step 1879: val loss: 0.5488531589508057\n",
      "Step 1880: train loss: 0.5673972964286804\n",
      "Step 1880: val loss: 0.5482863187789917\n",
      "Step 1881: train loss: 0.5668178200721741\n",
      "Step 1881: val loss: 0.5477204918861389\n",
      "Step 1882: train loss: 0.5662394165992737\n",
      "Step 1882: val loss: 0.5471557974815369\n",
      "Step 1883: train loss: 0.5656620860099792\n",
      "Step 1883: val loss: 0.5465919375419617\n",
      "Step 1884: train loss: 0.5650857090950012\n",
      "Step 1884: val loss: 0.546029269695282\n",
      "Step 1885: train loss: 0.5645104646682739\n",
      "Step 1885: val loss: 0.5454675555229187\n",
      "Step 1886: train loss: 0.5639359951019287\n",
      "Step 1886: val loss: 0.5449069142341614\n",
      "Step 1887: train loss: 0.5633629560470581\n",
      "Step 1887: val loss: 0.5443474650382996\n",
      "Step 1888: train loss: 0.5627907514572144\n",
      "Step 1888: val loss: 0.5437890291213989\n",
      "Step 1889: train loss: 0.5622197985649109\n",
      "Step 1889: val loss: 0.5432314872741699\n",
      "Step 1890: train loss: 0.5616496801376343\n",
      "Step 1890: val loss: 0.5426748991012573\n",
      "Step 1891: train loss: 0.5610806345939636\n",
      "Step 1891: val loss: 0.5421196222305298\n",
      "Step 1892: train loss: 0.5605127215385437\n",
      "Step 1892: val loss: 0.5415651202201843\n",
      "Step 1893: train loss: 0.5599455833435059\n",
      "Step 1893: val loss: 0.5410119891166687\n",
      "Step 1894: train loss: 0.5593798160552979\n",
      "Step 1894: val loss: 0.5404595136642456\n",
      "Step 1895: train loss: 0.5588148236274719\n",
      "Step 1895: val loss: 0.5399084687232971\n",
      "Step 1896: train loss: 0.5582510828971863\n",
      "Step 1896: val loss: 0.5393583178520203\n",
      "Step 1897: train loss: 0.5576883554458618\n",
      "Step 1897: val loss: 0.5388089418411255\n",
      "Step 1898: train loss: 0.5571264028549194\n",
      "Step 1898: val loss: 0.5382608771324158\n",
      "Step 1899: train loss: 0.5565657019615173\n",
      "Step 1899: val loss: 0.5377135872840881\n",
      "Step 1900: train loss: 0.5560058355331421\n",
      "Step 1900: val loss: 0.5371676087379456\n",
      "Step 1901: train loss: 0.5554472804069519\n",
      "Step 1901: val loss: 0.5366222858428955\n",
      "Step 1902: train loss: 0.554889440536499\n",
      "Step 1902: val loss: 0.5360783338546753\n",
      "Step 1903: train loss: 0.5543326139450073\n",
      "Step 1903: val loss: 0.5355350971221924\n",
      "Step 1904: train loss: 0.5537770390510559\n",
      "Step 1904: val loss: 0.5349931120872498\n",
      "Step 1905: train loss: 0.5532222986221313\n",
      "Step 1905: val loss: 0.5344520807266235\n",
      "Step 1906: train loss: 0.5526686906814575\n",
      "Step 1906: val loss: 0.5339120626449585\n",
      "Step 1907: train loss: 0.5521159768104553\n",
      "Step 1907: val loss: 0.533373236656189\n",
      "Step 1908: train loss: 0.5515645146369934\n",
      "Step 1908: val loss: 0.532835066318512\n",
      "Step 1909: train loss: 0.5510138869285583\n",
      "Step 1909: val loss: 0.5322979092597961\n",
      "Step 1910: train loss: 0.5504642128944397\n",
      "Step 1910: val loss: 0.5317620635032654\n",
      "Step 1911: train loss: 0.5499157309532166\n",
      "Step 1911: val loss: 0.5312269926071167\n",
      "Step 1912: train loss: 0.5493680238723755\n",
      "Step 1912: val loss: 0.5306930541992188\n",
      "Step 1913: train loss: 0.5488215088844299\n",
      "Step 1913: val loss: 0.530160129070282\n",
      "Step 1914: train loss: 0.5482760071754456\n",
      "Step 1914: val loss: 0.5296279788017273\n",
      "Step 1915: train loss: 0.5477312207221985\n",
      "Step 1915: val loss: 0.5290970206260681\n",
      "Step 1916: train loss: 0.5471876859664917\n",
      "Step 1916: val loss: 0.5285669565200806\n",
      "Step 1917: train loss: 0.5466449856758118\n",
      "Step 1917: val loss: 0.5280380249023438\n",
      "Step 1918: train loss: 0.5461034178733826\n",
      "Step 1918: val loss: 0.5275099873542786\n",
      "Step 1919: train loss: 0.5455628037452698\n",
      "Step 1919: val loss: 0.5269829034805298\n",
      "Step 1920: train loss: 0.5450230240821838\n",
      "Step 1920: val loss: 0.526456892490387\n",
      "Step 1921: train loss: 0.5444844365119934\n",
      "Step 1921: val loss: 0.525931715965271\n",
      "Step 1922: train loss: 0.5439468026161194\n",
      "Step 1922: val loss: 0.5254074931144714\n",
      "Step 1923: train loss: 0.5434098243713379\n",
      "Step 1923: val loss: 0.5248843431472778\n",
      "Step 1924: train loss: 0.5428742170333862\n",
      "Step 1924: val loss: 0.5243621468544006\n",
      "Step 1925: train loss: 0.5423392057418823\n",
      "Step 1925: val loss: 0.5238410830497742\n",
      "Step 1926: train loss: 0.541805624961853\n",
      "Step 1926: val loss: 0.5233207941055298\n",
      "Step 1927: train loss: 0.541272759437561\n",
      "Step 1927: val loss: 0.5228014588356018\n",
      "Step 1928: train loss: 0.5407407283782959\n",
      "Step 1928: val loss: 0.5222833156585693\n",
      "Step 1929: train loss: 0.540209949016571\n",
      "Step 1929: val loss: 0.5217658281326294\n",
      "Step 1930: train loss: 0.5396799445152283\n",
      "Step 1930: val loss: 0.5212492942810059\n",
      "Step 1931: train loss: 0.5391508936882019\n",
      "Step 1931: val loss: 0.5207340121269226\n",
      "Step 1932: train loss: 0.5386229157447815\n",
      "Step 1932: val loss: 0.520219624042511\n",
      "Step 1933: train loss: 0.5380958318710327\n",
      "Step 1933: val loss: 0.519706130027771\n",
      "Step 1934: train loss: 0.5375698208808899\n",
      "Step 1934: val loss: 0.5191936492919922\n",
      "Step 1935: train loss: 0.5370447039604187\n",
      "Step 1935: val loss: 0.5186820030212402\n",
      "Step 1936: train loss: 0.5365203619003296\n",
      "Step 1936: val loss: 0.5181714296340942\n",
      "Step 1937: train loss: 0.5359973907470703\n",
      "Step 1937: val loss: 0.5176618695259094\n",
      "Step 1938: train loss: 0.535474956035614\n",
      "Step 1938: val loss: 0.5171529054641724\n",
      "Step 1939: train loss: 0.5349535346031189\n",
      "Step 1939: val loss: 0.5166451930999756\n",
      "Step 1940: train loss: 0.5344332456588745\n",
      "Step 1940: val loss: 0.5161384344100952\n",
      "Step 1941: train loss: 0.5339137315750122\n",
      "Step 1941: val loss: 0.5156327486038208\n",
      "Step 1942: train loss: 0.5333954095840454\n",
      "Step 1942: val loss: 0.5151278972625732\n",
      "Step 1943: train loss: 0.532878041267395\n",
      "Step 1943: val loss: 0.514623761177063\n",
      "Step 1944: train loss: 0.5323613286018372\n",
      "Step 1944: val loss: 0.514120876789093\n",
      "Step 1945: train loss: 0.5318458080291748\n",
      "Step 1945: val loss: 0.5136187076568604\n",
      "Step 1946: train loss: 0.5313311219215393\n",
      "Step 1946: val loss: 0.5131175518035889\n",
      "Step 1947: train loss: 0.5308173298835754\n",
      "Step 1947: val loss: 0.5126173496246338\n",
      "Step 1948: train loss: 0.530304491519928\n",
      "Step 1948: val loss: 0.5121181011199951\n",
      "Step 1949: train loss: 0.5297926664352417\n",
      "Step 1949: val loss: 0.5116196274757385\n",
      "Step 1950: train loss: 0.529281497001648\n",
      "Step 1950: val loss: 0.5111222863197327\n",
      "Step 1951: train loss: 0.5287715792655945\n",
      "Step 1951: val loss: 0.5106257796287537\n",
      "Step 1952: train loss: 0.5282625555992126\n",
      "Step 1952: val loss: 0.5101301074028015\n",
      "Step 1953: train loss: 0.5277541875839233\n",
      "Step 1953: val loss: 0.5096355080604553\n",
      "Step 1954: train loss: 0.5272470712661743\n",
      "Step 1954: val loss: 0.5091418027877808\n",
      "Step 1955: train loss: 0.5267407894134521\n",
      "Step 1955: val loss: 0.5086488723754883\n",
      "Step 1956: train loss: 0.5262351036071777\n",
      "Step 1956: val loss: 0.5081570148468018\n",
      "Step 1957: train loss: 0.5257307291030884\n",
      "Step 1957: val loss: 0.5076660513877869\n",
      "Step 1958: train loss: 0.5252270698547363\n",
      "Step 1958: val loss: 0.5071759819984436\n",
      "Step 1959: train loss: 0.5247243046760559\n",
      "Step 1959: val loss: 0.506686806678772\n",
      "Step 1960: train loss: 0.5242226123809814\n",
      "Step 1960: val loss: 0.5061985850334167\n",
      "Step 1961: train loss: 0.5237218141555786\n",
      "Step 1961: val loss: 0.5057111978530884\n",
      "Step 1962: train loss: 0.5232216715812683\n",
      "Step 1962: val loss: 0.5052247047424316\n",
      "Step 1963: train loss: 0.5227227807044983\n",
      "Step 1963: val loss: 0.5047392845153809\n",
      "Step 1964: train loss: 0.5222246050834656\n",
      "Step 1964: val loss: 0.5042545795440674\n",
      "Step 1965: train loss: 0.5217272043228149\n",
      "Step 1965: val loss: 0.5037710070610046\n",
      "Step 1966: train loss: 0.5212309956550598\n",
      "Step 1966: val loss: 0.5032880902290344\n",
      "Step 1967: train loss: 0.5207356214523315\n",
      "Step 1967: val loss: 0.5028061270713806\n",
      "Step 1968: train loss: 0.5202410221099854\n",
      "Step 1968: val loss: 0.502325177192688\n",
      "Step 1969: train loss: 0.5197474956512451\n",
      "Step 1969: val loss: 0.5018451809883118\n",
      "Step 1970: train loss: 0.519254744052887\n",
      "Step 1970: val loss: 0.5013659000396729\n",
      "Step 1971: train loss: 0.5187628269195557\n",
      "Step 1971: val loss: 0.5008876919746399\n",
      "Step 1972: train loss: 0.5182719826698303\n",
      "Step 1972: val loss: 0.5004101395606995\n",
      "Step 1973: train loss: 0.5177819132804871\n",
      "Step 1973: val loss: 0.499933660030365\n",
      "Step 1974: train loss: 0.5172927975654602\n",
      "Step 1974: val loss: 0.49945807456970215\n",
      "Step 1975: train loss: 0.5168046355247498\n",
      "Step 1975: val loss: 0.49898332357406616\n",
      "Step 1976: train loss: 0.5163171887397766\n",
      "Step 1976: val loss: 0.4985094666481018\n",
      "Step 1977: train loss: 0.5158307552337646\n",
      "Step 1977: val loss: 0.4980364739894867\n",
      "Step 1978: train loss: 0.5153453350067139\n",
      "Step 1978: val loss: 0.4975645840167999\n",
      "Step 1979: train loss: 0.5148605704307556\n",
      "Step 1979: val loss: 0.4970933198928833\n",
      "Step 1980: train loss: 0.5143767595291138\n",
      "Step 1980: val loss: 0.49662303924560547\n",
      "Step 1981: train loss: 0.5138939023017883\n",
      "Step 1981: val loss: 0.49615365266799927\n",
      "Step 1982: train loss: 0.5134119391441345\n",
      "Step 1982: val loss: 0.49568507075309753\n",
      "Step 1983: train loss: 0.5129307508468628\n",
      "Step 1983: val loss: 0.4952174723148346\n",
      "Step 1984: train loss: 0.5124505758285522\n",
      "Step 1984: val loss: 0.4947507381439209\n",
      "Step 1985: train loss: 0.5119711756706238\n",
      "Step 1985: val loss: 0.4942847192287445\n",
      "Step 1986: train loss: 0.5114926099777222\n",
      "Step 1986: val loss: 0.4938196539878845\n",
      "Step 1987: train loss: 0.5110148191452026\n",
      "Step 1987: val loss: 0.49335548281669617\n",
      "Step 1988: train loss: 0.5105381608009338\n",
      "Step 1988: val loss: 0.49289217591285706\n",
      "Step 1989: train loss: 0.5100621581077576\n",
      "Step 1989: val loss: 0.4924297034740448\n",
      "Step 1990: train loss: 0.5095869898796082\n",
      "Step 1990: val loss: 0.4919680953025818\n",
      "Step 1991: train loss: 0.5091129541397095\n",
      "Step 1991: val loss: 0.4915074408054352\n",
      "Step 1992: train loss: 0.5086395740509033\n",
      "Step 1992: val loss: 0.4910474121570587\n",
      "Step 1993: train loss: 0.5081669688224792\n",
      "Step 1993: val loss: 0.49058863520622253\n",
      "Step 1994: train loss: 0.5076955556869507\n",
      "Step 1994: val loss: 0.4901304244995117\n",
      "Step 1995: train loss: 0.5072248578071594\n",
      "Step 1995: val loss: 0.4896731376647949\n",
      "Step 1996: train loss: 0.5067549347877502\n",
      "Step 1996: val loss: 0.4892166554927826\n",
      "Step 1997: train loss: 0.5062857270240784\n",
      "Step 1997: val loss: 0.48876118659973145\n",
      "Step 1998: train loss: 0.5058175921440125\n",
      "Step 1998: val loss: 0.4883064031600952\n",
      "Step 1999: train loss: 0.5053502321243286\n",
      "Step 1999: val loss: 0.4878525137901306\n",
      "Step 2000: train loss: 0.5048837065696716\n",
      "Step 2000: val loss: 0.48739948868751526\n",
      "Step 2001: train loss: 0.504418134689331\n",
      "Step 2001: val loss: 0.48694729804992676\n",
      "Step 2002: train loss: 0.5039533376693726\n",
      "Step 2002: val loss: 0.4864959120750427\n",
      "Step 2003: train loss: 0.5034893751144409\n",
      "Step 2003: val loss: 0.48604530096054077\n",
      "Step 2004: train loss: 0.5030261278152466\n",
      "Step 2004: val loss: 0.48559579253196716\n",
      "Step 2005: train loss: 0.5025638937950134\n",
      "Step 2005: val loss: 0.4851468801498413\n",
      "Step 2006: train loss: 0.5021024346351624\n",
      "Step 2006: val loss: 0.4846988320350647\n",
      "Step 2007: train loss: 0.5016418099403381\n",
      "Step 2007: val loss: 0.48425182700157166\n",
      "Step 2008: train loss: 0.5011821389198303\n",
      "Step 2008: val loss: 0.4838055968284607\n",
      "Step 2009: train loss: 0.5007233023643494\n",
      "Step 2009: val loss: 0.48336005210876465\n",
      "Step 2010: train loss: 0.5002651810646057\n",
      "Step 2010: val loss: 0.4829152524471283\n",
      "Step 2011: train loss: 0.49980786442756653\n",
      "Step 2011: val loss: 0.4824715852737427\n",
      "Step 2012: train loss: 0.49935147166252136\n",
      "Step 2012: val loss: 0.4820285737514496\n",
      "Step 2013: train loss: 0.49889588356018066\n",
      "Step 2013: val loss: 0.48158636689186096\n",
      "Step 2014: train loss: 0.49844107031822205\n",
      "Step 2014: val loss: 0.4811451733112335\n",
      "Step 2015: train loss: 0.4979872703552246\n",
      "Step 2015: val loss: 0.48070475459098816\n",
      "Step 2016: train loss: 0.49753430485725403\n",
      "Step 2016: val loss: 0.4802650809288025\n",
      "Step 2017: train loss: 0.4970819056034088\n",
      "Step 2017: val loss: 0.4798261821269989\n",
      "Step 2018: train loss: 0.4966304302215576\n",
      "Step 2018: val loss: 0.47938820719718933\n",
      "Step 2019: train loss: 0.49617987871170044\n",
      "Step 2019: val loss: 0.47895100712776184\n",
      "Step 2020: train loss: 0.49573013186454773\n",
      "Step 2020: val loss: 0.47851452231407166\n",
      "Step 2021: train loss: 0.4952811300754547\n",
      "Step 2021: val loss: 0.4780789315700531\n",
      "Step 2022: train loss: 0.4948328137397766\n",
      "Step 2022: val loss: 0.4776442348957062\n",
      "Step 2023: train loss: 0.4943855404853821\n",
      "Step 2023: val loss: 0.47721025347709656\n",
      "Step 2024: train loss: 0.49393901228904724\n",
      "Step 2024: val loss: 0.4767771363258362\n",
      "Step 2025: train loss: 0.4934932291507721\n",
      "Step 2025: val loss: 0.4763447344303131\n",
      "Step 2026: train loss: 0.49304816126823425\n",
      "Step 2026: val loss: 0.47591328620910645\n",
      "Step 2027: train loss: 0.49260413646698\n",
      "Step 2027: val loss: 0.4754825532436371\n",
      "Step 2028: train loss: 0.4921607971191406\n",
      "Step 2028: val loss: 0.4750526249408722\n",
      "Step 2029: train loss: 0.49171826243400574\n",
      "Step 2029: val loss: 0.4746233820915222\n",
      "Step 2030: train loss: 0.491276353597641\n",
      "Step 2030: val loss: 0.47419512271881104\n",
      "Step 2031: train loss: 0.490835577249527\n",
      "Step 2031: val loss: 0.47376763820648193\n",
      "Step 2032: train loss: 0.49039554595947266\n",
      "Step 2032: val loss: 0.4733408987522125\n",
      "Step 2033: train loss: 0.4899561405181885\n",
      "Step 2033: val loss: 0.47291478514671326\n",
      "Step 2034: train loss: 0.489517480134964\n",
      "Step 2034: val loss: 0.47248977422714233\n",
      "Step 2035: train loss: 0.4890798330307007\n",
      "Step 2035: val loss: 0.47206538915634155\n",
      "Step 2036: train loss: 0.48864296078681946\n",
      "Step 2036: val loss: 0.4716418981552124\n",
      "Step 2037: train loss: 0.48820677399635315\n",
      "Step 2037: val loss: 0.47121909260749817\n",
      "Step 2038: train loss: 0.48777130246162415\n",
      "Step 2038: val loss: 0.47079724073410034\n",
      "Step 2039: train loss: 0.4873369038105011\n",
      "Step 2039: val loss: 0.4703761339187622\n",
      "Step 2040: train loss: 0.4869031310081482\n",
      "Step 2040: val loss: 0.4699556529521942\n",
      "Step 2041: train loss: 0.48647016286849976\n",
      "Step 2041: val loss: 0.4695359766483307\n",
      "Step 2042: train loss: 0.48603782057762146\n",
      "Step 2042: val loss: 0.46911731362342834\n",
      "Step 2043: train loss: 0.48560649156570435\n",
      "Step 2043: val loss: 0.46869930624961853\n",
      "Step 2044: train loss: 0.4851759672164917\n",
      "Step 2044: val loss: 0.4682820737361908\n",
      "Step 2045: train loss: 0.4847460687160492\n",
      "Step 2045: val loss: 0.4678654372692108\n",
      "Step 2046: train loss: 0.48431700468063354\n",
      "Step 2046: val loss: 0.46744993329048157\n",
      "Step 2047: train loss: 0.4838888645172119\n",
      "Step 2047: val loss: 0.46703505516052246\n",
      "Step 2048: train loss: 0.4834613502025604\n",
      "Step 2048: val loss: 0.4666209816932678\n",
      "Step 2049: train loss: 0.4830347001552582\n",
      "Step 2049: val loss: 0.4662075340747833\n",
      "Step 2050: train loss: 0.4826086759567261\n",
      "Step 2050: val loss: 0.46579474210739136\n",
      "Step 2051: train loss: 0.4821833372116089\n",
      "Step 2051: val loss: 0.46538299322128296\n",
      "Step 2052: train loss: 0.48175910115242004\n",
      "Step 2052: val loss: 0.46497204899787903\n",
      "Step 2053: train loss: 0.48133546113967896\n",
      "Step 2053: val loss: 0.46456173062324524\n",
      "Step 2054: train loss: 0.48091253638267517\n",
      "Step 2054: val loss: 0.46415218710899353\n",
      "Step 2055: train loss: 0.4804902970790863\n",
      "Step 2055: val loss: 0.46374356746673584\n",
      "Step 2056: train loss: 0.4800691604614258\n",
      "Step 2056: val loss: 0.46333563327789307\n",
      "Step 2057: train loss: 0.47964856028556824\n",
      "Step 2057: val loss: 0.4629283547401428\n",
      "Step 2058: train loss: 0.4792288541793823\n",
      "Step 2058: val loss: 0.46252185106277466\n",
      "Step 2059: train loss: 0.4788096845149994\n",
      "Step 2059: val loss: 0.4621160626411438\n",
      "Step 2060: train loss: 0.47839125990867615\n",
      "Step 2060: val loss: 0.46171122789382935\n",
      "Step 2061: train loss: 0.4779738187789917\n",
      "Step 2061: val loss: 0.4613071084022522\n",
      "Step 2062: train loss: 0.4775570034980774\n",
      "Step 2062: val loss: 0.46090367436408997\n",
      "Step 2063: train loss: 0.47714105248451233\n",
      "Step 2063: val loss: 0.4605008661746979\n",
      "Step 2064: train loss: 0.476725697517395\n",
      "Step 2064: val loss: 0.4600990116596222\n",
      "Step 2065: train loss: 0.4763113260269165\n",
      "Step 2065: val loss: 0.4596979022026062\n",
      "Step 2066: train loss: 0.4758976697921753\n",
      "Step 2066: val loss: 0.4592975676059723\n",
      "Step 2067: train loss: 0.4754846692085266\n",
      "Step 2067: val loss: 0.4588978588581085\n",
      "Step 2068: train loss: 0.4750724136829376\n",
      "Step 2068: val loss: 0.4584987163543701\n",
      "Step 2069: train loss: 0.4746607542037964\n",
      "Step 2069: val loss: 0.45810067653656006\n",
      "Step 2070: train loss: 0.4742501378059387\n",
      "Step 2070: val loss: 0.4577033221721649\n",
      "Step 2071: train loss: 0.4738401770591736\n",
      "Step 2071: val loss: 0.4573066830635071\n",
      "Step 2072: train loss: 0.47343090176582336\n",
      "Step 2072: val loss: 0.4569106698036194\n",
      "Step 2073: train loss: 0.47302234172821045\n",
      "Step 2073: val loss: 0.45651543140411377\n",
      "Step 2074: train loss: 0.4726144075393677\n",
      "Step 2074: val loss: 0.4561210870742798\n",
      "Step 2075: train loss: 0.47220751643180847\n",
      "Step 2075: val loss: 0.45572733879089355\n",
      "Step 2076: train loss: 0.47180137038230896\n",
      "Step 2076: val loss: 0.4553343653678894\n",
      "Step 2077: train loss: 0.47139570116996765\n",
      "Step 2077: val loss: 0.45494213700294495\n",
      "Step 2078: train loss: 0.47099077701568604\n",
      "Step 2078: val loss: 0.45455050468444824\n",
      "Step 2079: train loss: 0.4705866575241089\n",
      "Step 2079: val loss: 0.4541597068309784\n",
      "Step 2080: train loss: 0.470183402299881\n",
      "Step 2080: val loss: 0.453769713640213\n",
      "Step 2081: train loss: 0.469780832529068\n",
      "Step 2081: val loss: 0.45338037610054016\n",
      "Step 2082: train loss: 0.4693789780139923\n",
      "Step 2082: val loss: 0.4529918134212494\n",
      "Step 2083: train loss: 0.468977689743042\n",
      "Step 2083: val loss: 0.45260393619537354\n",
      "Step 2084: train loss: 0.46857714653015137\n",
      "Step 2084: val loss: 0.45221689343452454\n",
      "Step 2085: train loss: 0.46817758679389954\n",
      "Step 2085: val loss: 0.4518304169178009\n",
      "Step 2086: train loss: 0.46777868270874023\n",
      "Step 2086: val loss: 0.4514448344707489\n",
      "Step 2087: train loss: 0.4673804044723511\n",
      "Step 2087: val loss: 0.45105981826782227\n",
      "Step 2088: train loss: 0.4669828414916992\n",
      "Step 2088: val loss: 0.4506753981113434\n",
      "Step 2089: train loss: 0.4665859043598175\n",
      "Step 2089: val loss: 0.4502919912338257\n",
      "Step 2090: train loss: 0.46619004011154175\n",
      "Step 2090: val loss: 0.4499092996120453\n",
      "Step 2091: train loss: 0.46579474210739136\n",
      "Step 2091: val loss: 0.4495272636413574\n",
      "Step 2092: train loss: 0.4654000401496887\n",
      "Step 2092: val loss: 0.44914576411247253\n",
      "Step 2093: train loss: 0.46500611305236816\n",
      "Step 2093: val loss: 0.4487650990486145\n",
      "Step 2094: train loss: 0.4646127223968506\n",
      "Step 2094: val loss: 0.44838500022888184\n",
      "Step 2095: train loss: 0.4642201066017151\n",
      "Step 2095: val loss: 0.4480058252811432\n",
      "Step 2096: train loss: 0.4638284742832184\n",
      "Step 2096: val loss: 0.44762736558914185\n",
      "Step 2097: train loss: 0.4634374678134918\n",
      "Step 2097: val loss: 0.44724956154823303\n",
      "Step 2098: train loss: 0.4630470275878906\n",
      "Step 2098: val loss: 0.4468725025653839\n",
      "Step 2099: train loss: 0.4626573324203491\n",
      "Step 2099: val loss: 0.44649600982666016\n",
      "Step 2100: train loss: 0.4622681736946106\n",
      "Step 2100: val loss: 0.44612008333206177\n",
      "Step 2101: train loss: 0.46187981963157654\n",
      "Step 2101: val loss: 0.44574519991874695\n",
      "Step 2102: train loss: 0.4614923894405365\n",
      "Step 2102: val loss: 0.44537097215652466\n",
      "Step 2103: train loss: 0.4611055552959442\n",
      "Step 2103: val loss: 0.44499731063842773\n",
      "Step 2104: train loss: 0.4607194662094116\n",
      "Step 2104: val loss: 0.4446244239807129\n",
      "Step 2105: train loss: 0.4603339433670044\n",
      "Step 2105: val loss: 0.444252073764801\n",
      "Step 2106: train loss: 0.4599491059780121\n",
      "Step 2106: val loss: 0.44388043880462646\n",
      "Step 2107: train loss: 0.4595648944377899\n",
      "Step 2107: val loss: 0.4435097873210907\n",
      "Step 2108: train loss: 0.45918163657188416\n",
      "Step 2108: val loss: 0.4431396424770355\n",
      "Step 2109: train loss: 0.45879891514778137\n",
      "Step 2109: val loss: 0.44277021288871765\n",
      "Step 2110: train loss: 0.45841699838638306\n",
      "Step 2110: val loss: 0.4424014687538147\n",
      "Step 2111: train loss: 0.4580356478691101\n",
      "Step 2111: val loss: 0.44203341007232666\n",
      "Step 2112: train loss: 0.4576549828052521\n",
      "Step 2112: val loss: 0.4416658282279968\n",
      "Step 2113: train loss: 0.45727506279945374\n",
      "Step 2113: val loss: 0.4412992000579834\n",
      "Step 2114: train loss: 0.4568958580493927\n",
      "Step 2114: val loss: 0.44093331694602966\n",
      "Step 2115: train loss: 0.45651739835739136\n",
      "Step 2115: val loss: 0.4405680000782013\n",
      "Step 2116: train loss: 0.45613959431648254\n",
      "Step 2116: val loss: 0.44020333886146545\n",
      "Step 2117: train loss: 0.4557623267173767\n",
      "Step 2117: val loss: 0.4398394227027893\n",
      "Step 2118: train loss: 0.4553857743740082\n",
      "Step 2118: val loss: 0.43947601318359375\n",
      "Step 2119: train loss: 0.4550098776817322\n",
      "Step 2119: val loss: 0.4391135275363922\n",
      "Step 2120: train loss: 0.4546349048614502\n",
      "Step 2120: val loss: 0.4387516975402832\n",
      "Step 2121: train loss: 0.45426058769226074\n",
      "Step 2121: val loss: 0.4383905231952667\n",
      "Step 2122: train loss: 0.4538869261741638\n",
      "Step 2122: val loss: 0.4380299746990204\n",
      "Step 2123: train loss: 0.45351383090019226\n",
      "Step 2123: val loss: 0.43767017126083374\n",
      "Step 2124: train loss: 0.45314130187034607\n",
      "Step 2124: val loss: 0.4373108148574829\n",
      "Step 2125: train loss: 0.45276954770088196\n",
      "Step 2125: val loss: 0.43695226311683655\n",
      "Step 2126: train loss: 0.4523983299732208\n",
      "Step 2126: val loss: 0.4365943968296051\n",
      "Step 2127: train loss: 0.4520280659198761\n",
      "Step 2127: val loss: 0.43623730540275574\n",
      "Step 2128: train loss: 0.4516585171222687\n",
      "Step 2128: val loss: 0.4358809292316437\n",
      "Step 2129: train loss: 0.45128947496414185\n",
      "Step 2129: val loss: 0.43552497029304504\n",
      "Step 2130: train loss: 0.45092108845710754\n",
      "Step 2130: val loss: 0.43516990542411804\n",
      "Step 2131: train loss: 0.45055335760116577\n",
      "Step 2131: val loss: 0.4348152279853821\n",
      "Step 2132: train loss: 0.45018625259399414\n",
      "Step 2132: val loss: 0.4344612956047058\n",
      "Step 2133: train loss: 0.44981977343559265\n",
      "Step 2133: val loss: 0.43410828709602356\n",
      "Step 2134: train loss: 0.44945409893989563\n",
      "Step 2134: val loss: 0.4337557554244995\n",
      "Step 2135: train loss: 0.4490892291069031\n",
      "Step 2135: val loss: 0.43340393900871277\n",
      "Step 2136: train loss: 0.4487248361110687\n",
      "Step 2136: val loss: 0.43305280804634094\n",
      "Step 2137: train loss: 0.44836121797561646\n",
      "Step 2137: val loss: 0.4327022135257721\n",
      "Step 2138: train loss: 0.447998046875\n",
      "Step 2138: val loss: 0.4323522448539734\n",
      "Step 2139: train loss: 0.4476355314254761\n",
      "Step 2139: val loss: 0.4320029020309448\n",
      "Step 2140: train loss: 0.4472736716270447\n",
      "Step 2140: val loss: 0.43165433406829834\n",
      "Step 2141: train loss: 0.4469124376773834\n",
      "Step 2141: val loss: 0.4313063621520996\n",
      "Step 2142: train loss: 0.4465521574020386\n",
      "Step 2142: val loss: 0.43095916509628296\n",
      "Step 2143: train loss: 0.4461924135684967\n",
      "Step 2143: val loss: 0.43061262369155884\n",
      "Step 2144: train loss: 0.44583332538604736\n",
      "Step 2144: val loss: 0.43026652932167053\n",
      "Step 2145: train loss: 0.4454748034477234\n",
      "Step 2145: val loss: 0.42992115020751953\n",
      "Step 2146: train loss: 0.44511690735816956\n",
      "Step 2146: val loss: 0.42957645654678345\n",
      "Step 2147: train loss: 0.44475963711738586\n",
      "Step 2147: val loss: 0.4292323589324951\n",
      "Step 2148: train loss: 0.4444030225276947\n",
      "Step 2148: val loss: 0.42888885736465454\n",
      "Step 2149: train loss: 0.4440469741821289\n",
      "Step 2149: val loss: 0.42854616045951843\n",
      "Step 2150: train loss: 0.44369181990623474\n",
      "Step 2150: val loss: 0.42820414900779724\n",
      "Step 2151: train loss: 0.4433373212814331\n",
      "Step 2151: val loss: 0.4278627038002014\n",
      "Step 2152: train loss: 0.4429832398891449\n",
      "Step 2152: val loss: 0.42752188444137573\n",
      "Step 2153: train loss: 0.44262993335723877\n",
      "Step 2153: val loss: 0.4271816611289978\n",
      "Step 2154: train loss: 0.44227728247642517\n",
      "Step 2154: val loss: 0.42684206366539\n",
      "Step 2155: train loss: 0.44192516803741455\n",
      "Step 2155: val loss: 0.42650309205055237\n",
      "Step 2156: train loss: 0.4415736794471741\n",
      "Step 2156: val loss: 0.4261646866798401\n",
      "Step 2157: train loss: 0.4412228465080261\n",
      "Step 2157: val loss: 0.42582690715789795\n",
      "Step 2158: train loss: 0.4408724308013916\n",
      "Step 2158: val loss: 0.42548999190330505\n",
      "Step 2159: train loss: 0.44052302837371826\n",
      "Step 2159: val loss: 0.4251536726951599\n",
      "Step 2160: train loss: 0.44017428159713745\n",
      "Step 2160: val loss: 0.42481786012649536\n",
      "Step 2161: train loss: 0.43982604146003723\n",
      "Step 2161: val loss: 0.4244827628135681\n",
      "Step 2162: train loss: 0.43947839736938477\n",
      "Step 2162: val loss: 0.42414817214012146\n",
      "Step 2163: train loss: 0.4391314387321472\n",
      "Step 2163: val loss: 0.4238142669200897\n",
      "Step 2164: train loss: 0.43878501653671265\n",
      "Step 2164: val loss: 0.4234810471534729\n",
      "Step 2165: train loss: 0.4384392201900482\n",
      "Step 2165: val loss: 0.4231483042240143\n",
      "Step 2166: train loss: 0.43809396028518677\n",
      "Step 2166: val loss: 0.4228162169456482\n",
      "Step 2167: train loss: 0.43774935603141785\n",
      "Step 2167: val loss: 0.4224849343299866\n",
      "Step 2168: train loss: 0.43740567564964294\n",
      "Step 2168: val loss: 0.42215418815612793\n",
      "Step 2169: train loss: 0.43706244230270386\n",
      "Step 2169: val loss: 0.42182406783103943\n",
      "Step 2170: train loss: 0.4367199242115021\n",
      "Step 2170: val loss: 0.42149460315704346\n",
      "Step 2171: train loss: 0.43637797236442566\n",
      "Step 2171: val loss: 0.4211656451225281\n",
      "Step 2172: train loss: 0.436036616563797\n",
      "Step 2172: val loss: 0.4208373725414276\n",
      "Step 2173: train loss: 0.4356957674026489\n",
      "Step 2173: val loss: 0.4205097258090973\n",
      "Step 2174: train loss: 0.435355544090271\n",
      "Step 2174: val loss: 0.42018255591392517\n",
      "Step 2175: train loss: 0.4350159764289856\n",
      "Step 2175: val loss: 0.41985607147216797\n",
      "Step 2176: train loss: 0.43467697501182556\n",
      "Step 2176: val loss: 0.41953012347221375\n",
      "Step 2177: train loss: 0.4343385100364685\n",
      "Step 2177: val loss: 0.4192047715187073\n",
      "Step 2178: train loss: 0.4340006113052368\n",
      "Step 2178: val loss: 0.41888025403022766\n",
      "Step 2179: train loss: 0.43366366624832153\n",
      "Step 2179: val loss: 0.41855624318122864\n",
      "Step 2180: train loss: 0.433327317237854\n",
      "Step 2180: val loss: 0.4182329475879669\n",
      "Step 2181: train loss: 0.43299147486686707\n",
      "Step 2181: val loss: 0.41791006922721863\n",
      "Step 2182: train loss: 0.43265631794929504\n",
      "Step 2182: val loss: 0.4175880253314972\n",
      "Step 2183: train loss: 0.43232157826423645\n",
      "Step 2183: val loss: 0.4172663688659668\n",
      "Step 2184: train loss: 0.43198755383491516\n",
      "Step 2184: val loss: 0.41694536805152893\n",
      "Step 2185: train loss: 0.43165403604507446\n",
      "Step 2185: val loss: 0.41662484407424927\n",
      "Step 2186: train loss: 0.4313211143016815\n",
      "Step 2186: val loss: 0.4163050651550293\n",
      "Step 2187: train loss: 0.43098875880241394\n",
      "Step 2187: val loss: 0.4159858226776123\n",
      "Step 2188: train loss: 0.4306570887565613\n",
      "Step 2188: val loss: 0.41566702723503113\n",
      "Step 2189: train loss: 0.4303259551525116\n",
      "Step 2189: val loss: 0.41534900665283203\n",
      "Step 2190: train loss: 0.4299953281879425\n",
      "Step 2190: val loss: 0.41503167152404785\n",
      "Step 2191: train loss: 0.42966556549072266\n",
      "Step 2191: val loss: 0.41471487283706665\n",
      "Step 2192: train loss: 0.42933642864227295\n",
      "Step 2192: val loss: 0.4143986999988556\n",
      "Step 2193: train loss: 0.4290079176425934\n",
      "Step 2193: val loss: 0.4140831232070923\n",
      "Step 2194: train loss: 0.4286797344684601\n",
      "Step 2194: val loss: 0.41376811265945435\n",
      "Step 2195: train loss: 0.42835235595703125\n",
      "Step 2195: val loss: 0.4134536683559418\n",
      "Step 2196: train loss: 0.428025484085083\n",
      "Step 2196: val loss: 0.41313987970352173\n",
      "Step 2197: train loss: 0.42769911885261536\n",
      "Step 2197: val loss: 0.4128265678882599\n",
      "Step 2198: train loss: 0.42737337946891785\n",
      "Step 2198: val loss: 0.41251397132873535\n",
      "Step 2199: train loss: 0.4270482659339905\n",
      "Step 2199: val loss: 0.41220179200172424\n",
      "Step 2200: train loss: 0.42672374844551086\n",
      "Step 2200: val loss: 0.4118901789188385\n",
      "Step 2201: train loss: 0.42639967799186707\n",
      "Step 2201: val loss: 0.4115791618824005\n",
      "Step 2202: train loss: 0.4260762333869934\n",
      "Step 2202: val loss: 0.4112687110900879\n",
      "Step 2203: train loss: 0.4257533550262451\n",
      "Step 2203: val loss: 0.410958856344223\n",
      "Step 2204: train loss: 0.4254310131072998\n",
      "Step 2204: val loss: 0.41064974665641785\n",
      "Step 2205: train loss: 0.42510950565338135\n",
      "Step 2205: val loss: 0.41034114360809326\n",
      "Step 2206: train loss: 0.42478862404823303\n",
      "Step 2206: val loss: 0.4100332260131836\n",
      "Step 2207: train loss: 0.4244682490825653\n",
      "Step 2207: val loss: 0.4097258746623993\n",
      "Step 2208: train loss: 0.42414844036102295\n",
      "Step 2208: val loss: 0.4094190299510956\n",
      "Step 2209: train loss: 0.42382925748825073\n",
      "Step 2209: val loss: 0.4091128408908844\n",
      "Step 2210: train loss: 0.42351043224334717\n",
      "Step 2210: val loss: 0.40880709886550903\n",
      "Step 2211: train loss: 0.4231923818588257\n",
      "Step 2211: val loss: 0.4085019528865814\n",
      "Step 2212: train loss: 0.4228748083114624\n",
      "Step 2212: val loss: 0.40819743275642395\n",
      "Step 2213: train loss: 0.4225578010082245\n",
      "Step 2213: val loss: 0.40789344906806946\n",
      "Step 2214: train loss: 0.42224130034446716\n",
      "Step 2214: val loss: 0.407589852809906\n",
      "Step 2215: train loss: 0.42192548513412476\n",
      "Step 2215: val loss: 0.407287061214447\n",
      "Step 2216: train loss: 0.4216102063655853\n",
      "Step 2216: val loss: 0.40698468685150146\n",
      "Step 2217: train loss: 0.4212953448295593\n",
      "Step 2217: val loss: 0.4066828489303589\n",
      "Step 2218: train loss: 0.420981228351593\n",
      "Step 2218: val loss: 0.40638166666030884\n",
      "Step 2219: train loss: 0.42066749930381775\n",
      "Step 2219: val loss: 0.4060809910297394\n",
      "Step 2220: train loss: 0.4203544855117798\n",
      "Step 2220: val loss: 0.4057808518409729\n",
      "Step 2221: train loss: 0.42004188895225525\n",
      "Step 2221: val loss: 0.4054812788963318\n",
      "Step 2222: train loss: 0.41972988843917847\n",
      "Step 2222: val loss: 0.4051821827888489\n",
      "Step 2223: train loss: 0.4194183647632599\n",
      "Step 2223: val loss: 0.40488401055336\n",
      "Step 2224: train loss: 0.4191078245639801\n",
      "Step 2224: val loss: 0.40458613634109497\n",
      "Step 2225: train loss: 0.41879764199256897\n",
      "Step 2225: val loss: 0.4042890667915344\n",
      "Step 2226: train loss: 0.41848814487457275\n",
      "Step 2226: val loss: 0.4039924740791321\n",
      "Step 2227: train loss: 0.4181792140007019\n",
      "Step 2227: val loss: 0.40369635820388794\n",
      "Step 2228: train loss: 0.41787073016166687\n",
      "Step 2228: val loss: 0.40340086817741394\n",
      "Step 2229: train loss: 0.4175627529621124\n",
      "Step 2229: val loss: 0.4031059741973877\n",
      "Step 2230: train loss: 0.4172554314136505\n",
      "Step 2230: val loss: 0.40281152725219727\n",
      "Step 2231: train loss: 0.4169486463069916\n",
      "Step 2231: val loss: 0.4025176465511322\n",
      "Step 2232: train loss: 0.41664236783981323\n",
      "Step 2232: val loss: 0.4022243320941925\n",
      "Step 2233: train loss: 0.41633665561676025\n",
      "Step 2233: val loss: 0.4019315242767334\n",
      "Step 2234: train loss: 0.41603145003318787\n",
      "Step 2234: val loss: 0.40163928270339966\n",
      "Step 2235: train loss: 0.4157269597053528\n",
      "Step 2235: val loss: 0.4013475477695465\n",
      "Step 2236: train loss: 0.4154227375984192\n",
      "Step 2236: val loss: 0.4010563790798187\n",
      "Step 2237: train loss: 0.4151191711425781\n",
      "Step 2237: val loss: 0.4007657766342163\n",
      "Step 2238: train loss: 0.4148161709308624\n",
      "Step 2238: val loss: 0.4004756808280945\n",
      "Step 2239: train loss: 0.4145137071609497\n",
      "Step 2239: val loss: 0.400186151266098\n",
      "Step 2240: train loss: 0.41421177983283997\n",
      "Step 2240: val loss: 0.3998970687389374\n",
      "Step 2241: train loss: 0.4139103591442108\n",
      "Step 2241: val loss: 0.39960864186286926\n",
      "Step 2242: train loss: 0.4136095345020294\n",
      "Step 2242: val loss: 0.39932069182395935\n",
      "Step 2243: train loss: 0.4133092164993286\n",
      "Step 2243: val loss: 0.39903324842453003\n",
      "Step 2244: train loss: 0.413009375333786\n",
      "Step 2244: val loss: 0.3987463414669037\n",
      "Step 2245: train loss: 0.41271016001701355\n",
      "Step 2245: val loss: 0.39846011996269226\n",
      "Step 2246: train loss: 0.4124114513397217\n",
      "Step 2246: val loss: 0.3981742262840271\n",
      "Step 2247: train loss: 0.4121132493019104\n",
      "Step 2247: val loss: 0.39788904786109924\n",
      "Step 2248: train loss: 0.4118155539035797\n",
      "Step 2248: val loss: 0.39760419726371765\n",
      "Step 2249: train loss: 0.41151848435401917\n",
      "Step 2249: val loss: 0.3973199427127838\n",
      "Step 2250: train loss: 0.41122183203697205\n",
      "Step 2250: val loss: 0.39703625440597534\n",
      "Step 2251: train loss: 0.41092580556869507\n",
      "Step 2251: val loss: 0.39675310254096985\n",
      "Step 2252: train loss: 0.4106302857398987\n",
      "Step 2252: val loss: 0.39647039771080017\n",
      "Step 2253: train loss: 0.4103352725505829\n",
      "Step 2253: val loss: 0.396188348531723\n",
      "Step 2254: train loss: 0.41004082560539246\n",
      "Step 2254: val loss: 0.3959067463874817\n",
      "Step 2255: train loss: 0.40974682569503784\n",
      "Step 2255: val loss: 0.39562568068504333\n",
      "Step 2256: train loss: 0.4094533920288086\n",
      "Step 2256: val loss: 0.3953450322151184\n",
      "Step 2257: train loss: 0.4091605544090271\n",
      "Step 2257: val loss: 0.39506515860557556\n",
      "Step 2258: train loss: 0.4088681638240814\n",
      "Step 2258: val loss: 0.3947855830192566\n",
      "Step 2259: train loss: 0.4085763096809387\n",
      "Step 2259: val loss: 0.39450666308403015\n",
      "Step 2260: train loss: 0.408284991979599\n",
      "Step 2260: val loss: 0.39422816038131714\n",
      "Step 2261: train loss: 0.40799421072006226\n",
      "Step 2261: val loss: 0.3939502239227295\n",
      "Step 2262: train loss: 0.4077039659023285\n",
      "Step 2262: val loss: 0.39367297291755676\n",
      "Step 2263: train loss: 0.40741440653800964\n",
      "Step 2263: val loss: 0.3933962881565094\n",
      "Step 2264: train loss: 0.40712544322013855\n",
      "Step 2264: val loss: 0.3931201100349426\n",
      "Step 2265: train loss: 0.40683695673942566\n",
      "Step 2265: val loss: 0.39284446835517883\n",
      "Step 2266: train loss: 0.40654894709587097\n",
      "Step 2266: val loss: 0.39256933331489563\n",
      "Step 2267: train loss: 0.4062615931034088\n",
      "Step 2267: val loss: 0.3922947347164154\n",
      "Step 2268: train loss: 0.4059745967388153\n",
      "Step 2268: val loss: 0.39202067255973816\n",
      "Step 2269: train loss: 0.40568825602531433\n",
      "Step 2269: val loss: 0.39174702763557434\n",
      "Step 2270: train loss: 0.40540236234664917\n",
      "Step 2270: val loss: 0.39147382974624634\n",
      "Step 2271: train loss: 0.4051167368888855\n",
      "Step 2271: val loss: 0.39120128750801086\n",
      "Step 2272: train loss: 0.4048319160938263\n",
      "Step 2272: val loss: 0.3909290134906769\n",
      "Step 2273: train loss: 0.40454739332199097\n",
      "Step 2273: val loss: 0.39065736532211304\n",
      "Step 2274: train loss: 0.40426331758499146\n",
      "Step 2274: val loss: 0.390386164188385\n",
      "Step 2275: train loss: 0.40397968888282776\n",
      "Step 2275: val loss: 0.3901154100894928\n",
      "Step 2276: train loss: 0.4036966860294342\n",
      "Step 2276: val loss: 0.38984522223472595\n",
      "Step 2277: train loss: 0.40341418981552124\n",
      "Step 2277: val loss: 0.3895755112171173\n",
      "Step 2278: train loss: 0.4031321406364441\n",
      "Step 2278: val loss: 0.38930636644363403\n",
      "Step 2279: train loss: 0.40285059809684753\n",
      "Step 2279: val loss: 0.38903772830963135\n",
      "Step 2280: train loss: 0.40256965160369873\n",
      "Step 2280: val loss: 0.3887694478034973\n",
      "Step 2281: train loss: 0.40228915214538574\n",
      "Step 2281: val loss: 0.3885017931461334\n",
      "Step 2282: train loss: 0.40200915932655334\n",
      "Step 2282: val loss: 0.3882346749305725\n",
      "Step 2283: train loss: 0.40172964334487915\n",
      "Step 2283: val loss: 0.38796788454055786\n",
      "Step 2284: train loss: 0.4014506936073303\n",
      "Step 2284: val loss: 0.38770171999931335\n",
      "Step 2285: train loss: 0.40117231011390686\n",
      "Step 2285: val loss: 0.3874362111091614\n",
      "Step 2286: train loss: 0.40089428424835205\n",
      "Step 2286: val loss: 0.3871709108352661\n",
      "Step 2287: train loss: 0.4006168246269226\n",
      "Step 2287: val loss: 0.386906236410141\n",
      "Step 2288: train loss: 0.40033990144729614\n",
      "Step 2288: val loss: 0.38664212822914124\n",
      "Step 2289: train loss: 0.4000634253025055\n",
      "Step 2289: val loss: 0.3863784074783325\n",
      "Step 2290: train loss: 0.3997874855995178\n",
      "Step 2290: val loss: 0.38611528277397156\n",
      "Step 2291: train loss: 0.39951208233833313\n",
      "Step 2291: val loss: 0.3858526945114136\n",
      "Step 2292: train loss: 0.39923715591430664\n",
      "Step 2292: val loss: 0.38559046387672424\n",
      "Step 2293: train loss: 0.3989626467227936\n",
      "Step 2293: val loss: 0.3853287696838379\n",
      "Step 2294: train loss: 0.39868873357772827\n",
      "Step 2294: val loss: 0.3850676417350769\n",
      "Step 2295: train loss: 0.39841532707214355\n",
      "Step 2295: val loss: 0.38480693101882935\n",
      "Step 2296: train loss: 0.39814233779907227\n",
      "Step 2296: val loss: 0.3845467269420624\n",
      "Step 2297: train loss: 0.39786988496780396\n",
      "Step 2297: val loss: 0.3842870593070984\n",
      "Step 2298: train loss: 0.39759790897369385\n",
      "Step 2298: val loss: 0.3840278387069702\n",
      "Step 2299: train loss: 0.3973264992237091\n",
      "Step 2299: val loss: 0.38376909494400024\n",
      "Step 2300: train loss: 0.39705541729927063\n",
      "Step 2300: val loss: 0.38351088762283325\n",
      "Step 2301: train loss: 0.39678505063056946\n",
      "Step 2301: val loss: 0.38325318694114685\n",
      "Step 2302: train loss: 0.39651504158973694\n",
      "Step 2302: val loss: 0.38299596309661865\n",
      "Step 2303: train loss: 0.3962455987930298\n",
      "Step 2303: val loss: 0.38273921608924866\n",
      "Step 2304: train loss: 0.39597663283348083\n",
      "Step 2304: val loss: 0.38248303532600403\n",
      "Step 2305: train loss: 0.3957080543041229\n",
      "Step 2305: val loss: 0.38222718238830566\n",
      "Step 2306: train loss: 0.39544016122817993\n",
      "Step 2306: val loss: 0.38197198510169983\n",
      "Step 2307: train loss: 0.3951725959777832\n",
      "Step 2307: val loss: 0.38171714544296265\n",
      "Step 2308: train loss: 0.39490559697151184\n",
      "Step 2308: val loss: 0.3814629018306732\n",
      "Step 2309: train loss: 0.3946389853954315\n",
      "Step 2309: val loss: 0.3812091052532196\n",
      "Step 2310: train loss: 0.39437299966812134\n",
      "Step 2310: val loss: 0.3809557259082794\n",
      "Step 2311: train loss: 0.394107460975647\n",
      "Step 2311: val loss: 0.3807029128074646\n",
      "Step 2312: train loss: 0.3938423693180084\n",
      "Step 2312: val loss: 0.3804505169391632\n",
      "Step 2313: train loss: 0.39357778429985046\n",
      "Step 2313: val loss: 0.3801986575126648\n",
      "Step 2314: train loss: 0.3933137059211731\n",
      "Step 2314: val loss: 0.37994733452796936\n",
      "Step 2315: train loss: 0.3930501639842987\n",
      "Step 2315: val loss: 0.3796963691711426\n",
      "Step 2316: train loss: 0.3927869200706482\n",
      "Step 2316: val loss: 0.3794458210468292\n",
      "Step 2317: train loss: 0.3925240635871887\n",
      "Step 2317: val loss: 0.3791956305503845\n",
      "Step 2318: train loss: 0.39226165413856506\n",
      "Step 2318: val loss: 0.3789460062980652\n",
      "Step 2319: train loss: 0.3919997215270996\n",
      "Step 2319: val loss: 0.3786967992782593\n",
      "Step 2320: train loss: 0.3917383849620819\n",
      "Step 2320: val loss: 0.37844812870025635\n",
      "Step 2321: train loss: 0.39147740602493286\n",
      "Step 2321: val loss: 0.37819981575012207\n",
      "Step 2322: train loss: 0.3912169337272644\n",
      "Step 2322: val loss: 0.3779520094394684\n",
      "Step 2323: train loss: 0.39095696806907654\n",
      "Step 2323: val loss: 0.3777047395706177\n",
      "Step 2324: train loss: 0.3906974196434021\n",
      "Step 2324: val loss: 0.3774578869342804\n",
      "Step 2325: train loss: 0.39043840765953064\n",
      "Step 2325: val loss: 0.3772115409374237\n",
      "Step 2326: train loss: 0.3901797831058502\n",
      "Step 2326: val loss: 0.37696564197540283\n",
      "Step 2327: train loss: 0.3899216949939728\n",
      "Step 2327: val loss: 0.37672024965286255\n",
      "Step 2328: train loss: 0.3896641135215759\n",
      "Step 2328: val loss: 0.3764752447605133\n",
      "Step 2329: train loss: 0.3894069790840149\n",
      "Step 2329: val loss: 0.3762308359146118\n",
      "Step 2330: train loss: 0.38915032148361206\n",
      "Step 2330: val loss: 0.37598681449890137\n",
      "Step 2331: train loss: 0.3888941705226898\n",
      "Step 2331: val loss: 0.3757433295249939\n",
      "Step 2332: train loss: 0.3886384665966034\n",
      "Step 2332: val loss: 0.3755002021789551\n",
      "Step 2333: train loss: 0.38838323950767517\n",
      "Step 2333: val loss: 0.3752577006816864\n",
      "Step 2334: train loss: 0.38812851905822754\n",
      "Step 2334: val loss: 0.375015527009964\n",
      "Step 2335: train loss: 0.38787412643432617\n",
      "Step 2335: val loss: 0.374773770570755\n",
      "Step 2336: train loss: 0.38762012124061584\n",
      "Step 2336: val loss: 0.3745323717594147\n",
      "Step 2337: train loss: 0.38736656308174133\n",
      "Step 2337: val loss: 0.3742915391921997\n",
      "Step 2338: train loss: 0.38711345195770264\n",
      "Step 2338: val loss: 0.374051034450531\n",
      "Step 2339: train loss: 0.38686081767082214\n",
      "Step 2339: val loss: 0.3738110661506653\n",
      "Step 2340: train loss: 0.38660863041877747\n",
      "Step 2340: val loss: 0.373571515083313\n",
      "Step 2341: train loss: 0.386356920003891\n",
      "Step 2341: val loss: 0.3733324706554413\n",
      "Step 2342: train loss: 0.3861056864261627\n",
      "Step 2342: val loss: 0.3730938732624054\n",
      "Step 2343: train loss: 0.3858549892902374\n",
      "Step 2343: val loss: 0.3728557527065277\n",
      "Step 2344: train loss: 0.38560470938682556\n",
      "Step 2344: val loss: 0.37261807918548584\n",
      "Step 2345: train loss: 0.38535478711128235\n",
      "Step 2345: val loss: 0.3723808526992798\n",
      "Step 2346: train loss: 0.3851054906845093\n",
      "Step 2346: val loss: 0.3721441328525543\n",
      "Step 2347: train loss: 0.38485652208328247\n",
      "Step 2347: val loss: 0.3719077706336975\n",
      "Step 2348: train loss: 0.38460808992385864\n",
      "Step 2348: val loss: 0.3716719448566437\n",
      "Step 2349: train loss: 0.38436010479927063\n",
      "Step 2349: val loss: 0.37143659591674805\n",
      "Step 2350: train loss: 0.3841125965118408\n",
      "Step 2350: val loss: 0.3712014853954315\n",
      "Step 2351: train loss: 0.3838653266429901\n",
      "Step 2351: val loss: 0.3709668815135956\n",
      "Step 2352: train loss: 0.3836185038089752\n",
      "Step 2352: val loss: 0.3707326650619507\n",
      "Step 2353: train loss: 0.38337215781211853\n",
      "Step 2353: val loss: 0.3704988658428192\n",
      "Step 2354: train loss: 0.38312625885009766\n",
      "Step 2354: val loss: 0.3702656328678131\n",
      "Step 2355: train loss: 0.3828808069229126\n",
      "Step 2355: val loss: 0.37003272771835327\n",
      "Step 2356: train loss: 0.38263580203056335\n",
      "Step 2356: val loss: 0.369800329208374\n",
      "Step 2357: train loss: 0.3823912441730499\n",
      "Step 2357: val loss: 0.3695683479309082\n",
      "Step 2358: train loss: 0.3821471929550171\n",
      "Step 2358: val loss: 0.36933693289756775\n",
      "Step 2359: train loss: 0.3819035589694977\n",
      "Step 2359: val loss: 0.3691059350967407\n",
      "Step 2360: train loss: 0.3816604018211365\n",
      "Step 2360: val loss: 0.36887526512145996\n",
      "Step 2361: train loss: 0.38141772150993347\n",
      "Step 2361: val loss: 0.36864519119262695\n",
      "Step 2362: train loss: 0.3811754882335663\n",
      "Step 2362: val loss: 0.36841559410095215\n",
      "Step 2363: train loss: 0.3809336721897125\n",
      "Step 2363: val loss: 0.3681861162185669\n",
      "Step 2364: train loss: 0.3806920647621155\n",
      "Step 2364: val loss: 0.3679571747779846\n",
      "Step 2365: train loss: 0.3804510235786438\n",
      "Step 2365: val loss: 0.36772865056991577\n",
      "Step 2366: train loss: 0.3802103102207184\n",
      "Step 2366: val loss: 0.36750051379203796\n",
      "Step 2367: train loss: 0.3799702227115631\n",
      "Step 2367: val loss: 0.36727288365364075\n",
      "Step 2368: train loss: 0.37973034381866455\n",
      "Step 2368: val loss: 0.36704564094543457\n",
      "Step 2369: train loss: 0.37949106097221375\n",
      "Step 2369: val loss: 0.366818904876709\n",
      "Step 2370: train loss: 0.37925219535827637\n",
      "Step 2370: val loss: 0.3665926158428192\n",
      "Step 2371: train loss: 0.3790138065814972\n",
      "Step 2371: val loss: 0.3663666248321533\n",
      "Step 2372: train loss: 0.37877583503723145\n",
      "Step 2372: val loss: 0.36614128947257996\n",
      "Step 2373: train loss: 0.3785383403301239\n",
      "Step 2373: val loss: 0.36591628193855286\n",
      "Step 2374: train loss: 0.3783012628555298\n",
      "Step 2374: val loss: 0.36569157242774963\n",
      "Step 2375: train loss: 0.3780643939971924\n",
      "Step 2375: val loss: 0.36546725034713745\n",
      "Step 2376: train loss: 0.3778280019760132\n",
      "Step 2376: val loss: 0.36524340510368347\n",
      "Step 2377: train loss: 0.3775920569896698\n",
      "Step 2377: val loss: 0.3650200068950653\n",
      "Step 2378: train loss: 0.37735649943351746\n",
      "Step 2378: val loss: 0.36479705572128296\n",
      "Step 2379: train loss: 0.3771214485168457\n",
      "Step 2379: val loss: 0.36457449197769165\n",
      "Step 2380: train loss: 0.37688684463500977\n",
      "Step 2380: val loss: 0.36435237526893616\n",
      "Step 2381: train loss: 0.37665268778800964\n",
      "Step 2381: val loss: 0.36413082480430603\n",
      "Step 2382: train loss: 0.37641894817352295\n",
      "Step 2382: val loss: 0.36390960216522217\n",
      "Step 2383: train loss: 0.37618565559387207\n",
      "Step 2383: val loss: 0.36368873715400696\n",
      "Step 2384: train loss: 0.3759527802467346\n",
      "Step 2384: val loss: 0.36346831917762756\n",
      "Step 2385: train loss: 0.37572020292282104\n",
      "Step 2385: val loss: 0.36324816942214966\n",
      "Step 2386: train loss: 0.3754879832267761\n",
      "Step 2386: val loss: 0.3630284368991852\n",
      "Step 2387: train loss: 0.3752562403678894\n",
      "Step 2387: val loss: 0.3628092110157013\n",
      "Step 2388: train loss: 0.3750249445438385\n",
      "Step 2388: val loss: 0.36259034276008606\n",
      "Step 2389: train loss: 0.3747940957546234\n",
      "Step 2389: val loss: 0.3623720109462738\n",
      "Step 2390: train loss: 0.3745635747909546\n",
      "Step 2390: val loss: 0.3621540367603302\n",
      "Step 2391: train loss: 0.37433362007141113\n",
      "Step 2391: val loss: 0.36193645000457764\n",
      "Step 2392: train loss: 0.3741040527820587\n",
      "Step 2392: val loss: 0.36171936988830566\n",
      "Step 2393: train loss: 0.3738749027252197\n",
      "Step 2393: val loss: 0.3615027666091919\n",
      "Step 2394: train loss: 0.37364619970321655\n",
      "Step 2394: val loss: 0.36128634214401245\n",
      "Step 2395: train loss: 0.3734177350997925\n",
      "Step 2395: val loss: 0.3610703647136688\n",
      "Step 2396: train loss: 0.37318965792655945\n",
      "Step 2396: val loss: 0.3608546853065491\n",
      "Step 2397: train loss: 0.372962087392807\n",
      "Step 2397: val loss: 0.36063966155052185\n",
      "Step 2398: train loss: 0.3727349638938904\n",
      "Step 2398: val loss: 0.3604249060153961\n",
      "Step 2399: train loss: 0.37250816822052\n",
      "Step 2399: val loss: 0.3602106273174286\n",
      "Step 2400: train loss: 0.37228184938430786\n",
      "Step 2400: val loss: 0.3599967956542969\n",
      "Step 2401: train loss: 0.3720559775829315\n",
      "Step 2401: val loss: 0.3597833514213562\n",
      "Step 2402: train loss: 0.3718305826187134\n",
      "Step 2402: val loss: 0.35957029461860657\n",
      "Step 2403: train loss: 0.37160524725914\n",
      "Step 2403: val loss: 0.35935738682746887\n",
      "Step 2404: train loss: 0.3713805079460144\n",
      "Step 2404: val loss: 0.35914507508277893\n",
      "Step 2405: train loss: 0.37115612626075745\n",
      "Step 2405: val loss: 0.3589332103729248\n",
      "Step 2406: train loss: 0.3709321916103363\n",
      "Step 2406: val loss: 0.35872170329093933\n",
      "Step 2407: train loss: 0.3707086145877838\n",
      "Step 2407: val loss: 0.3585106432437897\n",
      "Step 2408: train loss: 0.3704855442047119\n",
      "Step 2408: val loss: 0.35829994082450867\n",
      "Step 2409: train loss: 0.37026283144950867\n",
      "Step 2409: val loss: 0.35808977484703064\n",
      "Step 2410: train loss: 0.3700405955314636\n",
      "Step 2410: val loss: 0.3578798174858093\n",
      "Step 2411: train loss: 0.369818776845932\n",
      "Step 2411: val loss: 0.35767021775245667\n",
      "Step 2412: train loss: 0.36959710717201233\n",
      "Step 2412: val loss: 0.35746100544929504\n",
      "Step 2413: train loss: 0.36937597393989563\n",
      "Step 2413: val loss: 0.3572523593902588\n",
      "Step 2414: train loss: 0.3691551983356476\n",
      "Step 2414: val loss: 0.357043981552124\n",
      "Step 2415: train loss: 0.36893486976623535\n",
      "Step 2415: val loss: 0.3568360507488251\n",
      "Step 2416: train loss: 0.36871492862701416\n",
      "Step 2416: val loss: 0.35662853717803955\n",
      "Step 2417: train loss: 0.36849549412727356\n",
      "Step 2417: val loss: 0.3564213812351227\n",
      "Step 2418: train loss: 0.3682763874530792\n",
      "Step 2418: val loss: 0.35621464252471924\n",
      "Step 2419: train loss: 0.3680574893951416\n",
      "Step 2419: val loss: 0.3560081422328949\n",
      "Step 2420: train loss: 0.3678390085697174\n",
      "Step 2420: val loss: 0.35580211877822876\n",
      "Step 2421: train loss: 0.3676210343837738\n",
      "Step 2421: val loss: 0.35559648275375366\n",
      "Step 2422: train loss: 0.36740341782569885\n",
      "Step 2422: val loss: 0.355391263961792\n",
      "Step 2423: train loss: 0.36718621850013733\n",
      "Step 2423: val loss: 0.35518646240234375\n",
      "Step 2424: train loss: 0.36696940660476685\n",
      "Step 2424: val loss: 0.3549819588661194\n",
      "Step 2425: train loss: 0.3667530119419098\n",
      "Step 2425: val loss: 0.354777991771698\n",
      "Step 2426: train loss: 0.36653706431388855\n",
      "Step 2426: val loss: 0.3545742332935333\n",
      "Step 2427: train loss: 0.3663214147090912\n",
      "Step 2427: val loss: 0.35437092185020447\n",
      "Step 2428: train loss: 0.3661061227321625\n",
      "Step 2428: val loss: 0.3541679382324219\n",
      "Step 2429: train loss: 0.36589115858078003\n",
      "Step 2429: val loss: 0.3539654612541199\n",
      "Step 2430: train loss: 0.365676611661911\n",
      "Step 2430: val loss: 0.35376328229904175\n",
      "Step 2431: train loss: 0.3654625415802002\n",
      "Step 2431: val loss: 0.3535616099834442\n",
      "Step 2432: train loss: 0.3652489185333252\n",
      "Step 2432: val loss: 0.35336029529571533\n",
      "Step 2433: train loss: 0.36503565311431885\n",
      "Step 2433: val loss: 0.35315921902656555\n",
      "Step 2434: train loss: 0.3648225963115692\n",
      "Step 2434: val loss: 0.352958619594574\n",
      "Step 2435: train loss: 0.364609956741333\n",
      "Step 2435: val loss: 0.3527581989765167\n",
      "Step 2436: train loss: 0.3643978238105774\n",
      "Step 2436: val loss: 0.3525584042072296\n",
      "Step 2437: train loss: 0.36418598890304565\n",
      "Step 2437: val loss: 0.352358877658844\n",
      "Step 2438: train loss: 0.36397454142570496\n",
      "Step 2438: val loss: 0.3521597981452942\n",
      "Step 2439: train loss: 0.3637634813785553\n",
      "Step 2439: val loss: 0.3519611656665802\n",
      "Step 2440: train loss: 0.36355289816856384\n",
      "Step 2440: val loss: 0.35176268219947815\n",
      "Step 2441: train loss: 0.3633424639701843\n",
      "Step 2441: val loss: 0.35156458616256714\n",
      "Step 2442: train loss: 0.363132506608963\n",
      "Step 2442: val loss: 0.35136690735816956\n",
      "Step 2443: train loss: 0.3629229664802551\n",
      "Step 2443: val loss: 0.3511697053909302\n",
      "Step 2444: train loss: 0.3627138137817383\n",
      "Step 2444: val loss: 0.3509727716445923\n",
      "Step 2445: train loss: 0.3625050187110901\n",
      "Step 2445: val loss: 0.3507763743400574\n",
      "Step 2446: train loss: 0.3622966706752777\n",
      "Step 2446: val loss: 0.35058024525642395\n",
      "Step 2447: train loss: 0.36208856105804443\n",
      "Step 2447: val loss: 0.35038435459136963\n",
      "Step 2448: train loss: 0.36188074946403503\n",
      "Step 2448: val loss: 0.35018885135650635\n",
      "Step 2449: train loss: 0.3616733253002167\n",
      "Step 2449: val loss: 0.3499937653541565\n",
      "Step 2450: train loss: 0.3614664375782013\n",
      "Step 2450: val loss: 0.3497992157936096\n",
      "Step 2451: train loss: 0.3612598776817322\n",
      "Step 2451: val loss: 0.349604994058609\n",
      "Step 2452: train loss: 0.3610537052154541\n",
      "Step 2452: val loss: 0.3494110703468323\n",
      "Step 2453: train loss: 0.36084800958633423\n",
      "Step 2453: val loss: 0.3492174744606018\n",
      "Step 2454: train loss: 0.3606424331665039\n",
      "Step 2454: val loss: 0.34902429580688477\n",
      "Step 2455: train loss: 0.360437273979187\n",
      "Step 2455: val loss: 0.34883129596710205\n",
      "Step 2456: train loss: 0.36023256182670593\n",
      "Step 2456: val loss: 0.3486389219760895\n",
      "Step 2457: train loss: 0.3600282073020935\n",
      "Step 2457: val loss: 0.3484468460083008\n",
      "Step 2458: train loss: 0.3598242402076721\n",
      "Step 2458: val loss: 0.34825509786605835\n",
      "Step 2459: train loss: 0.3596206605434418\n",
      "Step 2459: val loss: 0.3480636179447174\n",
      "Step 2460: train loss: 0.35941725969314575\n",
      "Step 2460: val loss: 0.3478725552558899\n",
      "Step 2461: train loss: 0.3592143952846527\n",
      "Step 2461: val loss: 0.3476819097995758\n",
      "Step 2462: train loss: 0.35901176929473877\n",
      "Step 2462: val loss: 0.34749162197113037\n",
      "Step 2463: train loss: 0.35880956053733826\n",
      "Step 2463: val loss: 0.3473016917705536\n",
      "Step 2464: train loss: 0.35860785841941833\n",
      "Step 2464: val loss: 0.34711211919784546\n",
      "Step 2465: train loss: 0.3584064245223999\n",
      "Step 2465: val loss: 0.3469228446483612\n",
      "Step 2466: train loss: 0.3582051992416382\n",
      "Step 2466: val loss: 0.34673386812210083\n",
      "Step 2467: train loss: 0.3580043911933899\n",
      "Step 2467: val loss: 0.34654539823532104\n",
      "Step 2468: train loss: 0.3578040897846222\n",
      "Step 2468: val loss: 0.34635716676712036\n",
      "Step 2469: train loss: 0.3576040267944336\n",
      "Step 2469: val loss: 0.34616944193840027\n",
      "Step 2470: train loss: 0.35740432143211365\n",
      "Step 2470: val loss: 0.3459819555282593\n",
      "Step 2471: train loss: 0.35720521211624146\n",
      "Step 2471: val loss: 0.34579476714134216\n",
      "Step 2472: train loss: 0.3570061922073364\n",
      "Step 2472: val loss: 0.3456079661846161\n",
      "Step 2473: train loss: 0.3568074405193329\n",
      "Step 2473: val loss: 0.34542152285575867\n",
      "Step 2474: train loss: 0.3566092550754547\n",
      "Step 2474: val loss: 0.3452354669570923\n",
      "Step 2475: train loss: 0.3564113676548004\n",
      "Step 2475: val loss: 0.34504979848861694\n",
      "Step 2476: train loss: 0.35621383786201477\n",
      "Step 2476: val loss: 0.3448646366596222\n",
      "Step 2477: train loss: 0.35601678490638733\n",
      "Step 2477: val loss: 0.344679594039917\n",
      "Step 2478: train loss: 0.35581985116004944\n",
      "Step 2478: val loss: 0.34449493885040283\n",
      "Step 2479: train loss: 0.35562339425086975\n",
      "Step 2479: val loss: 0.34431058168411255\n",
      "Step 2480: train loss: 0.35542720556259155\n",
      "Step 2480: val loss: 0.3441266119480133\n",
      "Step 2481: train loss: 0.35523149371147156\n",
      "Step 2481: val loss: 0.3439430892467499\n",
      "Step 2482: train loss: 0.35503607988357544\n",
      "Step 2482: val loss: 0.3437597453594208\n",
      "Step 2483: train loss: 0.3548409640789032\n",
      "Step 2483: val loss: 0.3435768783092499\n",
      "Step 2484: train loss: 0.3546461760997772\n",
      "Step 2484: val loss: 0.3433942198753357\n",
      "Step 2485: train loss: 0.3544518053531647\n",
      "Step 2485: val loss: 0.34321197867393494\n",
      "Step 2486: train loss: 0.3542577922344208\n",
      "Step 2486: val loss: 0.3430301547050476\n",
      "Step 2487: train loss: 0.35406404733657837\n",
      "Step 2487: val loss: 0.34284865856170654\n",
      "Step 2488: train loss: 0.3538708984851837\n",
      "Step 2488: val loss: 0.34266749024391174\n",
      "Step 2489: train loss: 0.35367780923843384\n",
      "Step 2489: val loss: 0.34248656034469604\n",
      "Step 2490: train loss: 0.353485107421875\n",
      "Step 2490: val loss: 0.3423060476779938\n",
      "Step 2491: train loss: 0.3532928228378296\n",
      "Step 2491: val loss: 0.34212589263916016\n",
      "Step 2492: train loss: 0.3531009256839752\n",
      "Step 2492: val loss: 0.3419460356235504\n",
      "Step 2493: train loss: 0.3529093563556671\n",
      "Step 2493: val loss: 0.3417665958404541\n",
      "Step 2494: train loss: 0.35271796584129333\n",
      "Step 2494: val loss: 0.3415873348712921\n",
      "Step 2495: train loss: 0.35252705216407776\n",
      "Step 2495: val loss: 0.34140852093696594\n",
      "Step 2496: train loss: 0.35233643651008606\n",
      "Step 2496: val loss: 0.34123003482818604\n",
      "Step 2497: train loss: 0.352146178483963\n",
      "Step 2497: val loss: 0.34105199575424194\n",
      "Step 2498: train loss: 0.3519562780857086\n",
      "Step 2498: val loss: 0.3408741354942322\n",
      "Step 2499: train loss: 0.3517666757106781\n",
      "Step 2499: val loss: 0.3406966030597687\n",
      "Step 2500: train loss: 0.35157740116119385\n",
      "Step 2500: val loss: 0.3405194878578186\n",
      "Step 2501: train loss: 0.35138845443725586\n",
      "Step 2501: val loss: 0.34034264087677\n",
      "Step 2502: train loss: 0.3511999547481537\n",
      "Step 2502: val loss: 0.34016621112823486\n",
      "Step 2503: train loss: 0.351011723279953\n",
      "Step 2503: val loss: 0.33999010920524597\n",
      "Step 2504: train loss: 0.3508237302303314\n",
      "Step 2504: val loss: 0.3398142158985138\n",
      "Step 2505: train loss: 0.35063618421554565\n",
      "Step 2505: val loss: 0.33963873982429504\n",
      "Step 2506: train loss: 0.3504489064216614\n",
      "Step 2506: val loss: 0.33946359157562256\n",
      "Step 2507: train loss: 0.35026201605796814\n",
      "Step 2507: val loss: 0.33928877115249634\n",
      "Step 2508: train loss: 0.35007554292678833\n",
      "Step 2508: val loss: 0.339114248752594\n",
      "Step 2509: train loss: 0.34988918900489807\n",
      "Step 2509: val loss: 0.33893999457359314\n",
      "Step 2510: train loss: 0.34970325231552124\n",
      "Step 2510: val loss: 0.3387661874294281\n",
      "Step 2511: train loss: 0.3495176434516907\n",
      "Step 2511: val loss: 0.3385927677154541\n",
      "Step 2512: train loss: 0.3493324816226959\n",
      "Step 2512: val loss: 0.3384196162223816\n",
      "Step 2513: train loss: 0.34914764761924744\n",
      "Step 2513: val loss: 0.3382466435432434\n",
      "Step 2514: train loss: 0.3489629626274109\n",
      "Step 2514: val loss: 0.33807411789894104\n",
      "Step 2515: train loss: 0.34877869486808777\n",
      "Step 2515: val loss: 0.3379019498825073\n",
      "Step 2516: train loss: 0.3485947847366333\n",
      "Step 2516: val loss: 0.3377301096916199\n",
      "Step 2517: train loss: 0.3484112620353699\n",
      "Step 2517: val loss: 0.3375585377216339\n",
      "Step 2518: train loss: 0.3482280373573303\n",
      "Step 2518: val loss: 0.3373873233795166\n",
      "Step 2519: train loss: 0.3480450510978699\n",
      "Step 2519: val loss: 0.33721646666526794\n",
      "Step 2520: train loss: 0.3478623926639557\n",
      "Step 2520: val loss: 0.3370458483695984\n",
      "Step 2521: train loss: 0.3476801812648773\n",
      "Step 2521: val loss: 0.33687567710876465\n",
      "Step 2522: train loss: 0.34749817848205566\n",
      "Step 2522: val loss: 0.33670574426651\n",
      "Step 2523: train loss: 0.3473166525363922\n",
      "Step 2523: val loss: 0.3365360498428345\n",
      "Step 2524: train loss: 0.3471352756023407\n",
      "Step 2524: val loss: 0.33636683225631714\n",
      "Step 2525: train loss: 0.3469542860984802\n",
      "Step 2525: val loss: 0.3361978232860565\n",
      "Step 2526: train loss: 0.34677359461784363\n",
      "Step 2526: val loss: 0.3360292911529541\n",
      "Step 2527: train loss: 0.34659337997436523\n",
      "Step 2527: val loss: 0.3358609974384308\n",
      "Step 2528: train loss: 0.34641343355178833\n",
      "Step 2528: val loss: 0.33569300174713135\n",
      "Step 2529: train loss: 0.34623366594314575\n",
      "Step 2529: val loss: 0.3355253338813782\n",
      "Step 2530: train loss: 0.3460543155670166\n",
      "Step 2530: val loss: 0.33535802364349365\n",
      "Step 2531: train loss: 0.3458752930164337\n",
      "Step 2531: val loss: 0.3351909816265106\n",
      "Step 2532: train loss: 0.34569671750068665\n",
      "Step 2532: val loss: 0.3350241482257843\n",
      "Step 2533: train loss: 0.3455181419849396\n",
      "Step 2533: val loss: 0.3348577320575714\n",
      "Step 2534: train loss: 0.34533998370170593\n",
      "Step 2534: val loss: 0.3346915543079376\n",
      "Step 2535: train loss: 0.34516218304634094\n",
      "Step 2535: val loss: 0.33452579379081726\n",
      "Step 2536: train loss: 0.34498485922813416\n",
      "Step 2536: val loss: 0.33436036109924316\n",
      "Step 2537: train loss: 0.3448077440261841\n",
      "Step 2537: val loss: 0.33419516682624817\n",
      "Step 2538: train loss: 0.3446308672428131\n",
      "Step 2538: val loss: 0.33403030037879944\n",
      "Step 2539: train loss: 0.3444543480873108\n",
      "Step 2539: val loss: 0.333865761756897\n",
      "Step 2540: train loss: 0.3442781865596771\n",
      "Step 2540: val loss: 0.33370161056518555\n",
      "Step 2541: train loss: 0.34410232305526733\n",
      "Step 2541: val loss: 0.3335375487804413\n",
      "Step 2542: train loss: 0.34392663836479187\n",
      "Step 2542: val loss: 0.33337390422821045\n",
      "Step 2543: train loss: 0.34375137090682983\n",
      "Step 2543: val loss: 0.3332105875015259\n",
      "Step 2544: train loss: 0.3435764014720917\n",
      "Step 2544: val loss: 0.3330475986003876\n",
      "Step 2545: train loss: 0.3434019088745117\n",
      "Step 2545: val loss: 0.3328850269317627\n",
      "Step 2546: train loss: 0.3432275652885437\n",
      "Step 2546: val loss: 0.33272260427474976\n",
      "Step 2547: train loss: 0.34305354952812195\n",
      "Step 2547: val loss: 0.3325604796409607\n",
      "Step 2548: train loss: 0.34287986159324646\n",
      "Step 2548: val loss: 0.33239877223968506\n",
      "Step 2549: train loss: 0.34270650148391724\n",
      "Step 2549: val loss: 0.3322373628616333\n",
      "Step 2550: train loss: 0.34253349900245667\n",
      "Step 2550: val loss: 0.33207619190216064\n",
      "Step 2551: train loss: 0.34236058592796326\n",
      "Step 2551: val loss: 0.33191531896591187\n",
      "Step 2552: train loss: 0.34218817949295044\n",
      "Step 2552: val loss: 0.33175480365753174\n",
      "Step 2553: train loss: 0.34201595187187195\n",
      "Step 2553: val loss: 0.33159464597702026\n",
      "Step 2554: train loss: 0.34184420108795166\n",
      "Step 2554: val loss: 0.3314346969127655\n",
      "Step 2555: train loss: 0.3416725993156433\n",
      "Step 2555: val loss: 0.33127495646476746\n",
      "Step 2556: train loss: 0.3415012061595917\n",
      "Step 2556: val loss: 0.33111557364463806\n",
      "Step 2557: train loss: 0.34133031964302063\n",
      "Step 2557: val loss: 0.3309566080570221\n",
      "Step 2558: train loss: 0.34115979075431824\n",
      "Step 2558: val loss: 0.33079779148101807\n",
      "Step 2559: train loss: 0.340989351272583\n",
      "Step 2559: val loss: 0.3306393027305603\n",
      "Step 2560: train loss: 0.34081926941871643\n",
      "Step 2560: val loss: 0.3304812014102936\n",
      "Step 2561: train loss: 0.3406495749950409\n",
      "Step 2561: val loss: 0.33032336831092834\n",
      "Step 2562: train loss: 0.34048014879226685\n",
      "Step 2562: val loss: 0.33016589283943176\n",
      "Step 2563: train loss: 0.34031111001968384\n",
      "Step 2563: val loss: 0.3300085961818695\n",
      "Step 2564: train loss: 0.34014230966567993\n",
      "Step 2564: val loss: 0.3298517167568207\n",
      "Step 2565: train loss: 0.3399737477302551\n",
      "Step 2565: val loss: 0.32969507575035095\n",
      "Step 2566: train loss: 0.33980557322502136\n",
      "Step 2566: val loss: 0.3295387923717499\n",
      "Step 2567: train loss: 0.3396376669406891\n",
      "Step 2567: val loss: 0.32938265800476074\n",
      "Step 2568: train loss: 0.3394700586795807\n",
      "Step 2568: val loss: 0.3292270004749298\n",
      "Step 2569: train loss: 0.339302659034729\n",
      "Step 2569: val loss: 0.3290715515613556\n",
      "Step 2570: train loss: 0.33913567662239075\n",
      "Step 2570: val loss: 0.32891637086868286\n",
      "Step 2571: train loss: 0.3389691710472107\n",
      "Step 2571: val loss: 0.328761488199234\n",
      "Step 2572: train loss: 0.3388025164604187\n",
      "Step 2572: val loss: 0.32860687375068665\n",
      "Step 2573: train loss: 0.33863645792007446\n",
      "Step 2573: val loss: 0.3284526467323303\n",
      "Step 2574: train loss: 0.3384706377983093\n",
      "Step 2574: val loss: 0.3282986283302307\n",
      "Step 2575: train loss: 0.33830511569976807\n",
      "Step 2575: val loss: 0.328144907951355\n",
      "Step 2576: train loss: 0.3381398022174835\n",
      "Step 2576: val loss: 0.3279914855957031\n",
      "Step 2577: train loss: 0.3379749059677124\n",
      "Step 2577: val loss: 0.32783836126327515\n",
      "Step 2578: train loss: 0.3378102481365204\n",
      "Step 2578: val loss: 0.32768556475639343\n",
      "Step 2579: train loss: 0.337645947933197\n",
      "Step 2579: val loss: 0.32753297686576843\n",
      "Step 2580: train loss: 0.3374817371368408\n",
      "Step 2580: val loss: 0.32738080620765686\n",
      "Step 2581: train loss: 0.33731797337532043\n",
      "Step 2581: val loss: 0.3272288739681244\n",
      "Step 2582: train loss: 0.3371545076370239\n",
      "Step 2582: val loss: 0.327077180147171\n",
      "Step 2583: train loss: 0.33699142932891846\n",
      "Step 2583: val loss: 0.3269258439540863\n",
      "Step 2584: train loss: 0.3368285000324249\n",
      "Step 2584: val loss: 0.32677459716796875\n",
      "Step 2585: train loss: 0.3366658389568329\n",
      "Step 2585: val loss: 0.3266238272190094\n",
      "Step 2586: train loss: 0.3365034759044647\n",
      "Step 2586: val loss: 0.3264733850955963\n",
      "Step 2587: train loss: 0.3363414406776428\n",
      "Step 2587: val loss: 0.3263230621814728\n",
      "Step 2588: train loss: 0.3361797034740448\n",
      "Step 2588: val loss: 0.3261731266975403\n",
      "Step 2589: train loss: 0.3360182046890259\n",
      "Step 2589: val loss: 0.3260234296321869\n",
      "Step 2590: train loss: 0.3358570635318756\n",
      "Step 2590: val loss: 0.32587411999702454\n",
      "Step 2591: train loss: 0.335696280002594\n",
      "Step 2591: val loss: 0.3257250189781189\n",
      "Step 2592: train loss: 0.3355356454849243\n",
      "Step 2592: val loss: 0.32557612657546997\n",
      "Step 2593: train loss: 0.3353753089904785\n",
      "Step 2593: val loss: 0.32542768120765686\n",
      "Step 2594: train loss: 0.33521533012390137\n",
      "Step 2594: val loss: 0.3252792954444885\n",
      "Step 2595: train loss: 0.33505547046661377\n",
      "Step 2595: val loss: 0.3251313865184784\n",
      "Step 2596: train loss: 0.3348959684371948\n",
      "Step 2596: val loss: 0.3249836266040802\n",
      "Step 2597: train loss: 0.33473682403564453\n",
      "Step 2597: val loss: 0.32483619451522827\n",
      "Step 2598: train loss: 0.3345779478549957\n",
      "Step 2598: val loss: 0.32468894124031067\n",
      "Step 2599: train loss: 0.33441925048828125\n",
      "Step 2599: val loss: 0.32454216480255127\n",
      "Step 2600: train loss: 0.33426085114479065\n",
      "Step 2600: val loss: 0.3243955075740814\n",
      "Step 2601: train loss: 0.33410289883613586\n",
      "Step 2601: val loss: 0.3242492973804474\n",
      "Step 2602: train loss: 0.3339451253414154\n",
      "Step 2602: val loss: 0.3241032063961029\n",
      "Step 2603: train loss: 0.3337875306606293\n",
      "Step 2603: val loss: 0.32395750284194946\n",
      "Step 2604: train loss: 0.33363038301467896\n",
      "Step 2604: val loss: 0.32381197810173035\n",
      "Step 2605: train loss: 0.33347341418266296\n",
      "Step 2605: val loss: 0.32366687059402466\n",
      "Step 2606: train loss: 0.3333168625831604\n",
      "Step 2606: val loss: 0.32352182269096375\n",
      "Step 2607: train loss: 0.33316051959991455\n",
      "Step 2607: val loss: 0.3233771324157715\n",
      "Step 2608: train loss: 0.33300432562828064\n",
      "Step 2608: val loss: 0.32323282957077026\n",
      "Step 2609: train loss: 0.33284854888916016\n",
      "Step 2609: val loss: 0.3230886161327362\n",
      "Step 2610: train loss: 0.33269286155700684\n",
      "Step 2610: val loss: 0.3229447603225708\n",
      "Step 2611: train loss: 0.33253759145736694\n",
      "Step 2611: val loss: 0.32280126214027405\n",
      "Step 2612: train loss: 0.3323827087879181\n",
      "Step 2612: val loss: 0.3226580321788788\n",
      "Step 2613: train loss: 0.3322279453277588\n",
      "Step 2613: val loss: 0.32251495122909546\n",
      "Step 2614: train loss: 0.33207347989082336\n",
      "Step 2614: val loss: 0.3223721981048584\n",
      "Step 2615: train loss: 0.3319193422794342\n",
      "Step 2615: val loss: 0.3222297132015228\n",
      "Step 2616: train loss: 0.33176544308662415\n",
      "Step 2616: val loss: 0.32208752632141113\n",
      "Step 2617: train loss: 0.33161187171936035\n",
      "Step 2617: val loss: 0.32194554805755615\n",
      "Step 2618: train loss: 0.33145850896835327\n",
      "Step 2618: val loss: 0.3218039274215698\n",
      "Step 2619: train loss: 0.3313054144382477\n",
      "Step 2619: val loss: 0.3216626048088074\n",
      "Step 2620: train loss: 0.33115270733833313\n",
      "Step 2620: val loss: 0.32152143120765686\n",
      "Step 2621: train loss: 0.33100005984306335\n",
      "Step 2621: val loss: 0.32138052582740784\n",
      "Step 2622: train loss: 0.330847829580307\n",
      "Step 2622: val loss: 0.32123997807502747\n",
      "Step 2623: train loss: 0.33069583773612976\n",
      "Step 2623: val loss: 0.3210996389389038\n",
      "Step 2624: train loss: 0.330544114112854\n",
      "Step 2624: val loss: 0.32095956802368164\n",
      "Step 2625: train loss: 0.33039262890815735\n",
      "Step 2625: val loss: 0.32081979513168335\n",
      "Step 2626: train loss: 0.33024147152900696\n",
      "Step 2626: val loss: 0.3206802010536194\n",
      "Step 2627: train loss: 0.33009058237075806\n",
      "Step 2627: val loss: 0.3205409646034241\n",
      "Step 2628: train loss: 0.3299398422241211\n",
      "Step 2628: val loss: 0.3204019367694855\n",
      "Step 2629: train loss: 0.32978948950767517\n",
      "Step 2629: val loss: 0.32026320695877075\n",
      "Step 2630: train loss: 0.32963937520980835\n",
      "Step 2630: val loss: 0.3201248347759247\n",
      "Step 2631: train loss: 0.329489529132843\n",
      "Step 2631: val loss: 0.31998658180236816\n",
      "Step 2632: train loss: 0.3293399512767792\n",
      "Step 2632: val loss: 0.3198484778404236\n",
      "Step 2633: train loss: 0.3291906714439392\n",
      "Step 2633: val loss: 0.3197108805179596\n",
      "Step 2634: train loss: 0.32904163002967834\n",
      "Step 2634: val loss: 0.31957340240478516\n",
      "Step 2635: train loss: 0.3288928270339966\n",
      "Step 2635: val loss: 0.319436252117157\n",
      "Step 2636: train loss: 0.32874423265457153\n",
      "Step 2636: val loss: 0.3192993700504303\n",
      "Step 2637: train loss: 0.32859599590301514\n",
      "Step 2637: val loss: 0.3191627860069275\n",
      "Step 2638: train loss: 0.3284481465816498\n",
      "Step 2638: val loss: 0.3190263509750366\n",
      "Step 2639: train loss: 0.3283003270626068\n",
      "Step 2639: val loss: 0.31889021396636963\n",
      "Step 2640: train loss: 0.3281528353691101\n",
      "Step 2640: val loss: 0.3187543749809265\n",
      "Step 2641: train loss: 0.32800570130348206\n",
      "Step 2641: val loss: 0.3186187148094177\n",
      "Step 2642: train loss: 0.3278586268424988\n",
      "Step 2642: val loss: 0.31848326325416565\n",
      "Step 2643: train loss: 0.32771196961402893\n",
      "Step 2643: val loss: 0.3183482587337494\n",
      "Step 2644: train loss: 0.32756558060646057\n",
      "Step 2644: val loss: 0.3182133436203003\n",
      "Step 2645: train loss: 0.32741931080818176\n",
      "Step 2645: val loss: 0.31807875633239746\n",
      "Step 2646: train loss: 0.3272733688354492\n",
      "Step 2646: val loss: 0.3179444670677185\n",
      "Step 2647: train loss: 0.32712775468826294\n",
      "Step 2647: val loss: 0.31781044602394104\n",
      "Step 2648: train loss: 0.32698237895965576\n",
      "Step 2648: val loss: 0.3176765739917755\n",
      "Step 2649: train loss: 0.3268372118473053\n",
      "Step 2649: val loss: 0.3175429701805115\n",
      "Step 2650: train loss: 0.3266923427581787\n",
      "Step 2650: val loss: 0.3174096941947937\n",
      "Step 2651: train loss: 0.32654786109924316\n",
      "Step 2651: val loss: 0.31727659702301025\n",
      "Step 2652: train loss: 0.3264034390449524\n",
      "Step 2652: val loss: 0.3171437680721283\n",
      "Step 2653: train loss: 0.3262592852115631\n",
      "Step 2653: val loss: 0.3170112371444702\n",
      "Step 2654: train loss: 0.3261154294013977\n",
      "Step 2654: val loss: 0.31687891483306885\n",
      "Step 2655: train loss: 0.32597172260284424\n",
      "Step 2655: val loss: 0.3167467713356018\n",
      "Step 2656: train loss: 0.32582834362983704\n",
      "Step 2656: val loss: 0.3166149854660034\n",
      "Step 2657: train loss: 0.3256852924823761\n",
      "Step 2657: val loss: 0.3164834976196289\n",
      "Step 2658: train loss: 0.32554253935813904\n",
      "Step 2658: val loss: 0.31635209918022156\n",
      "Step 2659: train loss: 0.32539990544319153\n",
      "Step 2659: val loss: 0.31622105836868286\n",
      "Step 2660: train loss: 0.3252575695514679\n",
      "Step 2660: val loss: 0.31609031558036804\n",
      "Step 2661: train loss: 0.32511553168296814\n",
      "Step 2661: val loss: 0.3159598112106323\n",
      "Step 2662: train loss: 0.3249736428260803\n",
      "Step 2662: val loss: 0.31582942605018616\n",
      "Step 2663: train loss: 0.32483211159706116\n",
      "Step 2663: val loss: 0.31569942831993103\n",
      "Step 2664: train loss: 0.3246907889842987\n",
      "Step 2664: val loss: 0.31556954979896545\n",
      "Step 2665: train loss: 0.3245496153831482\n",
      "Step 2665: val loss: 0.31543996930122375\n",
      "Step 2666: train loss: 0.32440879940986633\n",
      "Step 2666: val loss: 0.31531068682670593\n",
      "Step 2667: train loss: 0.32426828145980835\n",
      "Step 2667: val loss: 0.31518152356147766\n",
      "Step 2668: train loss: 0.3241278827190399\n",
      "Step 2668: val loss: 0.31505268812179565\n",
      "Step 2669: train loss: 0.323987752199173\n",
      "Step 2669: val loss: 0.31492406129837036\n",
      "Step 2670: train loss: 0.3238479197025299\n",
      "Step 2670: val loss: 0.31479567289352417\n",
      "Step 2671: train loss: 0.32370832562446594\n",
      "Step 2671: val loss: 0.3146675229072571\n",
      "Step 2672: train loss: 0.3235689401626587\n",
      "Step 2672: val loss: 0.3145396113395691\n",
      "Step 2673: train loss: 0.3234298527240753\n",
      "Step 2673: val loss: 0.31441205739974976\n",
      "Step 2674: train loss: 0.3232910633087158\n",
      "Step 2674: val loss: 0.3142847418785095\n",
      "Step 2675: train loss: 0.3231523931026459\n",
      "Step 2675: val loss: 0.3141576051712036\n",
      "Step 2676: train loss: 0.32301411032676697\n",
      "Step 2676: val loss: 0.3140307068824768\n",
      "Step 2677: train loss: 0.3228759765625\n",
      "Step 2677: val loss: 0.31390395760536194\n",
      "Step 2678: train loss: 0.3227381110191345\n",
      "Step 2678: val loss: 0.3137776255607605\n",
      "Step 2679: train loss: 0.32260045409202576\n",
      "Step 2679: val loss: 0.31365156173706055\n",
      "Step 2680: train loss: 0.32246312499046326\n",
      "Step 2680: val loss: 0.31352558732032776\n",
      "Step 2681: train loss: 0.3223259150981903\n",
      "Step 2681: val loss: 0.3133998215198517\n",
      "Step 2682: train loss: 0.322189062833786\n",
      "Step 2682: val loss: 0.31327447295188904\n",
      "Step 2683: train loss: 0.3220524489879608\n",
      "Step 2683: val loss: 0.3131491541862488\n",
      "Step 2684: train loss: 0.3219158947467804\n",
      "Step 2684: val loss: 0.3130241632461548\n",
      "Step 2685: train loss: 0.3217797875404358\n",
      "Step 2685: val loss: 0.31289950013160706\n",
      "Step 2686: train loss: 0.3216438889503479\n",
      "Step 2686: val loss: 0.3127748668193817\n",
      "Step 2687: train loss: 0.32150810956954956\n",
      "Step 2687: val loss: 0.3126506507396698\n",
      "Step 2688: train loss: 0.3213726878166199\n",
      "Step 2688: val loss: 0.3125266432762146\n",
      "Step 2689: train loss: 0.3212374746799469\n",
      "Step 2689: val loss: 0.3124028444290161\n",
      "Step 2690: train loss: 0.32110244035720825\n",
      "Step 2690: val loss: 0.3122793138027191\n",
      "Step 2691: train loss: 0.32096773386001587\n",
      "Step 2691: val loss: 0.31215599179267883\n",
      "Step 2692: train loss: 0.32083332538604736\n",
      "Step 2692: val loss: 0.3120327889919281\n",
      "Step 2693: train loss: 0.32069894671440125\n",
      "Step 2693: val loss: 0.31190991401672363\n",
      "Step 2694: train loss: 0.3205649256706238\n",
      "Step 2694: val loss: 0.31178730726242065\n",
      "Step 2695: train loss: 0.3204311728477478\n",
      "Step 2695: val loss: 0.31166496872901917\n",
      "Step 2696: train loss: 0.3202974796295166\n",
      "Step 2696: val loss: 0.31154271960258484\n",
      "Step 2697: train loss: 0.32016417384147644\n",
      "Step 2697: val loss: 0.3114207684993744\n",
      "Step 2698: train loss: 0.32003116607666016\n",
      "Step 2698: val loss: 0.31129908561706543\n",
      "Step 2699: train loss: 0.3198982775211334\n",
      "Step 2699: val loss: 0.3111776113510132\n",
      "Step 2700: train loss: 0.3197656273841858\n",
      "Step 2700: val loss: 0.31105637550354004\n",
      "Step 2701: train loss: 0.31963324546813965\n",
      "Step 2701: val loss: 0.3109353184700012\n",
      "Step 2702: train loss: 0.3195011019706726\n",
      "Step 2702: val loss: 0.3108144998550415\n",
      "Step 2703: train loss: 0.3193691372871399\n",
      "Step 2703: val loss: 0.3106939494609833\n",
      "Step 2704: train loss: 0.31923750042915344\n",
      "Step 2704: val loss: 0.3105735778808594\n",
      "Step 2705: train loss: 0.31910601258277893\n",
      "Step 2705: val loss: 0.31045353412628174\n",
      "Step 2706: train loss: 0.3189747929573059\n",
      "Step 2706: val loss: 0.3103336989879608\n",
      "Step 2707: train loss: 0.3188438415527344\n",
      "Step 2707: val loss: 0.31021398305892944\n",
      "Step 2708: train loss: 0.31871306896209717\n",
      "Step 2708: val loss: 0.31009459495544434\n",
      "Step 2709: train loss: 0.3185824453830719\n",
      "Step 2709: val loss: 0.3099754750728607\n",
      "Step 2710: train loss: 0.31845226883888245\n",
      "Step 2710: val loss: 0.30985644459724426\n",
      "Step 2711: train loss: 0.3183220624923706\n",
      "Step 2711: val loss: 0.30973777174949646\n",
      "Step 2712: train loss: 0.3181922733783722\n",
      "Step 2712: val loss: 0.30961936712265015\n",
      "Step 2713: train loss: 0.31806275248527527\n",
      "Step 2713: val loss: 0.30950096249580383\n",
      "Step 2714: train loss: 0.3179333209991455\n",
      "Step 2714: val loss: 0.30938291549682617\n",
      "Step 2715: train loss: 0.31780412793159485\n",
      "Step 2715: val loss: 0.3092651069164276\n",
      "Step 2716: train loss: 0.31767532229423523\n",
      "Step 2716: val loss: 0.309147447347641\n",
      "Step 2717: train loss: 0.3175465762615204\n",
      "Step 2717: val loss: 0.30903011560440063\n",
      "Step 2718: train loss: 0.3174181580543518\n",
      "Step 2718: val loss: 0.3089130222797394\n",
      "Step 2719: train loss: 0.31728991866111755\n",
      "Step 2719: val loss: 0.3087960481643677\n",
      "Step 2720: train loss: 0.31716188788414\n",
      "Step 2720: val loss: 0.3086793124675751\n",
      "Step 2721: train loss: 0.31703415513038635\n",
      "Step 2721: val loss: 0.30856287479400635\n",
      "Step 2722: train loss: 0.31690657138824463\n",
      "Step 2722: val loss: 0.30844661593437195\n",
      "Step 2723: train loss: 0.3167791962623596\n",
      "Step 2723: val loss: 0.30833059549331665\n",
      "Step 2724: train loss: 0.31665217876434326\n",
      "Step 2724: val loss: 0.3082147538661957\n",
      "Step 2725: train loss: 0.3165254294872284\n",
      "Step 2725: val loss: 0.30809924006462097\n",
      "Step 2726: train loss: 0.3163986802101135\n",
      "Step 2726: val loss: 0.30798378586769104\n",
      "Step 2727: train loss: 0.31627222895622253\n",
      "Step 2727: val loss: 0.30786874890327454\n",
      "Step 2728: train loss: 0.3161461353302002\n",
      "Step 2728: val loss: 0.3077537417411804\n",
      "Step 2729: train loss: 0.31602010130882263\n",
      "Step 2729: val loss: 0.30763909220695496\n",
      "Step 2730: train loss: 0.31589433550834656\n",
      "Step 2730: val loss: 0.3075246810913086\n",
      "Step 2731: train loss: 0.3157690167427063\n",
      "Step 2731: val loss: 0.3074103593826294\n",
      "Step 2732: train loss: 0.3156436085700989\n",
      "Step 2732: val loss: 0.3072963356971741\n",
      "Step 2733: train loss: 0.3155185282230377\n",
      "Step 2733: val loss: 0.3071824312210083\n",
      "Step 2734: train loss: 0.3153936266899109\n",
      "Step 2734: val loss: 0.307068794965744\n",
      "Step 2735: train loss: 0.31526893377304077\n",
      "Step 2735: val loss: 0.306955486536026\n",
      "Step 2736: train loss: 0.3151445984840393\n",
      "Step 2736: val loss: 0.3068421483039856\n",
      "Step 2737: train loss: 0.31502023339271545\n",
      "Step 2737: val loss: 0.30672919750213623\n",
      "Step 2738: train loss: 0.31489622592926025\n",
      "Step 2738: val loss: 0.30661651492118835\n",
      "Step 2739: train loss: 0.3147725462913513\n",
      "Step 2739: val loss: 0.30650395154953003\n",
      "Step 2740: train loss: 0.31464892625808716\n",
      "Step 2740: val loss: 0.30639156699180603\n",
      "Step 2741: train loss: 0.31452563405036926\n",
      "Step 2741: val loss: 0.3062794804573059\n",
      "Step 2742: train loss: 0.3144025206565857\n",
      "Step 2742: val loss: 0.3061676025390625\n",
      "Step 2743: train loss: 0.31427961587905884\n",
      "Step 2743: val loss: 0.3060559630393982\n",
      "Step 2744: train loss: 0.3141569197177887\n",
      "Step 2744: val loss: 0.3059444725513458\n",
      "Step 2745: train loss: 0.31403449177742004\n",
      "Step 2745: val loss: 0.30583325028419495\n",
      "Step 2746: train loss: 0.3139123022556305\n",
      "Step 2746: val loss: 0.305722177028656\n",
      "Step 2747: train loss: 0.3137902021408081\n",
      "Step 2747: val loss: 0.30561134219169617\n",
      "Step 2748: train loss: 0.3136683404445648\n",
      "Step 2748: val loss: 0.3055006265640259\n",
      "Step 2749: train loss: 0.31354671716690063\n",
      "Step 2749: val loss: 0.30539023876190186\n",
      "Step 2750: train loss: 0.3134254217147827\n",
      "Step 2750: val loss: 0.30527999997138977\n",
      "Step 2751: train loss: 0.3133041560649872\n",
      "Step 2751: val loss: 0.3051699995994568\n",
      "Step 2752: train loss: 0.31318312883377075\n",
      "Step 2752: val loss: 0.3050602376461029\n",
      "Step 2753: train loss: 0.31306248903274536\n",
      "Step 2753: val loss: 0.30495068430900574\n",
      "Step 2754: train loss: 0.31294193863868713\n",
      "Step 2754: val loss: 0.3048413395881653\n",
      "Step 2755: train loss: 0.31282156705856323\n",
      "Step 2755: val loss: 0.30473223328590393\n",
      "Step 2756: train loss: 0.3127014935016632\n",
      "Step 2756: val loss: 0.30462321639060974\n",
      "Step 2757: train loss: 0.3125815689563751\n",
      "Step 2757: val loss: 0.3045145273208618\n",
      "Step 2758: train loss: 0.31246188282966614\n",
      "Step 2758: val loss: 0.30440592765808105\n",
      "Step 2759: train loss: 0.3123423755168915\n",
      "Step 2759: val loss: 0.304297536611557\n",
      "Step 2760: train loss: 0.31222307682037354\n",
      "Step 2760: val loss: 0.30418944358825684\n",
      "Step 2761: train loss: 0.31210410594940186\n",
      "Step 2761: val loss: 0.3040815591812134\n",
      "Step 2762: train loss: 0.31198516488075256\n",
      "Step 2762: val loss: 0.303973913192749\n",
      "Step 2763: train loss: 0.31186649203300476\n",
      "Step 2763: val loss: 0.3038663864135742\n",
      "Step 2764: train loss: 0.3117481768131256\n",
      "Step 2764: val loss: 0.3037590980529785\n",
      "Step 2765: train loss: 0.31162992119789124\n",
      "Step 2765: val loss: 0.30365198850631714\n",
      "Step 2766: train loss: 0.31151190400123596\n",
      "Step 2766: val loss: 0.3035450577735901\n",
      "Step 2767: train loss: 0.311394065618515\n",
      "Step 2767: val loss: 0.30343833565711975\n",
      "Step 2768: train loss: 0.3112764358520508\n",
      "Step 2768: val loss: 0.3033318817615509\n",
      "Step 2769: train loss: 0.3111591041088104\n",
      "Step 2769: val loss: 0.3032255470752716\n",
      "Step 2770: train loss: 0.3110417425632477\n",
      "Step 2770: val loss: 0.3031195104122162\n",
      "Step 2771: train loss: 0.31092479825019836\n",
      "Step 2771: val loss: 0.3030136823654175\n",
      "Step 2772: train loss: 0.3108082115650177\n",
      "Step 2772: val loss: 0.3029079735279083\n",
      "Step 2773: train loss: 0.31069156527519226\n",
      "Step 2773: val loss: 0.30280250310897827\n",
      "Step 2774: train loss: 0.3105752170085907\n",
      "Step 2774: val loss: 0.30269721150398254\n",
      "Step 2775: train loss: 0.3104589581489563\n",
      "Step 2775: val loss: 0.30259209871292114\n",
      "Step 2776: train loss: 0.31034302711486816\n",
      "Step 2776: val loss: 0.30248719453811646\n",
      "Step 2777: train loss: 0.31022733449935913\n",
      "Step 2777: val loss: 0.30238255858421326\n",
      "Step 2778: train loss: 0.3101117014884949\n",
      "Step 2778: val loss: 0.3022781312465668\n",
      "Step 2779: train loss: 0.3099963665008545\n",
      "Step 2779: val loss: 0.3021738827228546\n",
      "Step 2780: train loss: 0.3098812997341156\n",
      "Step 2780: val loss: 0.302069753408432\n",
      "Step 2781: train loss: 0.30976632237434387\n",
      "Step 2781: val loss: 0.3019659221172333\n",
      "Step 2782: train loss: 0.3096516728401184\n",
      "Step 2782: val loss: 0.30186226963996887\n",
      "Step 2783: train loss: 0.30953705310821533\n",
      "Step 2783: val loss: 0.301758736371994\n",
      "Step 2784: train loss: 0.30942270159721375\n",
      "Step 2784: val loss: 0.30165553092956543\n",
      "Step 2785: train loss: 0.30930861830711365\n",
      "Step 2785: val loss: 0.30155232548713684\n",
      "Step 2786: train loss: 0.3091946840286255\n",
      "Step 2786: val loss: 0.3014495074748993\n",
      "Step 2787: train loss: 0.30908095836639404\n",
      "Step 2787: val loss: 0.30134689807891846\n",
      "Step 2788: train loss: 0.3089674711227417\n",
      "Step 2788: val loss: 0.3012443482875824\n",
      "Step 2789: train loss: 0.3088540732860565\n",
      "Step 2789: val loss: 0.301142156124115\n",
      "Step 2790: train loss: 0.3087410032749176\n",
      "Step 2790: val loss: 0.3010399639606476\n",
      "Step 2791: train loss: 0.30862805247306824\n",
      "Step 2791: val loss: 0.30093812942504883\n",
      "Step 2792: train loss: 0.3085153102874756\n",
      "Step 2792: val loss: 0.3008364737033844\n",
      "Step 2793: train loss: 0.3084028363227844\n",
      "Step 2793: val loss: 0.3007349371910095\n",
      "Step 2794: train loss: 0.3082904517650604\n",
      "Step 2794: val loss: 0.30063360929489136\n",
      "Step 2795: train loss: 0.3081783950328827\n",
      "Step 2795: val loss: 0.3005324602127075\n",
      "Step 2796: train loss: 0.30806633830070496\n",
      "Step 2796: val loss: 0.300431489944458\n",
      "Step 2797: train loss: 0.3079546391963959\n",
      "Step 2797: val loss: 0.3003308176994324\n",
      "Step 2798: train loss: 0.30784308910369873\n",
      "Step 2798: val loss: 0.3002302348613739\n",
      "Step 2799: train loss: 0.3077317178249359\n",
      "Step 2799: val loss: 0.3001299500465393\n",
      "Step 2800: train loss: 0.3076206147670746\n",
      "Step 2800: val loss: 0.3000296354293823\n",
      "Step 2801: train loss: 0.30750954151153564\n",
      "Step 2801: val loss: 0.299929678440094\n",
      "Step 2802: train loss: 0.30739879608154297\n",
      "Step 2802: val loss: 0.29983001947402954\n",
      "Step 2803: train loss: 0.307288259267807\n",
      "Step 2803: val loss: 0.2997303903102875\n",
      "Step 2804: train loss: 0.3071778416633606\n",
      "Step 2804: val loss: 0.29963093996047974\n",
      "Step 2805: train loss: 0.30706772208213806\n",
      "Step 2805: val loss: 0.2995316982269287\n",
      "Step 2806: train loss: 0.3069576621055603\n",
      "Step 2806: val loss: 0.2994326949119568\n",
      "Step 2807: train loss: 0.30684778094291687\n",
      "Step 2807: val loss: 0.29933393001556396\n",
      "Step 2808: train loss: 0.30673831701278687\n",
      "Step 2808: val loss: 0.29923516511917114\n",
      "Step 2809: train loss: 0.3066288232803345\n",
      "Step 2809: val loss: 0.29913678765296936\n",
      "Step 2810: train loss: 0.30651959776878357\n",
      "Step 2810: val loss: 0.29903849959373474\n",
      "Step 2811: train loss: 0.30641067028045654\n",
      "Step 2811: val loss: 0.298940509557724\n",
      "Step 2812: train loss: 0.30630186200141907\n",
      "Step 2812: val loss: 0.2988426387310028\n",
      "Step 2813: train loss: 0.30619317293167114\n",
      "Step 2813: val loss: 0.29874494671821594\n",
      "Step 2814: train loss: 0.3060847222805023\n",
      "Step 2814: val loss: 0.298647403717041\n",
      "Step 2815: train loss: 0.3059765100479126\n",
      "Step 2815: val loss: 0.2985500991344452\n",
      "Step 2816: train loss: 0.3058684766292572\n",
      "Step 2816: val loss: 0.2984529733657837\n",
      "Step 2817: train loss: 0.30576056241989136\n",
      "Step 2817: val loss: 0.29835599660873413\n",
      "Step 2818: train loss: 0.305652916431427\n",
      "Step 2818: val loss: 0.29825928807258606\n",
      "Step 2819: train loss: 0.3055453896522522\n",
      "Step 2819: val loss: 0.29816269874572754\n",
      "Step 2820: train loss: 0.3054380416870117\n",
      "Step 2820: val loss: 0.29806628823280334\n",
      "Step 2821: train loss: 0.30533087253570557\n",
      "Step 2821: val loss: 0.2979700267314911\n",
      "Step 2822: train loss: 0.30522391200065613\n",
      "Step 2822: val loss: 0.2978740930557251\n",
      "Step 2823: train loss: 0.30511727929115295\n",
      "Step 2823: val loss: 0.2977781891822815\n",
      "Step 2824: train loss: 0.3050106167793274\n",
      "Step 2824: val loss: 0.297682523727417\n",
      "Step 2825: train loss: 0.3049042820930481\n",
      "Step 2825: val loss: 0.29758697748184204\n",
      "Step 2826: train loss: 0.3047979772090912\n",
      "Step 2826: val loss: 0.2974916994571686\n",
      "Step 2827: train loss: 0.30469200015068054\n",
      "Step 2827: val loss: 0.29739660024642944\n",
      "Step 2828: train loss: 0.30458617210388184\n",
      "Step 2828: val loss: 0.29730165004730225\n",
      "Step 2829: train loss: 0.30448052287101746\n",
      "Step 2829: val loss: 0.2972068190574646\n",
      "Step 2830: train loss: 0.3043751120567322\n",
      "Step 2830: val loss: 0.29711228609085083\n",
      "Step 2831: train loss: 0.3042697608470917\n",
      "Step 2831: val loss: 0.297017902135849\n",
      "Step 2832: train loss: 0.30416470766067505\n",
      "Step 2832: val loss: 0.29692375659942627\n",
      "Step 2833: train loss: 0.30405980348587036\n",
      "Step 2833: val loss: 0.2968296706676483\n",
      "Step 2834: train loss: 0.3039551377296448\n",
      "Step 2834: val loss: 0.29673588275909424\n",
      "Step 2835: train loss: 0.30385062098503113\n",
      "Step 2835: val loss: 0.2966421842575073\n",
      "Step 2836: train loss: 0.30374622344970703\n",
      "Step 2836: val loss: 0.2965486943721771\n",
      "Step 2837: train loss: 0.3036421239376068\n",
      "Step 2837: val loss: 0.296455442905426\n",
      "Step 2838: train loss: 0.30353814363479614\n",
      "Step 2838: val loss: 0.2963622212409973\n",
      "Step 2839: train loss: 0.3034344017505646\n",
      "Step 2839: val loss: 0.29626935720443726\n",
      "Step 2840: train loss: 0.30333080887794495\n",
      "Step 2840: val loss: 0.29617664217948914\n",
      "Step 2841: train loss: 0.30322736501693726\n",
      "Step 2841: val loss: 0.2960840165615082\n",
      "Step 2842: train loss: 0.30312415957450867\n",
      "Step 2842: val loss: 0.2959915101528168\n",
      "Step 2843: train loss: 0.30302101373672485\n",
      "Step 2843: val loss: 0.29589930176734924\n",
      "Step 2844: train loss: 0.30291804671287537\n",
      "Step 2844: val loss: 0.29580727219581604\n",
      "Step 2845: train loss: 0.30281543731689453\n",
      "Step 2845: val loss: 0.2957153618335724\n",
      "Step 2846: train loss: 0.3027128577232361\n",
      "Step 2846: val loss: 0.2956237196922302\n",
      "Step 2847: train loss: 0.3026106059551239\n",
      "Step 2847: val loss: 0.29553207755088806\n",
      "Step 2848: train loss: 0.30250826478004456\n",
      "Step 2848: val loss: 0.2954407334327698\n",
      "Step 2849: train loss: 0.302406370639801\n",
      "Step 2849: val loss: 0.2953495383262634\n",
      "Step 2850: train loss: 0.30230462551116943\n",
      "Step 2850: val loss: 0.29525861144065857\n",
      "Step 2851: train loss: 0.3022029399871826\n",
      "Step 2851: val loss: 0.2951677739620209\n",
      "Step 2852: train loss: 0.3021015226840973\n",
      "Step 2852: val loss: 0.2950770854949951\n",
      "Step 2853: train loss: 0.3020002245903015\n",
      "Step 2853: val loss: 0.2949865758419037\n",
      "Step 2854: train loss: 0.30189913511276245\n",
      "Step 2854: val loss: 0.29489630460739136\n",
      "Step 2855: train loss: 0.30179816484451294\n",
      "Step 2855: val loss: 0.2948061525821686\n",
      "Step 2856: train loss: 0.30169743299484253\n",
      "Step 2856: val loss: 0.2947162687778473\n",
      "Step 2857: train loss: 0.30159690976142883\n",
      "Step 2857: val loss: 0.2946264445781708\n",
      "Step 2858: train loss: 0.3014965355396271\n",
      "Step 2858: val loss: 0.294536828994751\n",
      "Step 2859: train loss: 0.30139631032943726\n",
      "Step 2859: val loss: 0.2944473922252655\n",
      "Step 2860: train loss: 0.30129626393318176\n",
      "Step 2860: val loss: 0.2943580746650696\n",
      "Step 2861: train loss: 0.3011963367462158\n",
      "Step 2861: val loss: 0.29426896572113037\n",
      "Step 2862: train loss: 0.3010966181755066\n",
      "Step 2862: val loss: 0.29418009519577026\n",
      "Step 2863: train loss: 0.30099713802337646\n",
      "Step 2863: val loss: 0.29409122467041016\n",
      "Step 2864: train loss: 0.3008978068828583\n",
      "Step 2864: val loss: 0.2940027117729187\n",
      "Step 2865: train loss: 0.300798624753952\n",
      "Step 2865: val loss: 0.2939142882823944\n",
      "Step 2866: train loss: 0.30069971084594727\n",
      "Step 2866: val loss: 0.2938259541988373\n",
      "Step 2867: train loss: 0.30060073733329773\n",
      "Step 2867: val loss: 0.29373791813850403\n",
      "Step 2868: train loss: 0.3005021810531616\n",
      "Step 2868: val loss: 0.2936500012874603\n",
      "Step 2869: train loss: 0.3004036843776703\n",
      "Step 2869: val loss: 0.29356223344802856\n",
      "Step 2870: train loss: 0.30030539631843567\n",
      "Step 2870: val loss: 0.2934746742248535\n",
      "Step 2871: train loss: 0.3002072870731354\n",
      "Step 2871: val loss: 0.2933872640132904\n",
      "Step 2872: train loss: 0.300109326839447\n",
      "Step 2872: val loss: 0.29329997301101685\n",
      "Step 2873: train loss: 0.300011545419693\n",
      "Step 2873: val loss: 0.293212890625\n",
      "Step 2874: train loss: 0.2999138832092285\n",
      "Step 2874: val loss: 0.29312607645988464\n",
      "Step 2875: train loss: 0.29981645941734314\n",
      "Step 2875: val loss: 0.2930392920970917\n",
      "Step 2876: train loss: 0.29971909523010254\n",
      "Step 2876: val loss: 0.29295268654823303\n",
      "Step 2877: train loss: 0.2996219992637634\n",
      "Step 2877: val loss: 0.2928662896156311\n",
      "Step 2878: train loss: 0.29952505230903625\n",
      "Step 2878: val loss: 0.29278016090393066\n",
      "Step 2879: train loss: 0.29942837357521057\n",
      "Step 2879: val loss: 0.292694091796875\n",
      "Step 2880: train loss: 0.29933178424835205\n",
      "Step 2880: val loss: 0.2926080822944641\n",
      "Step 2881: train loss: 0.2992353141307831\n",
      "Step 2881: val loss: 0.2925224006175995\n",
      "Step 2882: train loss: 0.2991391122341156\n",
      "Step 2882: val loss: 0.29243677854537964\n",
      "Step 2883: train loss: 0.2990429401397705\n",
      "Step 2883: val loss: 0.2923513650894165\n",
      "Step 2884: train loss: 0.2989470660686493\n",
      "Step 2884: val loss: 0.2922661602497101\n",
      "Step 2885: train loss: 0.29885134100914\n",
      "Step 2885: val loss: 0.292181134223938\n",
      "Step 2886: train loss: 0.29875579476356506\n",
      "Step 2886: val loss: 0.29209619760513306\n",
      "Step 2887: train loss: 0.2986603379249573\n",
      "Step 2887: val loss: 0.29201140999794006\n",
      "Step 2888: train loss: 0.298565149307251\n",
      "Step 2888: val loss: 0.29192686080932617\n",
      "Step 2889: train loss: 0.29847002029418945\n",
      "Step 2889: val loss: 0.29184234142303467\n",
      "Step 2890: train loss: 0.29837509989738464\n",
      "Step 2890: val loss: 0.29175814986228943\n",
      "Step 2891: train loss: 0.2982803285121918\n",
      "Step 2891: val loss: 0.29167401790618896\n",
      "Step 2892: train loss: 0.29818570613861084\n",
      "Step 2892: val loss: 0.29159003496170044\n",
      "Step 2893: train loss: 0.2980913519859314\n",
      "Step 2893: val loss: 0.29150626063346863\n",
      "Step 2894: train loss: 0.29799705743789673\n",
      "Step 2894: val loss: 0.29142263531684875\n",
      "Step 2895: train loss: 0.2979029715061188\n",
      "Step 2895: val loss: 0.2913392186164856\n",
      "Step 2896: train loss: 0.2978091239929199\n",
      "Step 2896: val loss: 0.2912558913230896\n",
      "Step 2897: train loss: 0.29771527647972107\n",
      "Step 2897: val loss: 0.2911728024482727\n",
      "Step 2898: train loss: 0.2976217567920685\n",
      "Step 2898: val loss: 0.2910897135734558\n",
      "Step 2899: train loss: 0.29752829670906067\n",
      "Step 2899: val loss: 0.2910069525241852\n",
      "Step 2900: train loss: 0.29743507504463196\n",
      "Step 2900: val loss: 0.2909242808818817\n",
      "Step 2901: train loss: 0.29734182357788086\n",
      "Step 2901: val loss: 0.2908417284488678\n",
      "Step 2902: train loss: 0.297248899936676\n",
      "Step 2902: val loss: 0.29075950384140015\n",
      "Step 2903: train loss: 0.29715612530708313\n",
      "Step 2903: val loss: 0.2906772792339325\n",
      "Step 2904: train loss: 0.2970634996891022\n",
      "Step 2904: val loss: 0.2905952036380768\n",
      "Step 2905: train loss: 0.2969711720943451\n",
      "Step 2905: val loss: 0.29051336646080017\n",
      "Step 2906: train loss: 0.29687878489494324\n",
      "Step 2906: val loss: 0.2904317080974579\n",
      "Step 2907: train loss: 0.29678669571876526\n",
      "Step 2907: val loss: 0.29035013914108276\n",
      "Step 2908: train loss: 0.29669469594955444\n",
      "Step 2908: val loss: 0.29026877880096436\n",
      "Step 2909: train loss: 0.29660290479660034\n",
      "Step 2909: val loss: 0.2901875078678131\n",
      "Step 2910: train loss: 0.296511173248291\n",
      "Step 2910: val loss: 0.2901064157485962\n",
      "Step 2911: train loss: 0.2964196801185608\n",
      "Step 2911: val loss: 0.29002535343170166\n",
      "Step 2912: train loss: 0.2963283360004425\n",
      "Step 2912: val loss: 0.2899446487426758\n",
      "Step 2913: train loss: 0.29623711109161377\n",
      "Step 2913: val loss: 0.28986403346061707\n",
      "Step 2914: train loss: 0.29614612460136414\n",
      "Step 2914: val loss: 0.2897835671901703\n",
      "Step 2915: train loss: 0.29605525732040405\n",
      "Step 2915: val loss: 0.28970327973365784\n",
      "Step 2916: train loss: 0.29596462845802307\n",
      "Step 2916: val loss: 0.2896231710910797\n",
      "Step 2917: train loss: 0.2958739995956421\n",
      "Step 2917: val loss: 0.28954312205314636\n",
      "Step 2918: train loss: 0.295783668756485\n",
      "Step 2918: val loss: 0.2894633114337921\n",
      "Step 2919: train loss: 0.29569342732429504\n",
      "Step 2919: val loss: 0.2893836200237274\n",
      "Step 2920: train loss: 0.2956033945083618\n",
      "Step 2920: val loss: 0.28930404782295227\n",
      "Step 2921: train loss: 0.29551348090171814\n",
      "Step 2921: val loss: 0.2892247140407562\n",
      "Step 2922: train loss: 0.29542362689971924\n",
      "Step 2922: val loss: 0.2891453802585602\n",
      "Step 2923: train loss: 0.2953340411186218\n",
      "Step 2923: val loss: 0.2890663146972656\n",
      "Step 2924: train loss: 0.29524457454681396\n",
      "Step 2924: val loss: 0.2889874279499054\n",
      "Step 2925: train loss: 0.2951553761959076\n",
      "Step 2925: val loss: 0.28890860080718994\n",
      "Step 2926: train loss: 0.295066237449646\n",
      "Step 2926: val loss: 0.28883007168769836\n",
      "Step 2927: train loss: 0.29497724771499634\n",
      "Step 2927: val loss: 0.2887515425682068\n",
      "Step 2928: train loss: 0.2948884963989258\n",
      "Step 2928: val loss: 0.2886732816696167\n",
      "Step 2929: train loss: 0.2947997450828552\n",
      "Step 2929: val loss: 0.2885950803756714\n",
      "Step 2930: train loss: 0.29471123218536377\n",
      "Step 2930: val loss: 0.2885170578956604\n",
      "Step 2931: train loss: 0.29462286829948425\n",
      "Step 2931: val loss: 0.28843924403190613\n",
      "Step 2932: train loss: 0.2945346236228943\n",
      "Step 2932: val loss: 0.288361519575119\n",
      "Step 2933: train loss: 0.2944466471672058\n",
      "Step 2933: val loss: 0.28828394412994385\n",
      "Step 2934: train loss: 0.2943587303161621\n",
      "Step 2934: val loss: 0.2882065176963806\n",
      "Step 2935: train loss: 0.29427099227905273\n",
      "Step 2935: val loss: 0.28812915086746216\n",
      "Step 2936: train loss: 0.2941833436489105\n",
      "Step 2936: val loss: 0.2880520224571228\n",
      "Step 2937: train loss: 0.29409587383270264\n",
      "Step 2937: val loss: 0.28797513246536255\n",
      "Step 2938: train loss: 0.29400867223739624\n",
      "Step 2938: val loss: 0.28789830207824707\n",
      "Step 2939: train loss: 0.29392147064208984\n",
      "Step 2939: val loss: 0.2878216505050659\n",
      "Step 2940: train loss: 0.2938345670700073\n",
      "Step 2940: val loss: 0.2877451777458191\n",
      "Step 2941: train loss: 0.2937477231025696\n",
      "Step 2941: val loss: 0.28766876459121704\n",
      "Step 2942: train loss: 0.29366105794906616\n",
      "Step 2942: val loss: 0.2875925302505493\n",
      "Step 2943: train loss: 0.2935745120048523\n",
      "Step 2943: val loss: 0.2875165045261383\n",
      "Step 2944: train loss: 0.29348820447921753\n",
      "Step 2944: val loss: 0.2874405086040497\n",
      "Step 2945: train loss: 0.29340189695358276\n",
      "Step 2945: val loss: 0.2873648405075073\n",
      "Step 2946: train loss: 0.2933158874511719\n",
      "Step 2946: val loss: 0.2872890532016754\n",
      "Step 2947: train loss: 0.29322996735572815\n",
      "Step 2947: val loss: 0.2872135639190674\n",
      "Step 2948: train loss: 0.29314422607421875\n",
      "Step 2948: val loss: 0.2871382236480713\n",
      "Step 2949: train loss: 0.29305848479270935\n",
      "Step 2949: val loss: 0.28706303238868713\n",
      "Step 2950: train loss: 0.29297301173210144\n",
      "Step 2950: val loss: 0.28698796033859253\n",
      "Step 2951: train loss: 0.2928876578807831\n",
      "Step 2951: val loss: 0.2869129776954651\n",
      "Step 2952: train loss: 0.29280245304107666\n",
      "Step 2952: val loss: 0.286838173866272\n",
      "Step 2953: train loss: 0.29271742701530457\n",
      "Step 2953: val loss: 0.28676357865333557\n",
      "Step 2954: train loss: 0.292632520198822\n",
      "Step 2954: val loss: 0.28668907284736633\n",
      "Step 2955: train loss: 0.2925477623939514\n",
      "Step 2955: val loss: 0.28661471605300903\n",
      "Step 2956: train loss: 0.2924632430076599\n",
      "Step 2956: val loss: 0.2865406274795532\n",
      "Step 2957: train loss: 0.29237881302833557\n",
      "Step 2957: val loss: 0.286466509103775\n",
      "Step 2958: train loss: 0.29229453206062317\n",
      "Step 2958: val loss: 0.2863926589488983\n",
      "Step 2959: train loss: 0.2922104597091675\n",
      "Step 2959: val loss: 0.2863188683986664\n",
      "Step 2960: train loss: 0.2921264171600342\n",
      "Step 2960: val loss: 0.2862452268600464\n",
      "Step 2961: train loss: 0.29204261302948\n",
      "Step 2961: val loss: 0.2861717641353607\n",
      "Step 2962: train loss: 0.29195883870124817\n",
      "Step 2962: val loss: 0.2860984802246094\n",
      "Step 2963: train loss: 0.29187530279159546\n",
      "Step 2963: val loss: 0.2860252559185028\n",
      "Step 2964: train loss: 0.2917918562889099\n",
      "Step 2964: val loss: 0.28595224022865295\n",
      "Step 2965: train loss: 0.2917086184024811\n",
      "Step 2965: val loss: 0.2858792841434479\n",
      "Step 2966: train loss: 0.291625440120697\n",
      "Step 2966: val loss: 0.2858065068721771\n",
      "Step 2967: train loss: 0.29154250025749207\n",
      "Step 2967: val loss: 0.2857338488101959\n",
      "Step 2968: train loss: 0.2914596498012543\n",
      "Step 2968: val loss: 0.2856614291667938\n",
      "Step 2969: train loss: 0.29137691855430603\n",
      "Step 2969: val loss: 0.2855890095233917\n",
      "Step 2970: train loss: 0.2912943661212921\n",
      "Step 2970: val loss: 0.28551679849624634\n",
      "Step 2971: train loss: 0.29121196269989014\n",
      "Step 2971: val loss: 0.2854447066783905\n",
      "Step 2972: train loss: 0.2911296784877777\n",
      "Step 2972: val loss: 0.28537270426750183\n",
      "Step 2973: train loss: 0.2910475432872772\n",
      "Step 2973: val loss: 0.28530097007751465\n",
      "Step 2974: train loss: 0.2909655272960663\n",
      "Step 2974: val loss: 0.28522929549217224\n",
      "Step 2975: train loss: 0.2908836007118225\n",
      "Step 2975: val loss: 0.2851577699184418\n",
      "Step 2976: train loss: 0.29080188274383545\n",
      "Step 2976: val loss: 0.28508639335632324\n",
      "Step 2977: train loss: 0.2907203137874603\n",
      "Step 2977: val loss: 0.2850150167942047\n",
      "Step 2978: train loss: 0.29063886404037476\n",
      "Step 2978: val loss: 0.28494390845298767\n",
      "Step 2979: train loss: 0.2905576229095459\n",
      "Step 2979: val loss: 0.28487297892570496\n",
      "Step 2980: train loss: 0.29047656059265137\n",
      "Step 2980: val loss: 0.2848021984100342\n",
      "Step 2981: train loss: 0.2903955280780792\n",
      "Step 2981: val loss: 0.28473153710365295\n",
      "Step 2982: train loss: 0.2903147339820862\n",
      "Step 2982: val loss: 0.2846609950065613\n",
      "Step 2983: train loss: 0.2902340590953827\n",
      "Step 2983: val loss: 0.28459057211875916\n",
      "Step 2984: train loss: 0.29015353322029114\n",
      "Step 2984: val loss: 0.28452032804489136\n",
      "Step 2985: train loss: 0.29007312655448914\n",
      "Step 2985: val loss: 0.28445014357566833\n",
      "Step 2986: train loss: 0.2899928689002991\n",
      "Step 2986: val loss: 0.284380167722702\n",
      "Step 2987: train loss: 0.28991273045539856\n",
      "Step 2987: val loss: 0.28431031107902527\n",
      "Step 2988: train loss: 0.2898327708244324\n",
      "Step 2988: val loss: 0.28424057364463806\n",
      "Step 2989: train loss: 0.28975287079811096\n",
      "Step 2989: val loss: 0.2841709852218628\n",
      "Step 2990: train loss: 0.2896731495857239\n",
      "Step 2990: val loss: 0.28410157561302185\n",
      "Step 2991: train loss: 0.28959357738494873\n",
      "Step 2991: val loss: 0.28403228521347046\n",
      "Step 2992: train loss: 0.2895141541957855\n",
      "Step 2992: val loss: 0.28396302461624146\n",
      "Step 2993: train loss: 0.28943485021591187\n",
      "Step 2993: val loss: 0.28389403223991394\n",
      "Step 2994: train loss: 0.28935566544532776\n",
      "Step 2994: val loss: 0.2838250994682312\n",
      "Step 2995: train loss: 0.2892766296863556\n",
      "Step 2995: val loss: 0.2837563455104828\n",
      "Step 2996: train loss: 0.289197713136673\n",
      "Step 2996: val loss: 0.28368762135505676\n",
      "Step 2997: train loss: 0.2891189455986023\n",
      "Step 2997: val loss: 0.2836191952228546\n",
      "Step 2998: train loss: 0.2890404164791107\n",
      "Step 2998: val loss: 0.28355076909065247\n",
      "Step 2999: train loss: 0.288961797952652\n",
      "Step 2999: val loss: 0.28348255157470703\n",
      "After training weight values: Parameter containing:\n",
      "tensor([[ 0.0958, -4.7258,  1.9298,  0.9580,  0.0576]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. Adapt the code of linear regression in the 1D case presented in the lecture (at the end of Sec.2.1) to perform polynomial regression using the generated training dataset D.\n",
    "\n",
    "#2. Find and report a suitable learning rate and number of iterations for gradient descent. \n",
    "#3. Report the initial (random) values and the estimate of w you obtained after training.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "else \"cpu\")\n",
    "\n",
    "model = nn.Linear(5, 1, bias = False)\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "sample_size = 500\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_train = x_train.float()\n",
    "x_train = x_train.to(DEVICE)\n",
    "\n",
    "y_train = torch.from_numpy(y_train.reshape((sample_size, 1))).float().to(DEVICE)\n",
    "x_validate = torch.from_numpy(x_validate.reshape((sample_size, 5))).float().to(DEVICE)\n",
    "y_validate = torch.from_numpy(y_validate.reshape((sample_size, 1))).float().to(DEVICE)\n",
    "\n",
    "num_steps = 3000\n",
    "#w0_evolution = []\n",
    "#w1_evolution = []\n",
    "#w2_evolution = []\n",
    "#w3_evolution = []\n",
    "#w4_evolution = []\n",
    "\n",
    "print(f\"Initial weight values: {model.weight}\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  y_ = model(x_train)\n",
    "  loss = loss_fn(y_, y_train)\n",
    "  print(f\"Step {step}: train loss: {loss}\")\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  #w0_evolution.append(model.weight[0,0].item())\n",
    "  #w1_evolution.append(model.weight[0,1].item())\n",
    "  #w2_evolution.append(model.weight[0,2].item()) \n",
    "  #w3_evolution.append(model.weight[0,3].item()) \n",
    "  #w4_evolution.append(model.weight[0,4].item()) \n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    y_ = model(x_validate)\n",
    "    val_loss = loss_fn(y_, y_validate)\n",
    "  \n",
    "  print(f\"Step {step}: val loss: {val_loss}\")\n",
    "\n",
    "print(f\"After training weight values: {model.weight}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  y_ = model(x_validate)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_validate[:,1].cpu().numpy(), y_validate.cpu().numpy(), \".\")\n",
    "ax.plot(x_validate[:,1].cpu().numpy(), y_.cpu().numpy(), \".\")\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=16)\n",
    "ax.set_ylabel(\"y\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4f0e51-29cf-4496-957b-c06a150b1783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss: 49.082611083984375\n",
      "Step 0: val loss: 39.4189338684082\n",
      "Step 1: train loss: 33.46139907836914\n",
      "Step 1: val loss: 26.69407844543457\n",
      "Step 2: train loss: 27.434371948242188\n",
      "Step 2: val loss: 26.93743133544922\n",
      "Step 3: train loss: 24.953584671020508\n",
      "Step 3: val loss: 23.667545318603516\n",
      "Step 4: train loss: 23.83202362060547\n",
      "Step 4: val loss: 23.860328674316406\n",
      "Step 5: train loss: 23.2590274810791\n",
      "Step 5: val loss: 22.764617919921875\n",
      "Step 6: train loss: 22.922611236572266\n",
      "Step 6: val loss: 22.729690551757812\n",
      "Step 7: train loss: 22.696216583251953\n",
      "Step 7: val loss: 22.26478385925293\n",
      "Step 8: train loss: 22.524967193603516\n",
      "Step 8: val loss: 22.15339469909668\n",
      "Step 9: train loss: 22.383243560791016\n",
      "Step 9: val loss: 21.90460968017578\n",
      "Step 10: train loss: 22.258207321166992\n",
      "Step 10: val loss: 21.780677795410156\n",
      "Step 11: train loss: 22.143022537231445\n",
      "Step 11: val loss: 21.615251541137695\n",
      "Step 12: train loss: 22.03388214111328\n",
      "Step 12: val loss: 21.496387481689453\n",
      "Step 13: train loss: 21.928611755371094\n",
      "Step 13: val loss: 21.366600036621094\n",
      "Step 14: train loss: 21.825925827026367\n",
      "Step 14: val loss: 21.255382537841797\n",
      "Step 15: train loss: 21.725065231323242\n",
      "Step 15: val loss: 21.14251136779785\n",
      "Step 16: train loss: 21.625579833984375\n",
      "Step 16: val loss: 21.037755966186523\n",
      "Step 17: train loss: 21.527193069458008\n",
      "Step 17: val loss: 20.93385124206543\n",
      "Step 18: train loss: 21.42974090576172\n",
      "Step 18: val loss: 20.833940505981445\n",
      "Step 19: train loss: 21.333114624023438\n",
      "Step 19: val loss: 20.735336303710938\n",
      "Step 20: train loss: 21.23725700378418\n",
      "Step 20: val loss: 20.638996124267578\n",
      "Step 21: train loss: 21.142127990722656\n",
      "Step 21: val loss: 20.54387664794922\n",
      "Step 22: train loss: 21.04769515991211\n",
      "Step 22: val loss: 20.450231552124023\n",
      "Step 23: train loss: 20.95394515991211\n",
      "Step 23: val loss: 20.357637405395508\n",
      "Step 24: train loss: 20.860862731933594\n",
      "Step 24: val loss: 20.266130447387695\n",
      "Step 25: train loss: 20.7684383392334\n",
      "Step 25: val loss: 20.17552375793457\n",
      "Step 26: train loss: 20.676664352416992\n",
      "Step 26: val loss: 20.085792541503906\n",
      "Step 27: train loss: 20.585529327392578\n",
      "Step 27: val loss: 19.996858596801758\n",
      "Step 28: train loss: 20.495033264160156\n",
      "Step 28: val loss: 19.908687591552734\n",
      "Step 29: train loss: 20.405168533325195\n",
      "Step 29: val loss: 19.821231842041016\n",
      "Step 30: train loss: 20.315929412841797\n",
      "Step 30: val loss: 19.734474182128906\n",
      "Step 31: train loss: 20.227310180664062\n",
      "Step 31: val loss: 19.648380279541016\n",
      "Step 32: train loss: 20.13930320739746\n",
      "Step 32: val loss: 19.562936782836914\n",
      "Step 33: train loss: 20.051910400390625\n",
      "Step 33: val loss: 19.47812843322754\n",
      "Step 34: train loss: 19.96511459350586\n",
      "Step 34: val loss: 19.39394187927246\n",
      "Step 35: train loss: 19.878921508789062\n",
      "Step 35: val loss: 19.31036376953125\n",
      "Step 36: train loss: 19.79332160949707\n",
      "Step 36: val loss: 19.22738265991211\n",
      "Step 37: train loss: 19.708314895629883\n",
      "Step 37: val loss: 19.14499282836914\n",
      "Step 38: train loss: 19.623886108398438\n",
      "Step 38: val loss: 19.063180923461914\n",
      "Step 39: train loss: 19.540040969848633\n",
      "Step 39: val loss: 18.981945037841797\n",
      "Step 40: train loss: 19.45677375793457\n",
      "Step 40: val loss: 18.901281356811523\n",
      "Step 41: train loss: 19.37407112121582\n",
      "Step 41: val loss: 18.821178436279297\n",
      "Step 42: train loss: 19.29193687438965\n",
      "Step 42: val loss: 18.74163055419922\n",
      "Step 43: train loss: 19.21036148071289\n",
      "Step 43: val loss: 18.662635803222656\n",
      "Step 44: train loss: 19.12934112548828\n",
      "Step 44: val loss: 18.584182739257812\n",
      "Step 45: train loss: 19.04887580871582\n",
      "Step 45: val loss: 18.506277084350586\n",
      "Step 46: train loss: 18.968957901000977\n",
      "Step 46: val loss: 18.42890167236328\n",
      "Step 47: train loss: 18.889577865600586\n",
      "Step 47: val loss: 18.352062225341797\n",
      "Step 48: train loss: 18.810739517211914\n",
      "Step 48: val loss: 18.275745391845703\n",
      "Step 49: train loss: 18.732433319091797\n",
      "Step 49: val loss: 18.199953079223633\n",
      "Step 50: train loss: 18.6546573638916\n",
      "Step 50: val loss: 18.124677658081055\n",
      "Step 51: train loss: 18.57740592956543\n",
      "Step 51: val loss: 18.049915313720703\n",
      "Step 52: train loss: 18.500675201416016\n",
      "Step 52: val loss: 17.975662231445312\n",
      "Step 53: train loss: 18.424461364746094\n",
      "Step 53: val loss: 17.901912689208984\n",
      "Step 54: train loss: 18.3487606048584\n",
      "Step 54: val loss: 17.828664779663086\n",
      "Step 55: train loss: 18.2735652923584\n",
      "Step 55: val loss: 17.75591278076172\n",
      "Step 56: train loss: 18.19887351989746\n",
      "Step 56: val loss: 17.683652877807617\n",
      "Step 57: train loss: 18.124683380126953\n",
      "Step 57: val loss: 17.611879348754883\n",
      "Step 58: train loss: 18.050989151000977\n",
      "Step 58: val loss: 17.54059410095215\n",
      "Step 59: train loss: 17.977785110473633\n",
      "Step 59: val loss: 17.469783782958984\n",
      "Step 60: train loss: 17.905067443847656\n",
      "Step 60: val loss: 17.399450302124023\n",
      "Step 61: train loss: 17.832836151123047\n",
      "Step 61: val loss: 17.32958984375\n",
      "Step 62: train loss: 17.76108169555664\n",
      "Step 62: val loss: 17.260194778442383\n",
      "Step 63: train loss: 17.689804077148438\n",
      "Step 63: val loss: 17.191268920898438\n",
      "Step 64: train loss: 17.61899757385254\n",
      "Step 64: val loss: 17.122800827026367\n",
      "Step 65: train loss: 17.548660278320312\n",
      "Step 65: val loss: 17.054786682128906\n",
      "Step 66: train loss: 17.47878646850586\n",
      "Step 66: val loss: 16.98723030090332\n",
      "Step 67: train loss: 17.409374237060547\n",
      "Step 67: val loss: 16.92011833190918\n",
      "Step 68: train loss: 17.340417861938477\n",
      "Step 68: val loss: 16.85345458984375\n",
      "Step 69: train loss: 17.271913528442383\n",
      "Step 69: val loss: 16.787233352661133\n",
      "Step 70: train loss: 17.203859329223633\n",
      "Step 70: val loss: 16.721446990966797\n",
      "Step 71: train loss: 17.136249542236328\n",
      "Step 71: val loss: 16.656095504760742\n",
      "Step 72: train loss: 17.069082260131836\n",
      "Step 72: val loss: 16.591175079345703\n",
      "Step 73: train loss: 17.002355575561523\n",
      "Step 73: val loss: 16.526683807373047\n",
      "Step 74: train loss: 16.936058044433594\n",
      "Step 74: val loss: 16.462615966796875\n",
      "Step 75: train loss: 16.870197296142578\n",
      "Step 75: val loss: 16.398967742919922\n",
      "Step 76: train loss: 16.80476188659668\n",
      "Step 76: val loss: 16.335735321044922\n",
      "Step 77: train loss: 16.7397518157959\n",
      "Step 77: val loss: 16.272916793823242\n",
      "Step 78: train loss: 16.675159454345703\n",
      "Step 78: val loss: 16.210508346557617\n",
      "Step 79: train loss: 16.61098861694336\n",
      "Step 79: val loss: 16.14850616455078\n",
      "Step 80: train loss: 16.54722785949707\n",
      "Step 80: val loss: 16.0869083404541\n",
      "Step 81: train loss: 16.4838809967041\n",
      "Step 81: val loss: 16.02570915222168\n",
      "Step 82: train loss: 16.420942306518555\n",
      "Step 82: val loss: 15.964909553527832\n",
      "Step 83: train loss: 16.35840606689453\n",
      "Step 83: val loss: 15.904502868652344\n",
      "Step 84: train loss: 16.296268463134766\n",
      "Step 84: val loss: 15.844483375549316\n",
      "Step 85: train loss: 16.23453140258789\n",
      "Step 85: val loss: 15.784852027893066\n",
      "Step 86: train loss: 16.173187255859375\n",
      "Step 86: val loss: 15.725605010986328\n",
      "Step 87: train loss: 16.112234115600586\n",
      "Step 87: val loss: 15.666740417480469\n",
      "Step 88: train loss: 16.051671981811523\n",
      "Step 88: val loss: 15.608250617980957\n",
      "Step 89: train loss: 15.99149227142334\n",
      "Step 89: val loss: 15.550137519836426\n",
      "Step 90: train loss: 15.931693077087402\n",
      "Step 90: val loss: 15.492395401000977\n",
      "Step 91: train loss: 15.872275352478027\n",
      "Step 91: val loss: 15.43502140045166\n",
      "Step 92: train loss: 15.813233375549316\n",
      "Step 92: val loss: 15.378013610839844\n",
      "Step 93: train loss: 15.754561424255371\n",
      "Step 93: val loss: 15.321369171142578\n",
      "Step 94: train loss: 15.696261405944824\n",
      "Step 94: val loss: 15.265082359313965\n",
      "Step 95: train loss: 15.638328552246094\n",
      "Step 95: val loss: 15.209151268005371\n",
      "Step 96: train loss: 15.580756187438965\n",
      "Step 96: val loss: 15.15357780456543\n",
      "Step 97: train loss: 15.523548126220703\n",
      "Step 97: val loss: 15.09835147857666\n",
      "Step 98: train loss: 15.466695785522461\n",
      "Step 98: val loss: 15.043475151062012\n",
      "Step 99: train loss: 15.410200119018555\n",
      "Step 99: val loss: 14.988944053649902\n",
      "Step 100: train loss: 15.354056358337402\n",
      "Step 100: val loss: 14.9347562789917\n",
      "Step 101: train loss: 15.298260688781738\n",
      "Step 101: val loss: 14.88090705871582\n",
      "Step 102: train loss: 15.242812156677246\n",
      "Step 102: val loss: 14.827391624450684\n",
      "Step 103: train loss: 15.187707901000977\n",
      "Step 103: val loss: 14.774215698242188\n",
      "Step 104: train loss: 15.13294506072998\n",
      "Step 104: val loss: 14.721365928649902\n",
      "Step 105: train loss: 15.078520774841309\n",
      "Step 105: val loss: 14.668850898742676\n",
      "Step 106: train loss: 15.024433135986328\n",
      "Step 106: val loss: 14.616658210754395\n",
      "Step 107: train loss: 14.970675468444824\n",
      "Step 107: val loss: 14.564787864685059\n",
      "Step 108: train loss: 14.91724967956543\n",
      "Step 108: val loss: 14.513240814208984\n",
      "Step 109: train loss: 14.864151954650879\n",
      "Step 109: val loss: 14.462011337280273\n",
      "Step 110: train loss: 14.811378479003906\n",
      "Step 110: val loss: 14.411097526550293\n",
      "Step 111: train loss: 14.758929252624512\n",
      "Step 111: val loss: 14.360496520996094\n",
      "Step 112: train loss: 14.706799507141113\n",
      "Step 112: val loss: 14.310206413269043\n",
      "Step 113: train loss: 14.654985427856445\n",
      "Step 113: val loss: 14.260226249694824\n",
      "Step 114: train loss: 14.603487968444824\n",
      "Step 114: val loss: 14.210548400878906\n",
      "Step 115: train loss: 14.552302360534668\n",
      "Step 115: val loss: 14.161175727844238\n",
      "Step 116: train loss: 14.50142765045166\n",
      "Step 116: val loss: 14.112102508544922\n",
      "Step 117: train loss: 14.450860977172852\n",
      "Step 117: val loss: 14.063328742980957\n",
      "Step 118: train loss: 14.40059757232666\n",
      "Step 118: val loss: 14.014852523803711\n",
      "Step 119: train loss: 14.350638389587402\n",
      "Step 119: val loss: 13.966665267944336\n",
      "Step 120: train loss: 14.30097770690918\n",
      "Step 120: val loss: 13.91877555847168\n",
      "Step 121: train loss: 14.251615524291992\n",
      "Step 121: val loss: 13.871170043945312\n",
      "Step 122: train loss: 14.20254898071289\n",
      "Step 122: val loss: 13.823851585388184\n",
      "Step 123: train loss: 14.153775215148926\n",
      "Step 123: val loss: 13.776819229125977\n",
      "Step 124: train loss: 14.105292320251465\n",
      "Step 124: val loss: 13.730066299438477\n",
      "Step 125: train loss: 14.057097434997559\n",
      "Step 125: val loss: 13.683595657348633\n",
      "Step 126: train loss: 14.009190559387207\n",
      "Step 126: val loss: 13.637401580810547\n",
      "Step 127: train loss: 13.961566925048828\n",
      "Step 127: val loss: 13.591483116149902\n",
      "Step 128: train loss: 13.914225578308105\n",
      "Step 128: val loss: 13.545836448669434\n",
      "Step 129: train loss: 13.867161750793457\n",
      "Step 129: val loss: 13.50046157836914\n",
      "Step 130: train loss: 13.820378303527832\n",
      "Step 130: val loss: 13.455355644226074\n",
      "Step 131: train loss: 13.773870468139648\n",
      "Step 131: val loss: 13.410516738891602\n",
      "Step 132: train loss: 13.727632522583008\n",
      "Step 132: val loss: 13.365941047668457\n",
      "Step 133: train loss: 13.681666374206543\n",
      "Step 133: val loss: 13.321629524230957\n",
      "Step 134: train loss: 13.635971069335938\n",
      "Step 134: val loss: 13.277578353881836\n",
      "Step 135: train loss: 13.59053897857666\n",
      "Step 135: val loss: 13.233782768249512\n",
      "Step 136: train loss: 13.545374870300293\n",
      "Step 136: val loss: 13.190248489379883\n",
      "Step 137: train loss: 13.500472068786621\n",
      "Step 137: val loss: 13.146964073181152\n",
      "Step 138: train loss: 13.455828666687012\n",
      "Step 138: val loss: 13.103933334350586\n",
      "Step 139: train loss: 13.411445617675781\n",
      "Step 139: val loss: 13.061152458190918\n",
      "Step 140: train loss: 13.367318153381348\n",
      "Step 140: val loss: 13.018619537353516\n",
      "Step 141: train loss: 13.323444366455078\n",
      "Step 141: val loss: 12.976333618164062\n",
      "Step 142: train loss: 13.279824256896973\n",
      "Step 142: val loss: 12.934290885925293\n",
      "Step 143: train loss: 13.236454963684082\n",
      "Step 143: val loss: 12.89249038696289\n",
      "Step 144: train loss: 13.19333267211914\n",
      "Step 144: val loss: 12.850931167602539\n",
      "Step 145: train loss: 13.150459289550781\n",
      "Step 145: val loss: 12.809609413146973\n",
      "Step 146: train loss: 13.107827186584473\n",
      "Step 146: val loss: 12.768526077270508\n",
      "Step 147: train loss: 13.065439224243164\n",
      "Step 147: val loss: 12.727675437927246\n",
      "Step 148: train loss: 13.02329158782959\n",
      "Step 148: val loss: 12.687060356140137\n",
      "Step 149: train loss: 12.981385231018066\n",
      "Step 149: val loss: 12.646676063537598\n",
      "Step 150: train loss: 12.939714431762695\n",
      "Step 150: val loss: 12.60651969909668\n",
      "Step 151: train loss: 12.898279190063477\n",
      "Step 151: val loss: 12.5665922164917\n",
      "Step 152: train loss: 12.857078552246094\n",
      "Step 152: val loss: 12.52688980102539\n",
      "Step 153: train loss: 12.816109657287598\n",
      "Step 153: val loss: 12.487411499023438\n",
      "Step 154: train loss: 12.775368690490723\n",
      "Step 154: val loss: 12.448153495788574\n",
      "Step 155: train loss: 12.734856605529785\n",
      "Step 155: val loss: 12.409117698669434\n",
      "Step 156: train loss: 12.694572448730469\n",
      "Step 156: val loss: 12.370302200317383\n",
      "Step 157: train loss: 12.654511451721191\n",
      "Step 157: val loss: 12.331701278686523\n",
      "Step 158: train loss: 12.61467456817627\n",
      "Step 158: val loss: 12.293317794799805\n",
      "Step 159: train loss: 12.575057983398438\n",
      "Step 159: val loss: 12.255145072937012\n",
      "Step 160: train loss: 12.535660743713379\n",
      "Step 160: val loss: 12.217188835144043\n",
      "Step 161: train loss: 12.496482849121094\n",
      "Step 161: val loss: 12.179439544677734\n",
      "Step 162: train loss: 12.45751953125\n",
      "Step 162: val loss: 12.141899108886719\n",
      "Step 163: train loss: 12.418771743774414\n",
      "Step 163: val loss: 12.10456657409668\n",
      "Step 164: train loss: 12.38023567199707\n",
      "Step 164: val loss: 12.067439079284668\n",
      "Step 165: train loss: 12.341911315917969\n",
      "Step 165: val loss: 12.030517578125\n",
      "Step 166: train loss: 12.303796768188477\n",
      "Step 166: val loss: 11.993796348571777\n",
      "Step 167: train loss: 12.265892028808594\n",
      "Step 167: val loss: 11.957275390625\n",
      "Step 168: train loss: 12.228190422058105\n",
      "Step 168: val loss: 11.920952796936035\n",
      "Step 169: train loss: 12.190695762634277\n",
      "Step 169: val loss: 11.884832382202148\n",
      "Step 170: train loss: 12.153403282165527\n",
      "Step 170: val loss: 11.848904609680176\n",
      "Step 171: train loss: 12.116314888000488\n",
      "Step 171: val loss: 11.813172340393066\n",
      "Step 172: train loss: 12.079423904418945\n",
      "Step 172: val loss: 11.777632713317871\n",
      "Step 173: train loss: 12.042732238769531\n",
      "Step 173: val loss: 11.742284774780273\n",
      "Step 174: train loss: 12.006239891052246\n",
      "Step 174: val loss: 11.707128524780273\n",
      "Step 175: train loss: 11.969941139221191\n",
      "Step 175: val loss: 11.672159194946289\n",
      "Step 176: train loss: 11.933836936950684\n",
      "Step 176: val loss: 11.637378692626953\n",
      "Step 177: train loss: 11.89792537689209\n",
      "Step 177: val loss: 11.602783203125\n",
      "Step 178: train loss: 11.86220645904541\n",
      "Step 178: val loss: 11.56837272644043\n",
      "Step 179: train loss: 11.826675415039062\n",
      "Step 179: val loss: 11.534143447875977\n",
      "Step 180: train loss: 11.79133415222168\n",
      "Step 180: val loss: 11.500097274780273\n",
      "Step 181: train loss: 11.756179809570312\n",
      "Step 181: val loss: 11.466229438781738\n",
      "Step 182: train loss: 11.721209526062012\n",
      "Step 182: val loss: 11.43254280090332\n",
      "Step 183: train loss: 11.686427116394043\n",
      "Step 183: val loss: 11.399033546447754\n",
      "Step 184: train loss: 11.651823997497559\n",
      "Step 184: val loss: 11.36569881439209\n",
      "Step 185: train loss: 11.617403030395508\n",
      "Step 185: val loss: 11.332540512084961\n",
      "Step 186: train loss: 11.583162307739258\n",
      "Step 186: val loss: 11.299554824829102\n",
      "Step 187: train loss: 11.549101829528809\n",
      "Step 187: val loss: 11.266739845275879\n",
      "Step 188: train loss: 11.515216827392578\n",
      "Step 188: val loss: 11.234098434448242\n",
      "Step 189: train loss: 11.481508255004883\n",
      "Step 189: val loss: 11.201623916625977\n",
      "Step 190: train loss: 11.44797420501709\n",
      "Step 190: val loss: 11.169320106506348\n",
      "Step 191: train loss: 11.414613723754883\n",
      "Step 191: val loss: 11.137182235717773\n",
      "Step 192: train loss: 11.381424903869629\n",
      "Step 192: val loss: 11.105210304260254\n",
      "Step 193: train loss: 11.348406791687012\n",
      "Step 193: val loss: 11.07340145111084\n",
      "Step 194: train loss: 11.315560340881348\n",
      "Step 194: val loss: 11.041755676269531\n",
      "Step 195: train loss: 11.282880783081055\n",
      "Step 195: val loss: 11.010275840759277\n",
      "Step 196: train loss: 11.250368118286133\n",
      "Step 196: val loss: 10.978951454162598\n",
      "Step 197: train loss: 11.218022346496582\n",
      "Step 197: val loss: 10.947790145874023\n",
      "Step 198: train loss: 11.185839653015137\n",
      "Step 198: val loss: 10.916786193847656\n",
      "Step 199: train loss: 11.153820991516113\n",
      "Step 199: val loss: 10.885937690734863\n",
      "Step 200: train loss: 11.121964454650879\n",
      "Step 200: val loss: 10.855246543884277\n",
      "Step 201: train loss: 11.090269088745117\n",
      "Step 201: val loss: 10.824710845947266\n",
      "Step 202: train loss: 11.058732986450195\n",
      "Step 202: val loss: 10.794327735900879\n",
      "Step 203: train loss: 11.027355194091797\n",
      "Step 203: val loss: 10.764098167419434\n",
      "Step 204: train loss: 10.996135711669922\n",
      "Step 204: val loss: 10.73401927947998\n",
      "Step 205: train loss: 10.965073585510254\n",
      "Step 205: val loss: 10.70409107208252\n",
      "Step 206: train loss: 10.934165954589844\n",
      "Step 206: val loss: 10.674310684204102\n",
      "Step 207: train loss: 10.903409957885742\n",
      "Step 207: val loss: 10.644680976867676\n",
      "Step 208: train loss: 10.872808456420898\n",
      "Step 208: val loss: 10.615195274353027\n",
      "Step 209: train loss: 10.842357635498047\n",
      "Step 209: val loss: 10.585855484008789\n",
      "Step 210: train loss: 10.812058448791504\n",
      "Step 210: val loss: 10.556661605834961\n",
      "Step 211: train loss: 10.781909942626953\n",
      "Step 211: val loss: 10.52761173248291\n",
      "Step 212: train loss: 10.751907348632812\n",
      "Step 212: val loss: 10.498703002929688\n",
      "Step 213: train loss: 10.722053527832031\n",
      "Step 213: val loss: 10.469935417175293\n",
      "Step 214: train loss: 10.692346572875977\n",
      "Step 214: val loss: 10.44131088256836\n",
      "Step 215: train loss: 10.662784576416016\n",
      "Step 215: val loss: 10.412823677062988\n",
      "Step 216: train loss: 10.633365631103516\n",
      "Step 216: val loss: 10.384474754333496\n",
      "Step 217: train loss: 10.604089736938477\n",
      "Step 217: val loss: 10.356266021728516\n",
      "Step 218: train loss: 10.574956893920898\n",
      "Step 218: val loss: 10.328189849853516\n",
      "Step 219: train loss: 10.545963287353516\n",
      "Step 219: val loss: 10.300251960754395\n",
      "Step 220: train loss: 10.517112731933594\n",
      "Step 220: val loss: 10.27244758605957\n",
      "Step 221: train loss: 10.488397598266602\n",
      "Step 221: val loss: 10.24477481842041\n",
      "Step 222: train loss: 10.459822654724121\n",
      "Step 222: val loss: 10.217236518859863\n",
      "Step 223: train loss: 10.431384086608887\n",
      "Step 223: val loss: 10.189828872680664\n",
      "Step 224: train loss: 10.403079986572266\n",
      "Step 224: val loss: 10.162550926208496\n",
      "Step 225: train loss: 10.374913215637207\n",
      "Step 225: val loss: 10.135404586791992\n",
      "Step 226: train loss: 10.346879959106445\n",
      "Step 226: val loss: 10.10838508605957\n",
      "Step 227: train loss: 10.318978309631348\n",
      "Step 227: val loss: 10.08149242401123\n",
      "Step 228: train loss: 10.291211128234863\n",
      "Step 228: val loss: 10.054728507995605\n",
      "Step 229: train loss: 10.26357364654541\n",
      "Step 229: val loss: 10.028090476989746\n",
      "Step 230: train loss: 10.236066818237305\n",
      "Step 230: val loss: 10.001577377319336\n",
      "Step 231: train loss: 10.208688735961914\n",
      "Step 231: val loss: 9.975186347961426\n",
      "Step 232: train loss: 10.181439399719238\n",
      "Step 232: val loss: 9.948920249938965\n",
      "Step 233: train loss: 10.154315948486328\n",
      "Step 233: val loss: 9.922774314880371\n",
      "Step 234: train loss: 10.12732219696045\n",
      "Step 234: val loss: 9.89675235748291\n",
      "Step 235: train loss: 10.100452423095703\n",
      "Step 235: val loss: 9.870848655700684\n",
      "Step 236: train loss: 10.073705673217773\n",
      "Step 236: val loss: 9.84506607055664\n",
      "Step 237: train loss: 10.047085762023926\n",
      "Step 237: val loss: 9.819400787353516\n",
      "Step 238: train loss: 10.020586967468262\n",
      "Step 238: val loss: 9.793852806091309\n",
      "Step 239: train loss: 9.994211196899414\n",
      "Step 239: val loss: 9.768424034118652\n",
      "Step 240: train loss: 9.967955589294434\n",
      "Step 240: val loss: 9.743110656738281\n",
      "Step 241: train loss: 9.941821098327637\n",
      "Step 241: val loss: 9.717912673950195\n",
      "Step 242: train loss: 9.915806770324707\n",
      "Step 242: val loss: 9.692827224731445\n",
      "Step 243: train loss: 9.889909744262695\n",
      "Step 243: val loss: 9.66785717010498\n",
      "Step 244: train loss: 9.864130973815918\n",
      "Step 244: val loss: 9.642997741699219\n",
      "Step 245: train loss: 9.838470458984375\n",
      "Step 245: val loss: 9.618254661560059\n",
      "Step 246: train loss: 9.8129243850708\n",
      "Step 246: val loss: 9.593620300292969\n",
      "Step 247: train loss: 9.787494659423828\n",
      "Step 247: val loss: 9.569097518920898\n",
      "Step 248: train loss: 9.762179374694824\n",
      "Step 248: val loss: 9.544683456420898\n",
      "Step 249: train loss: 9.736978530883789\n",
      "Step 249: val loss: 9.520378112792969\n",
      "Step 250: train loss: 9.711889266967773\n",
      "Step 250: val loss: 9.49618148803711\n",
      "Step 251: train loss: 9.686914443969727\n",
      "Step 251: val loss: 9.472092628479004\n",
      "Step 252: train loss: 9.662049293518066\n",
      "Step 252: val loss: 9.448110580444336\n",
      "Step 253: train loss: 9.637293815612793\n",
      "Step 253: val loss: 9.424233436584473\n",
      "Step 254: train loss: 9.612648963928223\n",
      "Step 254: val loss: 9.400461196899414\n",
      "Step 255: train loss: 9.588113784790039\n",
      "Step 255: val loss: 9.376792907714844\n",
      "Step 256: train loss: 9.563687324523926\n",
      "Step 256: val loss: 9.353230476379395\n",
      "Step 257: train loss: 9.539369583129883\n",
      "Step 257: val loss: 9.329768180847168\n",
      "Step 258: train loss: 9.515155792236328\n",
      "Step 258: val loss: 9.306410789489746\n",
      "Step 259: train loss: 9.491048812866211\n",
      "Step 259: val loss: 9.283153533935547\n",
      "Step 260: train loss: 9.467046737670898\n",
      "Step 260: val loss: 9.259998321533203\n",
      "Step 261: train loss: 9.443150520324707\n",
      "Step 261: val loss: 9.23694133758545\n",
      "Step 262: train loss: 9.41935920715332\n",
      "Step 262: val loss: 9.213984489440918\n",
      "Step 263: train loss: 9.395668983459473\n",
      "Step 263: val loss: 9.191126823425293\n",
      "Step 264: train loss: 9.372081756591797\n",
      "Step 264: val loss: 9.168367385864258\n",
      "Step 265: train loss: 9.348597526550293\n",
      "Step 265: val loss: 9.14570426940918\n",
      "Step 266: train loss: 9.325213432312012\n",
      "Step 266: val loss: 9.123137474060059\n",
      "Step 267: train loss: 9.30193042755127\n",
      "Step 267: val loss: 9.100667953491211\n",
      "Step 268: train loss: 9.278745651245117\n",
      "Step 268: val loss: 9.078292846679688\n",
      "Step 269: train loss: 9.255661010742188\n",
      "Step 269: val loss: 9.056014060974121\n",
      "Step 270: train loss: 9.232675552368164\n",
      "Step 270: val loss: 9.033825874328613\n",
      "Step 271: train loss: 9.209786415100098\n",
      "Step 271: val loss: 9.011734008789062\n",
      "Step 272: train loss: 9.186994552612305\n",
      "Step 272: val loss: 8.98973560333252\n",
      "Step 273: train loss: 9.164299011230469\n",
      "Step 273: val loss: 8.967826843261719\n",
      "Step 274: train loss: 9.14169979095459\n",
      "Step 274: val loss: 8.946009635925293\n",
      "Step 275: train loss: 9.119194984436035\n",
      "Step 275: val loss: 8.924284934997559\n",
      "Step 276: train loss: 9.096786499023438\n",
      "Step 276: val loss: 8.902649879455566\n",
      "Step 277: train loss: 9.074468612670898\n",
      "Step 277: val loss: 8.881103515625\n",
      "Step 278: train loss: 9.05224609375\n",
      "Step 278: val loss: 8.859647750854492\n",
      "Step 279: train loss: 9.030116081237793\n",
      "Step 279: val loss: 8.838278770446777\n",
      "Step 280: train loss: 9.008078575134277\n",
      "Step 280: val loss: 8.816998481750488\n",
      "Step 281: train loss: 8.986130714416504\n",
      "Step 281: val loss: 8.795804023742676\n",
      "Step 282: train loss: 8.964273452758789\n",
      "Step 282: val loss: 8.774698257446289\n",
      "Step 283: train loss: 8.942506790161133\n",
      "Step 283: val loss: 8.753676414489746\n",
      "Step 284: train loss: 8.920829772949219\n",
      "Step 284: val loss: 8.732741355895996\n",
      "Step 285: train loss: 8.899243354797363\n",
      "Step 285: val loss: 8.711891174316406\n",
      "Step 286: train loss: 8.877742767333984\n",
      "Step 286: val loss: 8.69112491607666\n",
      "Step 287: train loss: 8.856331825256348\n",
      "Step 287: val loss: 8.670443534851074\n",
      "Step 288: train loss: 8.835006713867188\n",
      "Step 288: val loss: 8.6498441696167\n",
      "Step 289: train loss: 8.81376838684082\n",
      "Step 289: val loss: 8.629326820373535\n",
      "Step 290: train loss: 8.792614936828613\n",
      "Step 290: val loss: 8.608892440795898\n",
      "Step 291: train loss: 8.771549224853516\n",
      "Step 291: val loss: 8.588540077209473\n",
      "Step 292: train loss: 8.750567436218262\n",
      "Step 292: val loss: 8.568266868591309\n",
      "Step 293: train loss: 8.729669570922852\n",
      "Step 293: val loss: 8.548076629638672\n",
      "Step 294: train loss: 8.708855628967285\n",
      "Step 294: val loss: 8.527963638305664\n",
      "Step 295: train loss: 8.688124656677246\n",
      "Step 295: val loss: 8.50793170928955\n",
      "Step 296: train loss: 8.66747760772705\n",
      "Step 296: val loss: 8.4879789352417\n",
      "Step 297: train loss: 8.646910667419434\n",
      "Step 297: val loss: 8.46810245513916\n",
      "Step 298: train loss: 8.626425743103027\n",
      "Step 298: val loss: 8.4483060836792\n",
      "Step 299: train loss: 8.606023788452148\n",
      "Step 299: val loss: 8.428586959838867\n",
      "Step 300: train loss: 8.585700988769531\n",
      "Step 300: val loss: 8.408942222595215\n",
      "Step 301: train loss: 8.565459251403809\n",
      "Step 301: val loss: 8.389375686645508\n",
      "Step 302: train loss: 8.545294761657715\n",
      "Step 302: val loss: 8.369884490966797\n",
      "Step 303: train loss: 8.5252103805542\n",
      "Step 303: val loss: 8.350469589233398\n",
      "Step 304: train loss: 8.505205154418945\n",
      "Step 304: val loss: 8.331128120422363\n",
      "Step 305: train loss: 8.48527717590332\n",
      "Step 305: val loss: 8.311861038208008\n",
      "Step 306: train loss: 8.465426445007324\n",
      "Step 306: val loss: 8.292670249938965\n",
      "Step 307: train loss: 8.445652961730957\n",
      "Step 307: val loss: 8.273550033569336\n",
      "Step 308: train loss: 8.425956726074219\n",
      "Step 308: val loss: 8.254504203796387\n",
      "Step 309: train loss: 8.406335830688477\n",
      "Step 309: val loss: 8.235528945922852\n",
      "Step 310: train loss: 8.38679027557373\n",
      "Step 310: val loss: 8.21662712097168\n",
      "Step 311: train loss: 8.36732006072998\n",
      "Step 311: val loss: 8.197796821594238\n",
      "Step 312: train loss: 8.34792423248291\n",
      "Step 312: val loss: 8.179037094116211\n",
      "Step 313: train loss: 8.328601837158203\n",
      "Step 313: val loss: 8.160347938537598\n",
      "Step 314: train loss: 8.309353828430176\n",
      "Step 314: val loss: 8.141727447509766\n",
      "Step 315: train loss: 8.290177345275879\n",
      "Step 315: val loss: 8.12317943572998\n",
      "Step 316: train loss: 8.271076202392578\n",
      "Step 316: val loss: 8.10469913482666\n",
      "Step 317: train loss: 8.252044677734375\n",
      "Step 317: val loss: 8.086288452148438\n",
      "Step 318: train loss: 8.233085632324219\n",
      "Step 318: val loss: 8.067944526672363\n",
      "Step 319: train loss: 8.214200019836426\n",
      "Step 319: val loss: 8.049671173095703\n",
      "Step 320: train loss: 8.195383071899414\n",
      "Step 320: val loss: 8.031463623046875\n",
      "Step 321: train loss: 8.176637649536133\n",
      "Step 321: val loss: 8.013322830200195\n",
      "Step 322: train loss: 8.157960891723633\n",
      "Step 322: val loss: 7.995247840881348\n",
      "Step 323: train loss: 8.139354705810547\n",
      "Step 323: val loss: 7.977241039276123\n",
      "Step 324: train loss: 8.120817184448242\n",
      "Step 324: val loss: 7.959299087524414\n",
      "Step 325: train loss: 8.102348327636719\n",
      "Step 325: val loss: 7.941422462463379\n",
      "Step 326: train loss: 8.08394718170166\n",
      "Step 326: val loss: 7.923611640930176\n",
      "Step 327: train loss: 8.065614700317383\n",
      "Step 327: val loss: 7.9058637619018555\n",
      "Step 328: train loss: 8.047348976135254\n",
      "Step 328: val loss: 7.888181686401367\n",
      "Step 329: train loss: 8.02915096282959\n",
      "Step 329: val loss: 7.870562553405762\n",
      "Step 330: train loss: 8.011017799377441\n",
      "Step 330: val loss: 7.8530073165893555\n",
      "Step 331: train loss: 7.992952823638916\n",
      "Step 331: val loss: 7.835514545440674\n",
      "Step 332: train loss: 7.9749531745910645\n",
      "Step 332: val loss: 7.81808614730835\n",
      "Step 333: train loss: 7.957017421722412\n",
      "Step 333: val loss: 7.800716876983643\n",
      "Step 334: train loss: 7.93914794921875\n",
      "Step 334: val loss: 7.783411026000977\n",
      "Step 335: train loss: 7.921342849731445\n",
      "Step 335: val loss: 7.7661662101745605\n",
      "Step 336: train loss: 7.90360164642334\n",
      "Step 336: val loss: 7.748983383178711\n",
      "Step 337: train loss: 7.885924339294434\n",
      "Step 337: val loss: 7.731860160827637\n",
      "Step 338: train loss: 7.868310928344727\n",
      "Step 338: val loss: 7.714797496795654\n",
      "Step 339: train loss: 7.850759983062744\n",
      "Step 339: val loss: 7.6977949142456055\n",
      "Step 340: train loss: 7.83327054977417\n",
      "Step 340: val loss: 7.680851936340332\n",
      "Step 341: train loss: 7.8158440589904785\n",
      "Step 341: val loss: 7.663968086242676\n",
      "Step 342: train loss: 7.7984795570373535\n",
      "Step 342: val loss: 7.64714241027832\n",
      "Step 343: train loss: 7.781177043914795\n",
      "Step 343: val loss: 7.630376815795898\n",
      "Step 344: train loss: 7.763935089111328\n",
      "Step 344: val loss: 7.613668441772461\n",
      "Step 345: train loss: 7.746753215789795\n",
      "Step 345: val loss: 7.597017765045166\n",
      "Step 346: train loss: 7.72963285446167\n",
      "Step 346: val loss: 7.5804243087768555\n",
      "Step 347: train loss: 7.71257209777832\n",
      "Step 347: val loss: 7.5638885498046875\n",
      "Step 348: train loss: 7.695571422576904\n",
      "Step 348: val loss: 7.547409534454346\n",
      "Step 349: train loss: 7.678629398345947\n",
      "Step 349: val loss: 7.530986785888672\n",
      "Step 350: train loss: 7.661746025085449\n",
      "Step 350: val loss: 7.514620780944824\n",
      "Step 351: train loss: 7.644921779632568\n",
      "Step 351: val loss: 7.498310565948486\n",
      "Step 352: train loss: 7.628155708312988\n",
      "Step 352: val loss: 7.482053756713867\n",
      "Step 353: train loss: 7.611447334289551\n",
      "Step 353: val loss: 7.465853214263916\n",
      "Step 354: train loss: 7.594796657562256\n",
      "Step 354: val loss: 7.44970703125\n",
      "Step 355: train loss: 7.5782036781311035\n",
      "Step 355: val loss: 7.4336161613464355\n",
      "Step 356: train loss: 7.561667442321777\n",
      "Step 356: val loss: 7.417579174041748\n",
      "Step 357: train loss: 7.545186519622803\n",
      "Step 357: val loss: 7.4015960693359375\n",
      "Step 358: train loss: 7.528763294219971\n",
      "Step 358: val loss: 7.3856658935546875\n",
      "Step 359: train loss: 7.51239538192749\n",
      "Step 359: val loss: 7.369789123535156\n",
      "Step 360: train loss: 7.496082305908203\n",
      "Step 360: val loss: 7.3539652824401855\n",
      "Step 361: train loss: 7.479825019836426\n",
      "Step 361: val loss: 7.338193416595459\n",
      "Step 362: train loss: 7.463623523712158\n",
      "Step 362: val loss: 7.322474002838135\n",
      "Step 363: train loss: 7.447474956512451\n",
      "Step 363: val loss: 7.306807518005371\n",
      "Step 364: train loss: 7.431382656097412\n",
      "Step 364: val loss: 7.291192054748535\n",
      "Step 365: train loss: 7.415343761444092\n",
      "Step 365: val loss: 7.275628089904785\n",
      "Step 366: train loss: 7.399357795715332\n",
      "Step 366: val loss: 7.260114669799805\n",
      "Step 367: train loss: 7.383425235748291\n",
      "Step 367: val loss: 7.244652271270752\n",
      "Step 368: train loss: 7.367546081542969\n",
      "Step 368: val loss: 7.229241371154785\n",
      "Step 369: train loss: 7.351719856262207\n",
      "Step 369: val loss: 7.213879108428955\n",
      "Step 370: train loss: 7.335945129394531\n",
      "Step 370: val loss: 7.19856595993042\n",
      "Step 371: train loss: 7.320223808288574\n",
      "Step 371: val loss: 7.183304786682129\n",
      "Step 372: train loss: 7.304553508758545\n",
      "Step 372: val loss: 7.1680908203125\n",
      "Step 373: train loss: 7.288934230804443\n",
      "Step 373: val loss: 7.152927875518799\n",
      "Step 374: train loss: 7.273367881774902\n",
      "Step 374: val loss: 7.137811660766602\n",
      "Step 375: train loss: 7.257850646972656\n",
      "Step 375: val loss: 7.122745513916016\n",
      "Step 376: train loss: 7.242385387420654\n",
      "Step 376: val loss: 7.107726573944092\n",
      "Step 377: train loss: 7.226970672607422\n",
      "Step 377: val loss: 7.0927557945251465\n",
      "Step 378: train loss: 7.211605072021484\n",
      "Step 378: val loss: 7.077833652496338\n",
      "Step 379: train loss: 7.1962890625\n",
      "Step 379: val loss: 7.0629563331604\n",
      "Step 380: train loss: 7.181023597717285\n",
      "Step 380: val loss: 7.048129081726074\n",
      "Step 381: train loss: 7.165806770324707\n",
      "Step 381: val loss: 7.033347129821777\n",
      "Step 382: train loss: 7.150638580322266\n",
      "Step 382: val loss: 7.018611907958984\n",
      "Step 383: train loss: 7.135519504547119\n",
      "Step 383: val loss: 7.003922462463379\n",
      "Step 384: train loss: 7.120449542999268\n",
      "Step 384: val loss: 6.9892802238464355\n",
      "Step 385: train loss: 7.105426788330078\n",
      "Step 385: val loss: 6.9746832847595215\n",
      "Step 386: train loss: 7.090453147888184\n",
      "Step 386: val loss: 6.960132122039795\n",
      "Step 387: train loss: 7.075525760650635\n",
      "Step 387: val loss: 6.9456257820129395\n",
      "Step 388: train loss: 7.060647487640381\n",
      "Step 388: val loss: 6.931164741516113\n",
      "Step 389: train loss: 7.045815467834473\n",
      "Step 389: val loss: 6.916749000549316\n",
      "Step 390: train loss: 7.03102970123291\n",
      "Step 390: val loss: 6.902377605438232\n",
      "Step 391: train loss: 7.016291618347168\n",
      "Step 391: val loss: 6.888049602508545\n",
      "Step 392: train loss: 7.001599311828613\n",
      "Step 392: val loss: 6.8737664222717285\n",
      "Step 393: train loss: 6.986952781677246\n",
      "Step 393: val loss: 6.859527111053467\n",
      "Step 394: train loss: 6.972351551055908\n",
      "Step 394: val loss: 6.845332622528076\n",
      "Step 395: train loss: 6.957797050476074\n",
      "Step 395: val loss: 6.831179141998291\n",
      "Step 396: train loss: 6.943286895751953\n",
      "Step 396: val loss: 6.817070484161377\n",
      "Step 397: train loss: 6.928823471069336\n",
      "Step 397: val loss: 6.803002834320068\n",
      "Step 398: train loss: 6.914403438568115\n",
      "Step 398: val loss: 6.788979530334473\n",
      "Step 399: train loss: 6.900028228759766\n",
      "Step 399: val loss: 6.774999141693115\n",
      "Step 400: train loss: 6.885699272155762\n",
      "Step 400: val loss: 6.76106071472168\n",
      "Step 401: train loss: 6.871412754058838\n",
      "Step 401: val loss: 6.747162818908691\n",
      "Step 402: train loss: 6.8571696281433105\n",
      "Step 402: val loss: 6.733307361602783\n",
      "Step 403: train loss: 6.8429718017578125\n",
      "Step 403: val loss: 6.719493865966797\n",
      "Step 404: train loss: 6.8288164138793945\n",
      "Step 404: val loss: 6.705722332000732\n",
      "Step 405: train loss: 6.814703941345215\n",
      "Step 405: val loss: 6.691990852355957\n",
      "Step 406: train loss: 6.800635814666748\n",
      "Step 406: val loss: 6.678300380706787\n",
      "Step 407: train loss: 6.786609172821045\n",
      "Step 407: val loss: 6.664651393890381\n",
      "Step 408: train loss: 6.7726263999938965\n",
      "Step 408: val loss: 6.6510419845581055\n",
      "Step 409: train loss: 6.7586846351623535\n",
      "Step 409: val loss: 6.637473106384277\n",
      "Step 410: train loss: 6.744785785675049\n",
      "Step 410: val loss: 6.623944282531738\n",
      "Step 411: train loss: 6.73092794418335\n",
      "Step 411: val loss: 6.610455513000488\n",
      "Step 412: train loss: 6.717113018035889\n",
      "Step 412: val loss: 6.597005367279053\n",
      "Step 413: train loss: 6.703338146209717\n",
      "Step 413: val loss: 6.5835957527160645\n",
      "Step 414: train loss: 6.689605236053467\n",
      "Step 414: val loss: 6.570224761962891\n",
      "Step 415: train loss: 6.675912857055664\n",
      "Step 415: val loss: 6.556893348693848\n",
      "Step 416: train loss: 6.6622633934021\n",
      "Step 416: val loss: 6.543600082397461\n",
      "Step 417: train loss: 6.648651599884033\n",
      "Step 417: val loss: 6.530346393585205\n",
      "Step 418: train loss: 6.635081768035889\n",
      "Step 418: val loss: 6.517130374908447\n",
      "Step 419: train loss: 6.621551036834717\n",
      "Step 419: val loss: 6.503952503204346\n",
      "Step 420: train loss: 6.608060359954834\n",
      "Step 420: val loss: 6.4908127784729\n",
      "Step 421: train loss: 6.594610691070557\n",
      "Step 421: val loss: 6.477711200714111\n",
      "Step 422: train loss: 6.581201076507568\n",
      "Step 422: val loss: 6.464646816253662\n",
      "Step 423: train loss: 6.567829132080078\n",
      "Step 423: val loss: 6.451619625091553\n",
      "Step 424: train loss: 6.554496765136719\n",
      "Step 424: val loss: 6.438631534576416\n",
      "Step 425: train loss: 6.541203498840332\n",
      "Step 425: val loss: 6.425678730010986\n",
      "Step 426: train loss: 6.527950286865234\n",
      "Step 426: val loss: 6.412763595581055\n",
      "Step 427: train loss: 6.514734268188477\n",
      "Step 427: val loss: 6.399884223937988\n",
      "Step 428: train loss: 6.501557350158691\n",
      "Step 428: val loss: 6.38704252243042\n",
      "Step 429: train loss: 6.488418102264404\n",
      "Step 429: val loss: 6.374236106872559\n",
      "Step 430: train loss: 6.475316524505615\n",
      "Step 430: val loss: 6.361466884613037\n",
      "Step 431: train loss: 6.462253093719482\n",
      "Step 431: val loss: 6.348732948303223\n",
      "Step 432: train loss: 6.449226379394531\n",
      "Step 432: val loss: 6.336035251617432\n",
      "Step 433: train loss: 6.436238765716553\n",
      "Step 433: val loss: 6.323371887207031\n",
      "Step 434: train loss: 6.423287868499756\n",
      "Step 434: val loss: 6.3107452392578125\n",
      "Step 435: train loss: 6.410372734069824\n",
      "Step 435: val loss: 6.298154354095459\n",
      "Step 436: train loss: 6.397496700286865\n",
      "Step 436: val loss: 6.285597801208496\n",
      "Step 437: train loss: 6.384655475616455\n",
      "Step 437: val loss: 6.273075580596924\n",
      "Step 438: train loss: 6.371851444244385\n",
      "Step 438: val loss: 6.260589122772217\n",
      "Step 439: train loss: 6.359084129333496\n",
      "Step 439: val loss: 6.248136520385742\n",
      "Step 440: train loss: 6.3463521003723145\n",
      "Step 440: val loss: 6.235718250274658\n",
      "Step 441: train loss: 6.333656311035156\n",
      "Step 441: val loss: 6.2233357429504395\n",
      "Step 442: train loss: 6.320997714996338\n",
      "Step 442: val loss: 6.2109856605529785\n",
      "Step 443: train loss: 6.308373928070068\n",
      "Step 443: val loss: 6.198669910430908\n",
      "Step 444: train loss: 6.2957844734191895\n",
      "Step 444: val loss: 6.18638801574707\n",
      "Step 445: train loss: 6.283230304718018\n",
      "Step 445: val loss: 6.174139022827148\n",
      "Step 446: train loss: 6.270712375640869\n",
      "Step 446: val loss: 6.161924839019775\n",
      "Step 447: train loss: 6.258230209350586\n",
      "Step 447: val loss: 6.149743556976318\n",
      "Step 448: train loss: 6.245781421661377\n",
      "Step 448: val loss: 6.137594699859619\n",
      "Step 449: train loss: 6.233367443084717\n",
      "Step 449: val loss: 6.125480651855469\n",
      "Step 450: train loss: 6.220987796783447\n",
      "Step 450: val loss: 6.113397598266602\n",
      "Step 451: train loss: 6.208643436431885\n",
      "Step 451: val loss: 6.10134744644165\n",
      "Step 452: train loss: 6.196331977844238\n",
      "Step 452: val loss: 6.089329242706299\n",
      "Step 453: train loss: 6.184055328369141\n",
      "Step 453: val loss: 6.077345848083496\n",
      "Step 454: train loss: 6.171812057495117\n",
      "Step 454: val loss: 6.065391540527344\n",
      "Step 455: train loss: 6.159602165222168\n",
      "Step 455: val loss: 6.053471088409424\n",
      "Step 456: train loss: 6.147426128387451\n",
      "Step 456: val loss: 6.041581630706787\n",
      "Step 457: train loss: 6.135283946990967\n",
      "Step 457: val loss: 6.029725551605225\n",
      "Step 458: train loss: 6.12317419052124\n",
      "Step 458: val loss: 6.017899513244629\n",
      "Step 459: train loss: 6.111097812652588\n",
      "Step 459: val loss: 6.006105422973633\n",
      "Step 460: train loss: 6.099053859710693\n",
      "Step 460: val loss: 5.994343280792236\n",
      "Step 461: train loss: 6.087043762207031\n",
      "Step 461: val loss: 5.982611179351807\n",
      "Step 462: train loss: 6.075066566467285\n",
      "Step 462: val loss: 5.970911026000977\n",
      "Step 463: train loss: 6.0631184577941895\n",
      "Step 463: val loss: 5.959240436553955\n",
      "Step 464: train loss: 6.051205158233643\n",
      "Step 464: val loss: 5.94760274887085\n",
      "Step 465: train loss: 6.039323806762695\n",
      "Step 465: val loss: 5.9359941482543945\n",
      "Step 466: train loss: 6.027474880218506\n",
      "Step 466: val loss: 5.924417018890381\n",
      "Step 467: train loss: 6.015657424926758\n",
      "Step 467: val loss: 5.912868976593018\n",
      "Step 468: train loss: 6.003870964050293\n",
      "Step 468: val loss: 5.901351451873779\n",
      "Step 469: train loss: 5.992117404937744\n",
      "Step 469: val loss: 5.889864444732666\n",
      "Step 470: train loss: 5.980393409729004\n",
      "Step 470: val loss: 5.878406047821045\n",
      "Step 471: train loss: 5.9687018394470215\n",
      "Step 471: val loss: 5.866979598999023\n",
      "Step 472: train loss: 5.957040786743164\n",
      "Step 472: val loss: 5.855581283569336\n",
      "Step 473: train loss: 5.945411205291748\n",
      "Step 473: val loss: 5.844212055206299\n",
      "Step 474: train loss: 5.933811664581299\n",
      "Step 474: val loss: 5.832874298095703\n",
      "Step 475: train loss: 5.922243595123291\n",
      "Step 475: val loss: 5.821563243865967\n",
      "Step 476: train loss: 5.91070556640625\n",
      "Step 476: val loss: 5.810283184051514\n",
      "Step 477: train loss: 5.899198055267334\n",
      "Step 477: val loss: 5.799030303955078\n",
      "Step 478: train loss: 5.887720584869385\n",
      "Step 478: val loss: 5.787808418273926\n",
      "Step 479: train loss: 5.876274585723877\n",
      "Step 479: val loss: 5.776615142822266\n",
      "Step 480: train loss: 5.864857196807861\n",
      "Step 480: val loss: 5.765449047088623\n",
      "Step 481: train loss: 5.853470325469971\n",
      "Step 481: val loss: 5.754312515258789\n",
      "Step 482: train loss: 5.842113494873047\n",
      "Step 482: val loss: 5.743203639984131\n",
      "Step 483: train loss: 5.830785751342773\n",
      "Step 483: val loss: 5.732123851776123\n",
      "Step 484: train loss: 5.81948709487915\n",
      "Step 484: val loss: 5.721071720123291\n",
      "Step 485: train loss: 5.808218002319336\n",
      "Step 485: val loss: 5.710047721862793\n",
      "Step 486: train loss: 5.7969794273376465\n",
      "Step 486: val loss: 5.699051380157471\n",
      "Step 487: train loss: 5.785768508911133\n",
      "Step 487: val loss: 5.688083171844482\n",
      "Step 488: train loss: 5.7745866775512695\n",
      "Step 488: val loss: 5.67714262008667\n",
      "Step 489: train loss: 5.763434410095215\n",
      "Step 489: val loss: 5.666228294372559\n",
      "Step 490: train loss: 5.7523112297058105\n",
      "Step 490: val loss: 5.655343532562256\n",
      "Step 491: train loss: 5.741215229034424\n",
      "Step 491: val loss: 5.644484519958496\n",
      "Step 492: train loss: 5.7301483154296875\n",
      "Step 492: val loss: 5.63365364074707\n",
      "Step 493: train loss: 5.719110012054443\n",
      "Step 493: val loss: 5.622848987579346\n",
      "Step 494: train loss: 5.70810079574585\n",
      "Step 494: val loss: 5.612072944641113\n",
      "Step 495: train loss: 5.697118759155273\n",
      "Step 495: val loss: 5.601322174072266\n",
      "Step 496: train loss: 5.686164379119873\n",
      "Step 496: val loss: 5.5905985832214355\n",
      "Step 497: train loss: 5.675238132476807\n",
      "Step 497: val loss: 5.579901218414307\n",
      "Step 498: train loss: 5.664340019226074\n",
      "Step 498: val loss: 5.569231033325195\n",
      "Step 499: train loss: 5.653469085693359\n",
      "Step 499: val loss: 5.558587074279785\n",
      "Step 500: train loss: 5.64262580871582\n",
      "Step 500: val loss: 5.54796838760376\n",
      "Step 501: train loss: 5.631810665130615\n",
      "Step 501: val loss: 5.537378311157227\n",
      "Step 502: train loss: 5.621021747589111\n",
      "Step 502: val loss: 5.526812553405762\n",
      "Step 503: train loss: 5.610260963439941\n",
      "Step 503: val loss: 5.51627254486084\n",
      "Step 504: train loss: 5.599526405334473\n",
      "Step 504: val loss: 5.505759239196777\n",
      "Step 505: train loss: 5.58881950378418\n",
      "Step 505: val loss: 5.4952712059021\n",
      "Step 506: train loss: 5.578138828277588\n",
      "Step 506: val loss: 5.484808921813965\n",
      "Step 507: train loss: 5.5674848556518555\n",
      "Step 507: val loss: 5.474371433258057\n",
      "Step 508: train loss: 5.556857585906982\n",
      "Step 508: val loss: 5.463961124420166\n",
      "Step 509: train loss: 5.546257972717285\n",
      "Step 509: val loss: 5.453575611114502\n",
      "Step 510: train loss: 5.5356831550598145\n",
      "Step 510: val loss: 5.4432148933410645\n",
      "Step 511: train loss: 5.525135517120361\n",
      "Step 511: val loss: 5.432878494262695\n",
      "Step 512: train loss: 5.514614105224609\n",
      "Step 512: val loss: 5.422567844390869\n",
      "Step 513: train loss: 5.5041184425354\n",
      "Step 513: val loss: 5.4122819900512695\n",
      "Step 514: train loss: 5.493648529052734\n",
      "Step 514: val loss: 5.402021408081055\n",
      "Step 515: train loss: 5.483204364776611\n",
      "Step 515: val loss: 5.391784191131592\n",
      "Step 516: train loss: 5.4727864265441895\n",
      "Step 516: val loss: 5.381574630737305\n",
      "Step 517: train loss: 5.462395191192627\n",
      "Step 517: val loss: 5.371387481689453\n",
      "Step 518: train loss: 5.452027797698975\n",
      "Step 518: val loss: 5.361225128173828\n",
      "Step 519: train loss: 5.441687107086182\n",
      "Step 519: val loss: 5.3510870933532715\n",
      "Step 520: train loss: 5.431371688842773\n",
      "Step 520: val loss: 5.340973854064941\n",
      "Step 521: train loss: 5.421080589294434\n",
      "Step 521: val loss: 5.330883502960205\n",
      "Step 522: train loss: 5.410815238952637\n",
      "Step 522: val loss: 5.320818901062012\n",
      "Step 523: train loss: 5.400575160980225\n",
      "Step 523: val loss: 5.310776233673096\n",
      "Step 524: train loss: 5.390359401702881\n",
      "Step 524: val loss: 5.300759792327881\n",
      "Step 525: train loss: 5.380168914794922\n",
      "Step 525: val loss: 5.290765285491943\n",
      "Step 526: train loss: 5.370002746582031\n",
      "Step 526: val loss: 5.280796051025391\n",
      "Step 527: train loss: 5.359861373901367\n",
      "Step 527: val loss: 5.270848751068115\n",
      "Step 528: train loss: 5.349745273590088\n",
      "Step 528: val loss: 5.260925769805908\n",
      "Step 529: train loss: 5.339653491973877\n",
      "Step 529: val loss: 5.251026153564453\n",
      "Step 530: train loss: 5.329585075378418\n",
      "Step 530: val loss: 5.24114990234375\n",
      "Step 531: train loss: 5.3195414543151855\n",
      "Step 531: val loss: 5.231296539306641\n",
      "Step 532: train loss: 5.30952262878418\n",
      "Step 532: val loss: 5.221467018127441\n",
      "Step 533: train loss: 5.299527168273926\n",
      "Step 533: val loss: 5.211660385131836\n",
      "Step 534: train loss: 5.28955602645874\n",
      "Step 534: val loss: 5.201876163482666\n",
      "Step 535: train loss: 5.279608249664307\n",
      "Step 535: val loss: 5.19211483001709\n",
      "Step 536: train loss: 5.269685745239258\n",
      "Step 536: val loss: 5.182377815246582\n",
      "Step 537: train loss: 5.259786128997803\n",
      "Step 537: val loss: 5.172661781311035\n",
      "Step 538: train loss: 5.2499098777771\n",
      "Step 538: val loss: 5.162969589233398\n",
      "Step 539: train loss: 5.240057468414307\n",
      "Step 539: val loss: 5.153299331665039\n",
      "Step 540: train loss: 5.230228424072266\n",
      "Step 540: val loss: 5.143651485443115\n",
      "Step 541: train loss: 5.220422744750977\n",
      "Step 541: val loss: 5.134027004241943\n",
      "Step 542: train loss: 5.210639953613281\n",
      "Step 542: val loss: 5.124423980712891\n",
      "Step 543: train loss: 5.200881004333496\n",
      "Step 543: val loss: 5.114843368530273\n",
      "Step 544: train loss: 5.191145420074463\n",
      "Step 544: val loss: 5.105284690856934\n",
      "Step 545: train loss: 5.181432247161865\n",
      "Step 545: val loss: 5.095747947692871\n",
      "Step 546: train loss: 5.171741962432861\n",
      "Step 546: val loss: 5.086233615875244\n",
      "Step 547: train loss: 5.162075042724609\n",
      "Step 547: val loss: 5.076741695404053\n",
      "Step 548: train loss: 5.152430534362793\n",
      "Step 548: val loss: 5.067270278930664\n",
      "Step 549: train loss: 5.142808437347412\n",
      "Step 549: val loss: 5.057821750640869\n",
      "Step 550: train loss: 5.133208751678467\n",
      "Step 550: val loss: 5.048394680023193\n",
      "Step 551: train loss: 5.123632907867432\n",
      "Step 551: val loss: 5.03898811340332\n",
      "Step 552: train loss: 5.114078044891357\n",
      "Step 552: val loss: 5.029603958129883\n",
      "Step 553: train loss: 5.104546070098877\n",
      "Step 553: val loss: 5.020241737365723\n",
      "Step 554: train loss: 5.095036506652832\n",
      "Step 554: val loss: 5.010900974273682\n",
      "Step 555: train loss: 5.085549831390381\n",
      "Step 555: val loss: 5.001580715179443\n",
      "Step 556: train loss: 5.076084613800049\n",
      "Step 556: val loss: 4.992282390594482\n",
      "Step 557: train loss: 5.0666399002075195\n",
      "Step 557: val loss: 4.983003616333008\n",
      "Step 558: train loss: 5.057218551635742\n",
      "Step 558: val loss: 4.973747730255127\n",
      "Step 559: train loss: 5.047820091247559\n",
      "Step 559: val loss: 4.964511871337891\n",
      "Step 560: train loss: 5.0384416580200195\n",
      "Step 560: val loss: 4.955297470092773\n",
      "Step 561: train loss: 5.029086112976074\n",
      "Step 561: val loss: 4.946103572845459\n",
      "Step 562: train loss: 5.01975154876709\n",
      "Step 562: val loss: 4.936929702758789\n",
      "Step 563: train loss: 5.010437965393066\n",
      "Step 563: val loss: 4.9277777671813965\n",
      "Step 564: train loss: 5.0011467933654785\n",
      "Step 564: val loss: 4.91864538192749\n",
      "Step 565: train loss: 4.991877555847168\n",
      "Step 565: val loss: 4.909534931182861\n",
      "Step 566: train loss: 4.982628345489502\n",
      "Step 566: val loss: 4.900444507598877\n",
      "Step 567: train loss: 4.9734015464782715\n",
      "Step 567: val loss: 4.891374111175537\n",
      "Step 568: train loss: 4.964195251464844\n",
      "Step 568: val loss: 4.882323265075684\n",
      "Step 569: train loss: 4.955010890960693\n",
      "Step 569: val loss: 4.873294353485107\n",
      "Step 570: train loss: 4.945845603942871\n",
      "Step 570: val loss: 4.864284038543701\n",
      "Step 571: train loss: 4.936702728271484\n",
      "Step 571: val loss: 4.855295181274414\n",
      "Step 572: train loss: 4.927579879760742\n",
      "Step 572: val loss: 4.8463263511657715\n",
      "Step 573: train loss: 4.918478965759277\n",
      "Step 573: val loss: 4.837376594543457\n",
      "Step 574: train loss: 4.909398078918457\n",
      "Step 574: val loss: 4.828447341918945\n",
      "Step 575: train loss: 4.900338172912598\n",
      "Step 575: val loss: 4.819537162780762\n",
      "Step 576: train loss: 4.891298294067383\n",
      "Step 576: val loss: 4.810647964477539\n",
      "Step 577: train loss: 4.882279872894287\n",
      "Step 577: val loss: 4.801778316497803\n",
      "Step 578: train loss: 4.873281478881836\n",
      "Step 578: val loss: 4.792927265167236\n",
      "Step 579: train loss: 4.864302635192871\n",
      "Step 579: val loss: 4.7840962409973145\n",
      "Step 580: train loss: 4.855345249176025\n",
      "Step 580: val loss: 4.775285720825195\n",
      "Step 581: train loss: 4.846407890319824\n",
      "Step 581: val loss: 4.766493797302246\n",
      "Step 582: train loss: 4.837489128112793\n",
      "Step 582: val loss: 4.757721424102783\n",
      "Step 583: train loss: 4.828592300415039\n",
      "Step 583: val loss: 4.748968124389648\n",
      "Step 584: train loss: 4.8197150230407715\n",
      "Step 584: val loss: 4.740234375\n",
      "Step 585: train loss: 4.810857772827148\n",
      "Step 585: val loss: 4.73151969909668\n",
      "Step 586: train loss: 4.802020072937012\n",
      "Step 586: val loss: 4.7228240966796875\n",
      "Step 587: train loss: 4.793201923370361\n",
      "Step 587: val loss: 4.714148998260498\n",
      "Step 588: train loss: 4.784404277801514\n",
      "Step 588: val loss: 4.705490589141846\n",
      "Step 589: train loss: 4.7756266593933105\n",
      "Step 589: val loss: 4.696852684020996\n",
      "Step 590: train loss: 4.766867160797119\n",
      "Step 590: val loss: 4.688233375549316\n",
      "Step 591: train loss: 4.758128643035889\n",
      "Step 591: val loss: 4.679632186889648\n",
      "Step 592: train loss: 4.749408721923828\n",
      "Step 592: val loss: 4.671051979064941\n",
      "Step 593: train loss: 4.74070930480957\n",
      "Step 593: val loss: 4.662487983703613\n",
      "Step 594: train loss: 4.732028007507324\n",
      "Step 594: val loss: 4.6539435386657715\n",
      "Step 595: train loss: 4.723366737365723\n",
      "Step 595: val loss: 4.645416736602783\n",
      "Step 596: train loss: 4.714723587036133\n",
      "Step 596: val loss: 4.636909008026123\n",
      "Step 597: train loss: 4.706100940704346\n",
      "Step 597: val loss: 4.628420352935791\n",
      "Step 598: train loss: 4.69749641418457\n",
      "Step 598: val loss: 4.619950294494629\n",
      "Step 599: train loss: 4.6889119148254395\n",
      "Step 599: val loss: 4.61149787902832\n",
      "Step 600: train loss: 4.68034553527832\n",
      "Step 600: val loss: 4.60306453704834\n",
      "Step 601: train loss: 4.671798229217529\n",
      "Step 601: val loss: 4.594648361206055\n",
      "Step 602: train loss: 4.663270473480225\n",
      "Step 602: val loss: 4.586251735687256\n",
      "Step 603: train loss: 4.654760837554932\n",
      "Step 603: val loss: 4.577871799468994\n",
      "Step 604: train loss: 4.646270275115967\n",
      "Step 604: val loss: 4.569511413574219\n",
      "Step 605: train loss: 4.63779878616333\n",
      "Step 605: val loss: 4.561168193817139\n",
      "Step 606: train loss: 4.629345417022705\n",
      "Step 606: val loss: 4.552842617034912\n",
      "Step 607: train loss: 4.62091064453125\n",
      "Step 607: val loss: 4.544535160064697\n",
      "Step 608: train loss: 4.612493515014648\n",
      "Step 608: val loss: 4.536245822906494\n",
      "Step 609: train loss: 4.60409688949585\n",
      "Step 609: val loss: 4.527973651885986\n",
      "Step 610: train loss: 4.59571647644043\n",
      "Step 610: val loss: 4.519719123840332\n",
      "Step 611: train loss: 4.587355613708496\n",
      "Step 611: val loss: 4.511483192443848\n",
      "Step 612: train loss: 4.579011917114258\n",
      "Step 612: val loss: 4.5032639503479\n",
      "Step 613: train loss: 4.570687294006348\n",
      "Step 613: val loss: 4.495062828063965\n",
      "Step 614: train loss: 4.562380313873291\n",
      "Step 614: val loss: 4.486878395080566\n",
      "Step 615: train loss: 4.554091453552246\n",
      "Step 615: val loss: 4.478713035583496\n",
      "Step 616: train loss: 4.545821189880371\n",
      "Step 616: val loss: 4.4705634117126465\n",
      "Step 617: train loss: 4.537568092346191\n",
      "Step 617: val loss: 4.462433338165283\n",
      "Step 618: train loss: 4.52933406829834\n",
      "Step 618: val loss: 4.454318523406982\n",
      "Step 619: train loss: 4.521116733551025\n",
      "Step 619: val loss: 4.446222305297852\n",
      "Step 620: train loss: 4.512918949127197\n",
      "Step 620: val loss: 4.4381422996521\n",
      "Step 621: train loss: 4.504737377166748\n",
      "Step 621: val loss: 4.430079460144043\n",
      "Step 622: train loss: 4.496574401855469\n",
      "Step 622: val loss: 4.422035217285156\n",
      "Step 623: train loss: 4.488428592681885\n",
      "Step 623: val loss: 4.41400671005249\n",
      "Step 624: train loss: 4.4803009033203125\n",
      "Step 624: val loss: 4.405994892120361\n",
      "Step 625: train loss: 4.472189903259277\n",
      "Step 625: val loss: 4.398001670837402\n",
      "Step 626: train loss: 4.4640960693359375\n",
      "Step 626: val loss: 4.3900227546691895\n",
      "Step 627: train loss: 4.456019878387451\n",
      "Step 627: val loss: 4.3820624351501465\n",
      "Step 628: train loss: 4.447962284088135\n",
      "Step 628: val loss: 4.374119281768799\n",
      "Step 629: train loss: 4.4399213790893555\n",
      "Step 629: val loss: 4.366191387176514\n",
      "Step 630: train loss: 4.4318976402282715\n",
      "Step 630: val loss: 4.35828161239624\n",
      "Step 631: train loss: 4.423890590667725\n",
      "Step 631: val loss: 4.350387096405029\n",
      "Step 632: train loss: 4.415900707244873\n",
      "Step 632: val loss: 4.342510223388672\n",
      "Step 633: train loss: 4.407928943634033\n",
      "Step 633: val loss: 4.334649562835693\n",
      "Step 634: train loss: 4.399972915649414\n",
      "Step 634: val loss: 4.32680606842041\n",
      "Step 635: train loss: 4.392034530639648\n",
      "Step 635: val loss: 4.318977355957031\n",
      "Step 636: train loss: 4.384113311767578\n",
      "Step 636: val loss: 4.311166763305664\n",
      "Step 637: train loss: 4.376208782196045\n",
      "Step 637: val loss: 4.303371906280518\n",
      "Step 638: train loss: 4.368321418762207\n",
      "Step 638: val loss: 4.295592784881592\n",
      "Step 639: train loss: 4.360450267791748\n",
      "Step 639: val loss: 4.287830352783203\n",
      "Step 640: train loss: 4.352596282958984\n",
      "Step 640: val loss: 4.28008508682251\n",
      "Step 641: train loss: 4.344759464263916\n",
      "Step 641: val loss: 4.272354602813721\n",
      "Step 642: train loss: 4.33693790435791\n",
      "Step 642: val loss: 4.264640808105469\n",
      "Step 643: train loss: 4.329134464263916\n",
      "Step 643: val loss: 4.256942272186279\n",
      "Step 644: train loss: 4.321346759796143\n",
      "Step 644: val loss: 4.249261379241943\n",
      "Step 645: train loss: 4.313575744628906\n",
      "Step 645: val loss: 4.2415947914123535\n",
      "Step 646: train loss: 4.305820465087891\n",
      "Step 646: val loss: 4.233945369720459\n",
      "Step 647: train loss: 4.2980828285217285\n",
      "Step 647: val loss: 4.226311206817627\n",
      "Step 648: train loss: 4.290360450744629\n",
      "Step 648: val loss: 4.218693256378174\n",
      "Step 649: train loss: 4.282655239105225\n",
      "Step 649: val loss: 4.211090564727783\n",
      "Step 650: train loss: 4.274965763092041\n",
      "Step 650: val loss: 4.2035040855407715\n",
      "Step 651: train loss: 4.267292499542236\n",
      "Step 651: val loss: 4.195933818817139\n",
      "Step 652: train loss: 4.259635925292969\n",
      "Step 652: val loss: 4.188378810882568\n",
      "Step 653: train loss: 4.25199556350708\n",
      "Step 653: val loss: 4.1808390617370605\n",
      "Step 654: train loss: 4.244370460510254\n",
      "Step 654: val loss: 4.173315048217773\n",
      "Step 655: train loss: 4.236761569976807\n",
      "Step 655: val loss: 4.165806293487549\n",
      "Step 656: train loss: 4.2291693687438965\n",
      "Step 656: val loss: 4.158313751220703\n",
      "Step 657: train loss: 4.221592426300049\n",
      "Step 657: val loss: 4.150835990905762\n",
      "Step 658: train loss: 4.214031219482422\n",
      "Step 658: val loss: 4.143373966217041\n",
      "Step 659: train loss: 4.206486225128174\n",
      "Step 659: val loss: 4.135927677154541\n",
      "Step 660: train loss: 4.198957443237305\n",
      "Step 660: val loss: 4.128496170043945\n",
      "Step 661: train loss: 4.191443920135498\n",
      "Step 661: val loss: 4.121079921722412\n",
      "Step 662: train loss: 4.183946132659912\n",
      "Step 662: val loss: 4.1136794090271\n",
      "Step 663: train loss: 4.176464557647705\n",
      "Step 663: val loss: 4.106293678283691\n",
      "Step 664: train loss: 4.168997764587402\n",
      "Step 664: val loss: 4.0989227294921875\n",
      "Step 665: train loss: 4.161546230316162\n",
      "Step 665: val loss: 4.0915679931640625\n",
      "Step 666: train loss: 4.154111385345459\n",
      "Step 666: val loss: 4.084227561950684\n",
      "Step 667: train loss: 4.14669132232666\n",
      "Step 667: val loss: 4.076902389526367\n",
      "Step 668: train loss: 4.139286994934082\n",
      "Step 668: val loss: 4.069591999053955\n",
      "Step 669: train loss: 4.131897926330566\n",
      "Step 669: val loss: 4.062295913696289\n",
      "Step 670: train loss: 4.1245245933532715\n",
      "Step 670: val loss: 4.05501651763916\n",
      "Step 671: train loss: 4.117166042327881\n",
      "Step 671: val loss: 4.047750949859619\n",
      "Step 672: train loss: 4.109822750091553\n",
      "Step 672: val loss: 4.040500164031982\n",
      "Step 673: train loss: 4.102495193481445\n",
      "Step 673: val loss: 4.03326416015625\n",
      "Step 674: train loss: 4.095181941986084\n",
      "Step 674: val loss: 4.02604341506958\n",
      "Step 675: train loss: 4.08788537979126\n",
      "Step 675: val loss: 4.018836498260498\n",
      "Step 676: train loss: 4.080602645874023\n",
      "Step 676: val loss: 4.0116448402404785\n",
      "Step 677: train loss: 4.07333517074585\n",
      "Step 677: val loss: 4.004467964172363\n",
      "Step 678: train loss: 4.066082954406738\n",
      "Step 678: val loss: 3.997305393218994\n",
      "Step 679: train loss: 4.058845043182373\n",
      "Step 679: val loss: 3.99015736579895\n",
      "Step 680: train loss: 4.0516228675842285\n",
      "Step 680: val loss: 3.9830236434936523\n",
      "Step 681: train loss: 4.044415473937988\n",
      "Step 681: val loss: 3.975905179977417\n",
      "Step 682: train loss: 4.037222862243652\n",
      "Step 682: val loss: 3.9688000679016113\n",
      "Step 683: train loss: 4.0300445556640625\n",
      "Step 683: val loss: 3.961710214614868\n",
      "Step 684: train loss: 4.022881507873535\n",
      "Step 684: val loss: 3.9546337127685547\n",
      "Step 685: train loss: 4.015732765197754\n",
      "Step 685: val loss: 3.947572708129883\n",
      "Step 686: train loss: 4.008599758148193\n",
      "Step 686: val loss: 3.940526008605957\n",
      "Step 687: train loss: 4.001481056213379\n",
      "Step 687: val loss: 3.933493137359619\n",
      "Step 688: train loss: 3.9943766593933105\n",
      "Step 688: val loss: 3.9264743328094482\n",
      "Step 689: train loss: 3.9872865676879883\n",
      "Step 689: val loss: 3.9194693565368652\n",
      "Step 690: train loss: 3.980210542678833\n",
      "Step 690: val loss: 3.9124786853790283\n",
      "Step 691: train loss: 3.9731500148773193\n",
      "Step 691: val loss: 3.9055025577545166\n",
      "Step 692: train loss: 3.96610426902771\n",
      "Step 692: val loss: 3.89854097366333\n",
      "Step 693: train loss: 3.9590721130371094\n",
      "Step 693: val loss: 3.8915929794311523\n",
      "Step 694: train loss: 3.952054500579834\n",
      "Step 694: val loss: 3.8846590518951416\n",
      "Step 695: train loss: 3.945051908493042\n",
      "Step 695: val loss: 3.877739191055298\n",
      "Step 696: train loss: 3.938062906265259\n",
      "Step 696: val loss: 3.8708324432373047\n",
      "Step 697: train loss: 3.9310877323150635\n",
      "Step 697: val loss: 3.8639402389526367\n",
      "Step 698: train loss: 3.9241275787353516\n",
      "Step 698: val loss: 3.8570621013641357\n",
      "Step 699: train loss: 3.9171812534332275\n",
      "Step 699: val loss: 3.850196123123169\n",
      "Step 700: train loss: 3.9102485179901123\n",
      "Step 700: val loss: 3.8433451652526855\n",
      "Step 701: train loss: 3.9033305644989014\n",
      "Step 701: val loss: 3.836508274078369\n",
      "Step 702: train loss: 3.896426200866699\n",
      "Step 702: val loss: 3.829683542251587\n",
      "Step 703: train loss: 3.889535903930664\n",
      "Step 703: val loss: 3.8228745460510254\n",
      "Step 704: train loss: 3.882660388946533\n",
      "Step 704: val loss: 3.8160786628723145\n",
      "Step 705: train loss: 3.8757994174957275\n",
      "Step 705: val loss: 3.809296131134033\n",
      "Step 706: train loss: 3.8689513206481934\n",
      "Step 706: val loss: 3.8025269508361816\n",
      "Step 707: train loss: 3.8621163368225098\n",
      "Step 707: val loss: 3.7957708835601807\n",
      "Step 708: train loss: 3.8552961349487305\n",
      "Step 708: val loss: 3.789029121398926\n",
      "Step 709: train loss: 3.8484890460968018\n",
      "Step 709: val loss: 3.7822999954223633\n",
      "Step 710: train loss: 3.841696262359619\n",
      "Step 710: val loss: 3.7755849361419678\n",
      "Step 711: train loss: 3.8349180221557617\n",
      "Step 711: val loss: 3.768883466720581\n",
      "Step 712: train loss: 3.8281521797180176\n",
      "Step 712: val loss: 3.762195110321045\n",
      "Step 713: train loss: 3.821401357650757\n",
      "Step 713: val loss: 3.7555196285247803\n",
      "Step 714: train loss: 3.8146631717681885\n",
      "Step 714: val loss: 3.7488584518432617\n",
      "Step 715: train loss: 3.807939052581787\n",
      "Step 715: val loss: 3.7422099113464355\n",
      "Step 716: train loss: 3.8012282848358154\n",
      "Step 716: val loss: 3.73557448387146\n",
      "Step 717: train loss: 3.7945311069488525\n",
      "Step 717: val loss: 3.728952646255493\n",
      "Step 718: train loss: 3.7878482341766357\n",
      "Step 718: val loss: 3.7223451137542725\n",
      "Step 719: train loss: 3.7811784744262695\n",
      "Step 719: val loss: 3.7157490253448486\n",
      "Step 720: train loss: 3.7745213508605957\n",
      "Step 720: val loss: 3.7091665267944336\n",
      "Step 721: train loss: 3.7678775787353516\n",
      "Step 721: val loss: 3.702597141265869\n",
      "Step 722: train loss: 3.7612476348876953\n",
      "Step 722: val loss: 3.6960411071777344\n",
      "Step 723: train loss: 3.754631280899048\n",
      "Step 723: val loss: 3.689497947692871\n",
      "Step 724: train loss: 3.748028039932251\n",
      "Step 724: val loss: 3.6829674243927\n",
      "Step 725: train loss: 3.741438150405884\n",
      "Step 725: val loss: 3.676450252532959\n",
      "Step 726: train loss: 3.734861135482788\n",
      "Step 726: val loss: 3.669945240020752\n",
      "Step 727: train loss: 3.728297710418701\n",
      "Step 727: val loss: 3.663454294204712\n",
      "Step 728: train loss: 3.721747636795044\n",
      "Step 728: val loss: 3.656975507736206\n",
      "Step 729: train loss: 3.7152099609375\n",
      "Step 729: val loss: 3.6505095958709717\n",
      "Step 730: train loss: 3.7086856365203857\n",
      "Step 730: val loss: 3.644056558609009\n",
      "Step 731: train loss: 3.702174663543701\n",
      "Step 731: val loss: 3.637615919113159\n",
      "Step 732: train loss: 3.695676326751709\n",
      "Step 732: val loss: 3.631188154220581\n",
      "Step 733: train loss: 3.6891911029815674\n",
      "Step 733: val loss: 3.6247735023498535\n",
      "Step 734: train loss: 3.6827189922332764\n",
      "Step 734: val loss: 3.6183714866638184\n",
      "Step 735: train loss: 3.676259994506836\n",
      "Step 735: val loss: 3.6119823455810547\n",
      "Step 736: train loss: 3.669813632965088\n",
      "Step 736: val loss: 3.605604887008667\n",
      "Step 737: train loss: 3.6633801460266113\n",
      "Step 737: val loss: 3.599240779876709\n",
      "Step 738: train loss: 3.6569595336914062\n",
      "Step 738: val loss: 3.592888116836548\n",
      "Step 739: train loss: 3.6505515575408936\n",
      "Step 739: val loss: 3.586548328399658\n",
      "Step 740: train loss: 3.6441562175750732\n",
      "Step 740: val loss: 3.580221652984619\n",
      "Step 741: train loss: 3.6377739906311035\n",
      "Step 741: val loss: 3.5739076137542725\n",
      "Step 742: train loss: 3.6314048767089844\n",
      "Step 742: val loss: 3.567605972290039\n",
      "Step 743: train loss: 3.625047445297241\n",
      "Step 743: val loss: 3.5613160133361816\n",
      "Step 744: train loss: 3.6187031269073486\n",
      "Step 744: val loss: 3.555039405822754\n",
      "Step 745: train loss: 3.6123721599578857\n",
      "Step 745: val loss: 3.548774003982544\n",
      "Step 746: train loss: 3.6060526371002197\n",
      "Step 746: val loss: 3.5425214767456055\n",
      "Step 747: train loss: 3.5997462272644043\n",
      "Step 747: val loss: 3.5362813472747803\n",
      "Step 748: train loss: 3.5934524536132812\n",
      "Step 748: val loss: 3.530052900314331\n",
      "Step 749: train loss: 3.5871706008911133\n",
      "Step 749: val loss: 3.523837089538574\n",
      "Step 750: train loss: 3.580902099609375\n",
      "Step 750: val loss: 3.5176339149475098\n",
      "Step 751: train loss: 3.5746452808380127\n",
      "Step 751: val loss: 3.5114428997039795\n",
      "Step 752: train loss: 3.568401575088501\n",
      "Step 752: val loss: 3.505263328552246\n",
      "Step 753: train loss: 3.5621697902679443\n",
      "Step 753: val loss: 3.499096155166626\n",
      "Step 754: train loss: 3.555950403213501\n",
      "Step 754: val loss: 3.49294114112854\n",
      "Step 755: train loss: 3.5497426986694336\n",
      "Step 755: val loss: 3.4867982864379883\n",
      "Step 756: train loss: 3.543548107147217\n",
      "Step 756: val loss: 3.4806675910949707\n",
      "Step 757: train loss: 3.537365674972534\n",
      "Step 757: val loss: 3.474548578262329\n",
      "Step 758: train loss: 3.5311954021453857\n",
      "Step 758: val loss: 3.4684417247772217\n",
      "Step 759: train loss: 3.5250370502471924\n",
      "Step 759: val loss: 3.4623467922210693\n",
      "Step 760: train loss: 3.5188910961151123\n",
      "Step 760: val loss: 3.456263542175293\n",
      "Step 761: train loss: 3.5127573013305664\n",
      "Step 761: val loss: 3.45019268989563\n",
      "Step 762: train loss: 3.506636142730713\n",
      "Step 762: val loss: 3.4441332817077637\n",
      "Step 763: train loss: 3.5005264282226562\n",
      "Step 763: val loss: 3.4380857944488525\n",
      "Step 764: train loss: 3.4944286346435547\n",
      "Step 764: val loss: 3.4320497512817383\n",
      "Step 765: train loss: 3.4883434772491455\n",
      "Step 765: val loss: 3.4260261058807373\n",
      "Step 766: train loss: 3.482269048690796\n",
      "Step 766: val loss: 3.420013427734375\n",
      "Step 767: train loss: 3.476207733154297\n",
      "Step 767: val loss: 3.414012908935547\n",
      "Step 768: train loss: 3.4701578617095947\n",
      "Step 768: val loss: 3.4080238342285156\n",
      "Step 769: train loss: 3.4641199111938477\n",
      "Step 769: val loss: 3.4020466804504395\n",
      "Step 770: train loss: 3.4580941200256348\n",
      "Step 770: val loss: 3.3960812091827393\n",
      "Step 771: train loss: 3.452080011367798\n",
      "Step 771: val loss: 3.390127182006836\n",
      "Step 772: train loss: 3.446077585220337\n",
      "Step 772: val loss: 3.384185314178467\n",
      "Step 773: train loss: 3.440086841583252\n",
      "Step 773: val loss: 3.378253936767578\n",
      "Step 774: train loss: 3.434108257293701\n",
      "Step 774: val loss: 3.372335195541382\n",
      "Step 775: train loss: 3.4281415939331055\n",
      "Step 775: val loss: 3.366427421569824\n",
      "Step 776: train loss: 3.4221863746643066\n",
      "Step 776: val loss: 3.3605313301086426\n",
      "Step 777: train loss: 3.4162423610687256\n",
      "Step 777: val loss: 3.3546459674835205\n",
      "Step 778: train loss: 3.4103102684020996\n",
      "Step 778: val loss: 3.34877347946167\n",
      "Step 779: train loss: 3.4043900966644287\n",
      "Step 779: val loss: 3.3429112434387207\n",
      "Step 780: train loss: 3.398481845855713\n",
      "Step 780: val loss: 3.3370611667633057\n",
      "Step 781: train loss: 3.3925845623016357\n",
      "Step 781: val loss: 3.33122181892395\n",
      "Step 782: train loss: 3.3866994380950928\n",
      "Step 782: val loss: 3.3253939151763916\n",
      "Step 783: train loss: 3.3808250427246094\n",
      "Step 783: val loss: 3.3195769786834717\n",
      "Step 784: train loss: 3.374962329864502\n",
      "Step 784: val loss: 3.313771963119507\n",
      "Step 785: train loss: 3.3691117763519287\n",
      "Step 785: val loss: 3.3079779148101807\n",
      "Step 786: train loss: 3.363271951675415\n",
      "Step 786: val loss: 3.3021953105926514\n",
      "Step 787: train loss: 3.3574435710906982\n",
      "Step 787: val loss: 3.296424150466919\n",
      "Step 788: train loss: 3.3516271114349365\n",
      "Step 788: val loss: 3.290663719177246\n",
      "Step 789: train loss: 3.3458216190338135\n",
      "Step 789: val loss: 3.284914255142212\n",
      "Step 790: train loss: 3.3400275707244873\n",
      "Step 790: val loss: 3.2791759967803955\n",
      "Step 791: train loss: 3.3342444896698\n",
      "Step 791: val loss: 3.273449420928955\n",
      "Step 792: train loss: 3.3284730911254883\n",
      "Step 792: val loss: 3.267733573913574\n",
      "Step 793: train loss: 3.3227128982543945\n",
      "Step 793: val loss: 3.2620291709899902\n",
      "Step 794: train loss: 3.3169643878936768\n",
      "Step 794: val loss: 3.256335735321045\n",
      "Step 795: train loss: 3.3112266063690186\n",
      "Step 795: val loss: 3.2506520748138428\n",
      "Step 796: train loss: 3.30549955368042\n",
      "Step 796: val loss: 3.244980573654175\n",
      "Step 797: train loss: 3.299783706665039\n",
      "Step 797: val loss: 3.2393198013305664\n",
      "Step 798: train loss: 3.2940797805786133\n",
      "Step 798: val loss: 3.233670473098755\n",
      "Step 799: train loss: 3.288386344909668\n",
      "Step 799: val loss: 3.2280309200286865\n",
      "Step 800: train loss: 3.2827045917510986\n",
      "Step 800: val loss: 3.222403049468994\n",
      "Step 801: train loss: 3.2770333290100098\n",
      "Step 801: val loss: 3.2167859077453613\n",
      "Step 802: train loss: 3.2713735103607178\n",
      "Step 802: val loss: 3.2111802101135254\n",
      "Step 803: train loss: 3.2657244205474854\n",
      "Step 803: val loss: 3.2055840492248535\n",
      "Step 804: train loss: 3.26008677482605\n",
      "Step 804: val loss: 3.200000047683716\n",
      "Step 805: train loss: 3.2544593811035156\n",
      "Step 805: val loss: 3.194425344467163\n",
      "Step 806: train loss: 3.2488434314727783\n",
      "Step 806: val loss: 3.1888625621795654\n",
      "Step 807: train loss: 3.2432379722595215\n",
      "Step 807: val loss: 3.183310031890869\n",
      "Step 808: train loss: 3.2376439571380615\n",
      "Step 808: val loss: 3.1777682304382324\n",
      "Step 809: train loss: 3.232060194015503\n",
      "Step 809: val loss: 3.172236919403076\n",
      "Step 810: train loss: 3.226487398147583\n",
      "Step 810: val loss: 3.1667165756225586\n",
      "Step 811: train loss: 3.2209255695343018\n",
      "Step 811: val loss: 3.1612069606781006\n",
      "Step 812: train loss: 3.21537446975708\n",
      "Step 812: val loss: 3.155708074569702\n",
      "Step 813: train loss: 3.209834098815918\n",
      "Step 813: val loss: 3.150218963623047\n",
      "Step 814: train loss: 3.2043044567108154\n",
      "Step 814: val loss: 3.1447412967681885\n",
      "Step 815: train loss: 3.1987853050231934\n",
      "Step 815: val loss: 3.139272928237915\n",
      "Step 816: train loss: 3.193276882171631\n",
      "Step 816: val loss: 3.133816719055176\n",
      "Step 817: train loss: 3.1877799034118652\n",
      "Step 817: val loss: 3.1283702850341797\n",
      "Step 818: train loss: 3.182293176651001\n",
      "Step 818: val loss: 3.122934341430664\n",
      "Step 819: train loss: 3.1768171787261963\n",
      "Step 819: val loss: 3.117508888244629\n",
      "Step 820: train loss: 3.171351671218872\n",
      "Step 820: val loss: 3.112093448638916\n",
      "Step 821: train loss: 3.165896415710449\n",
      "Step 821: val loss: 3.106688976287842\n",
      "Step 822: train loss: 3.160451650619507\n",
      "Step 822: val loss: 3.1012942790985107\n",
      "Step 823: train loss: 3.155017614364624\n",
      "Step 823: val loss: 3.095909595489502\n",
      "Step 824: train loss: 3.1495935916900635\n",
      "Step 824: val loss: 3.090535879135132\n",
      "Step 825: train loss: 3.1441807746887207\n",
      "Step 825: val loss: 3.085172653198242\n",
      "Step 826: train loss: 3.138777732849121\n",
      "Step 826: val loss: 3.079819679260254\n",
      "Step 827: train loss: 3.13338565826416\n",
      "Step 827: val loss: 3.074477195739746\n",
      "Step 828: train loss: 3.128004312515259\n",
      "Step 828: val loss: 3.0691442489624023\n",
      "Step 829: train loss: 3.1226325035095215\n",
      "Step 829: val loss: 3.063821792602539\n",
      "Step 830: train loss: 3.1172711849212646\n",
      "Step 830: val loss: 3.0585098266601562\n",
      "Step 831: train loss: 3.1119205951690674\n",
      "Step 831: val loss: 3.0532076358795166\n",
      "Step 832: train loss: 3.1065800189971924\n",
      "Step 832: val loss: 3.0479161739349365\n",
      "Step 833: train loss: 3.101250171661377\n",
      "Step 833: val loss: 3.0426344871520996\n",
      "Step 834: train loss: 3.0959300994873047\n",
      "Step 834: val loss: 3.0373620986938477\n",
      "Step 835: train loss: 3.0906200408935547\n",
      "Step 835: val loss: 3.0321009159088135\n",
      "Step 836: train loss: 3.0853209495544434\n",
      "Step 836: val loss: 3.0268495082855225\n",
      "Step 837: train loss: 3.080031394958496\n",
      "Step 837: val loss: 3.0216078758239746\n",
      "Step 838: train loss: 3.0747530460357666\n",
      "Step 838: val loss: 3.0163767337799072\n",
      "Step 839: train loss: 3.069483995437622\n",
      "Step 839: val loss: 3.011155843734741\n",
      "Step 840: train loss: 3.064225673675537\n",
      "Step 840: val loss: 3.00594425201416\n",
      "Step 841: train loss: 3.058976888656616\n",
      "Step 841: val loss: 3.0007431507110596\n",
      "Step 842: train loss: 3.053738594055176\n",
      "Step 842: val loss: 2.9955520629882812\n",
      "Step 843: train loss: 3.0485100746154785\n",
      "Step 843: val loss: 2.990370035171509\n",
      "Step 844: train loss: 3.0432918071746826\n",
      "Step 844: val loss: 2.985198497772217\n",
      "Step 845: train loss: 3.03808331489563\n",
      "Step 845: val loss: 2.980037212371826\n",
      "Step 846: train loss: 3.0328848361968994\n",
      "Step 846: val loss: 2.9748849868774414\n",
      "Step 847: train loss: 3.0276966094970703\n",
      "Step 847: val loss: 2.9697437286376953\n",
      "Step 848: train loss: 3.0225179195404053\n",
      "Step 848: val loss: 2.9646105766296387\n",
      "Step 849: train loss: 3.0173490047454834\n",
      "Step 849: val loss: 2.9594886302948\n",
      "Step 850: train loss: 3.012190818786621\n",
      "Step 850: val loss: 2.954376220703125\n",
      "Step 851: train loss: 3.007042407989502\n",
      "Step 851: val loss: 2.949272871017456\n",
      "Step 852: train loss: 3.001903533935547\n",
      "Step 852: val loss: 2.9441802501678467\n",
      "Step 853: train loss: 2.996774673461914\n",
      "Step 853: val loss: 2.9390969276428223\n",
      "Step 854: train loss: 2.9916558265686035\n",
      "Step 854: val loss: 2.934023141860962\n",
      "Step 855: train loss: 2.986546039581299\n",
      "Step 855: val loss: 2.9289588928222656\n",
      "Step 856: train loss: 2.9814462661743164\n",
      "Step 856: val loss: 2.9239046573638916\n",
      "Step 857: train loss: 2.9763567447662354\n",
      "Step 857: val loss: 2.9188599586486816\n",
      "Step 858: train loss: 2.97127628326416\n",
      "Step 858: val loss: 2.9138247966766357\n",
      "Step 859: train loss: 2.9662063121795654\n",
      "Step 859: val loss: 2.908799171447754\n",
      "Step 860: train loss: 2.9611454010009766\n",
      "Step 860: val loss: 2.9037833213806152\n",
      "Step 861: train loss: 2.956094741821289\n",
      "Step 861: val loss: 2.8987767696380615\n",
      "Step 862: train loss: 2.9510533809661865\n",
      "Step 862: val loss: 2.89378023147583\n",
      "Step 863: train loss: 2.9460220336914062\n",
      "Step 863: val loss: 2.8887927532196045\n",
      "Step 864: train loss: 2.94100022315979\n",
      "Step 864: val loss: 2.883815050125122\n",
      "Step 865: train loss: 2.9359874725341797\n",
      "Step 865: val loss: 2.8788466453552246\n",
      "Step 866: train loss: 2.9309849739074707\n",
      "Step 866: val loss: 2.8738882541656494\n",
      "Step 867: train loss: 2.9259917736053467\n",
      "Step 867: val loss: 2.86893892288208\n",
      "Step 868: train loss: 2.9210078716278076\n",
      "Step 868: val loss: 2.8639986515045166\n",
      "Step 869: train loss: 2.916033983230591\n",
      "Step 869: val loss: 2.8590681552886963\n",
      "Step 870: train loss: 2.911068916320801\n",
      "Step 870: val loss: 2.854146957397461\n",
      "Step 871: train loss: 2.906114101409912\n",
      "Step 871: val loss: 2.8492350578308105\n",
      "Step 872: train loss: 2.90116810798645\n",
      "Step 872: val loss: 2.844332456588745\n",
      "Step 873: train loss: 2.8962318897247314\n",
      "Step 873: val loss: 2.8394393920898438\n",
      "Step 874: train loss: 2.891305446624756\n",
      "Step 874: val loss: 2.8345556259155273\n",
      "Step 875: train loss: 2.8863883018493652\n",
      "Step 875: val loss: 2.829680919647217\n",
      "Step 876: train loss: 2.8814799785614014\n",
      "Step 876: val loss: 2.8248157501220703\n",
      "Step 877: train loss: 2.8765807151794434\n",
      "Step 877: val loss: 2.8199596405029297\n",
      "Step 878: train loss: 2.871692180633545\n",
      "Step 878: val loss: 2.815113067626953\n",
      "Step 879: train loss: 2.866811513900757\n",
      "Step 879: val loss: 2.8102753162384033\n",
      "Step 880: train loss: 2.86194109916687\n",
      "Step 880: val loss: 2.8054468631744385\n",
      "Step 881: train loss: 2.85707950592041\n",
      "Step 881: val loss: 2.8006277084350586\n",
      "Step 882: train loss: 2.852226972579956\n",
      "Step 882: val loss: 2.7958173751831055\n",
      "Step 883: train loss: 2.847384214401245\n",
      "Step 883: val loss: 2.7910163402557373\n",
      "Step 884: train loss: 2.842550277709961\n",
      "Step 884: val loss: 2.786224603652954\n",
      "Step 885: train loss: 2.83772611618042\n",
      "Step 885: val loss: 2.781442165374756\n",
      "Step 886: train loss: 2.8329105377197266\n",
      "Step 886: val loss: 2.7766685485839844\n",
      "Step 887: train loss: 2.8281047344207764\n",
      "Step 887: val loss: 2.7719037532806396\n",
      "Step 888: train loss: 2.823307514190674\n",
      "Step 888: val loss: 2.767148494720459\n",
      "Step 889: train loss: 2.8185203075408936\n",
      "Step 889: val loss: 2.762402296066284\n",
      "Step 890: train loss: 2.8137412071228027\n",
      "Step 890: val loss: 2.7576639652252197\n",
      "Step 891: train loss: 2.808971643447876\n",
      "Step 891: val loss: 2.7529361248016357\n",
      "Step 892: train loss: 2.804211139678955\n",
      "Step 892: val loss: 2.748216152191162\n",
      "Step 893: train loss: 2.799459218978882\n",
      "Step 893: val loss: 2.7435061931610107\n",
      "Step 894: train loss: 2.7947168350219727\n",
      "Step 894: val loss: 2.738804340362549\n",
      "Step 895: train loss: 2.7899835109710693\n",
      "Step 895: val loss: 2.7341113090515137\n",
      "Step 896: train loss: 2.7852590084075928\n",
      "Step 896: val loss: 2.7294278144836426\n",
      "Step 897: train loss: 2.7805426120758057\n",
      "Step 897: val loss: 2.724752187728882\n",
      "Step 898: train loss: 2.77583646774292\n",
      "Step 898: val loss: 2.720085859298706\n",
      "Step 899: train loss: 2.7711386680603027\n",
      "Step 899: val loss: 2.7154290676116943\n",
      "Step 900: train loss: 2.7664504051208496\n",
      "Step 900: val loss: 2.7107810974121094\n",
      "Step 901: train loss: 2.761770486831665\n",
      "Step 901: val loss: 2.7061405181884766\n",
      "Step 902: train loss: 2.757099151611328\n",
      "Step 902: val loss: 2.7015092372894287\n",
      "Step 903: train loss: 2.752437114715576\n",
      "Step 903: val loss: 2.6968882083892822\n",
      "Step 904: train loss: 2.74778413772583\n",
      "Step 904: val loss: 2.692274332046509\n",
      "Step 905: train loss: 2.7431397438049316\n",
      "Step 905: val loss: 2.6876699924468994\n",
      "Step 906: train loss: 2.7385034561157227\n",
      "Step 906: val loss: 2.6830742359161377\n",
      "Step 907: train loss: 2.733877182006836\n",
      "Step 907: val loss: 2.6784870624542236\n",
      "Step 908: train loss: 2.7292592525482178\n",
      "Step 908: val loss: 2.6739084720611572\n",
      "Step 909: train loss: 2.7246499061584473\n",
      "Step 909: val loss: 2.669339418411255\n",
      "Step 910: train loss: 2.7200498580932617\n",
      "Step 910: val loss: 2.6647775173187256\n",
      "Step 911: train loss: 2.7154581546783447\n",
      "Step 911: val loss: 2.6602251529693604\n",
      "Step 912: train loss: 2.7108747959136963\n",
      "Step 912: val loss: 2.6556813716888428\n",
      "Step 913: train loss: 2.7063000202178955\n",
      "Step 913: val loss: 2.6511452198028564\n",
      "Step 914: train loss: 2.7017340660095215\n",
      "Step 914: val loss: 2.6466195583343506\n",
      "Step 915: train loss: 2.6971774101257324\n",
      "Step 915: val loss: 2.6421008110046387\n",
      "Step 916: train loss: 2.692628860473633\n",
      "Step 916: val loss: 2.63759183883667\n",
      "Step 917: train loss: 2.688089370727539\n",
      "Step 917: val loss: 2.6330904960632324\n",
      "Step 918: train loss: 2.6835579872131348\n",
      "Step 918: val loss: 2.6285977363586426\n",
      "Step 919: train loss: 2.679035186767578\n",
      "Step 919: val loss: 2.6241140365600586\n",
      "Step 920: train loss: 2.6745216846466064\n",
      "Step 920: val loss: 2.619638442993164\n",
      "Step 921: train loss: 2.670016050338745\n",
      "Step 921: val loss: 2.615171194076538\n",
      "Step 922: train loss: 2.6655187606811523\n",
      "Step 922: val loss: 2.6107120513916016\n",
      "Step 923: train loss: 2.6610300540924072\n",
      "Step 923: val loss: 2.606261968612671\n",
      "Step 924: train loss: 2.6565499305725098\n",
      "Step 924: val loss: 2.6018199920654297\n",
      "Step 925: train loss: 2.6520779132843018\n",
      "Step 925: val loss: 2.597386121749878\n",
      "Step 926: train loss: 2.6476147174835205\n",
      "Step 926: val loss: 2.592961311340332\n",
      "Step 927: train loss: 2.643160104751587\n",
      "Step 927: val loss: 2.5885438919067383\n",
      "Step 928: train loss: 2.638713836669922\n",
      "Step 928: val loss: 2.5841355323791504\n",
      "Step 929: train loss: 2.634275436401367\n",
      "Step 929: val loss: 2.5797348022460938\n",
      "Step 930: train loss: 2.6298460960388184\n",
      "Step 930: val loss: 2.575343608856201\n",
      "Step 931: train loss: 2.625424861907959\n",
      "Step 931: val loss: 2.5709598064422607\n",
      "Step 932: train loss: 2.621011972427368\n",
      "Step 932: val loss: 2.5665838718414307\n",
      "Step 933: train loss: 2.6166069507598877\n",
      "Step 933: val loss: 2.5622172355651855\n",
      "Step 934: train loss: 2.612210988998413\n",
      "Step 934: val loss: 2.557858467102051\n",
      "Step 935: train loss: 2.607822895050049\n",
      "Step 935: val loss: 2.5535078048706055\n",
      "Step 936: train loss: 2.6034436225891113\n",
      "Step 936: val loss: 2.549165725708008\n",
      "Step 937: train loss: 2.5990724563598633\n",
      "Step 937: val loss: 2.5448312759399414\n",
      "Step 938: train loss: 2.5947089195251465\n",
      "Step 938: val loss: 2.54050612449646\n",
      "Step 939: train loss: 2.5903546810150146\n",
      "Step 939: val loss: 2.5361878871917725\n",
      "Step 940: train loss: 2.586008071899414\n",
      "Step 940: val loss: 2.5318784713745117\n",
      "Step 941: train loss: 2.581669330596924\n",
      "Step 941: val loss: 2.5275769233703613\n",
      "Step 942: train loss: 2.5773398876190186\n",
      "Step 942: val loss: 2.523284435272217\n",
      "Step 943: train loss: 2.5730178356170654\n",
      "Step 943: val loss: 2.518998622894287\n",
      "Step 944: train loss: 2.5687038898468018\n",
      "Step 944: val loss: 2.514721393585205\n",
      "Step 945: train loss: 2.5643980503082275\n",
      "Step 945: val loss: 2.5104520320892334\n",
      "Step 946: train loss: 2.5601003170013428\n",
      "Step 946: val loss: 2.5061917304992676\n",
      "Step 947: train loss: 2.5558111667633057\n",
      "Step 947: val loss: 2.5019381046295166\n",
      "Step 948: train loss: 2.5515296459198\n",
      "Step 948: val loss: 2.4976935386657715\n",
      "Step 949: train loss: 2.5472559928894043\n",
      "Step 949: val loss: 2.4934566020965576\n",
      "Step 950: train loss: 2.5429911613464355\n",
      "Step 950: val loss: 2.4892280101776123\n",
      "Step 951: train loss: 2.538734197616577\n",
      "Step 951: val loss: 2.48500657081604\n",
      "Step 952: train loss: 2.53448486328125\n",
      "Step 952: val loss: 2.4807939529418945\n",
      "Step 953: train loss: 2.5302436351776123\n",
      "Step 953: val loss: 2.476588726043701\n",
      "Step 954: train loss: 2.526010513305664\n",
      "Step 954: val loss: 2.472391366958618\n",
      "Step 955: train loss: 2.5217854976654053\n",
      "Step 955: val loss: 2.468202829360962\n",
      "Step 956: train loss: 2.5175681114196777\n",
      "Step 956: val loss: 2.4640207290649414\n",
      "Step 957: train loss: 2.5133585929870605\n",
      "Step 957: val loss: 2.4598476886749268\n",
      "Step 958: train loss: 2.509157419204712\n",
      "Step 958: val loss: 2.455681800842285\n",
      "Step 959: train loss: 2.5049638748168945\n",
      "Step 959: val loss: 2.4515249729156494\n",
      "Step 960: train loss: 2.5007784366607666\n",
      "Step 960: val loss: 2.447374105453491\n",
      "Step 961: train loss: 2.496600389480591\n",
      "Step 961: val loss: 2.4432318210601807\n",
      "Step 962: train loss: 2.4924299716949463\n",
      "Step 962: val loss: 2.4390978813171387\n",
      "Step 963: train loss: 2.4882688522338867\n",
      "Step 963: val loss: 2.434971332550049\n",
      "Step 964: train loss: 2.484114408493042\n",
      "Step 964: val loss: 2.4308526515960693\n",
      "Step 965: train loss: 2.4799680709838867\n",
      "Step 965: val loss: 2.426741361618042\n",
      "Step 966: train loss: 2.4758291244506836\n",
      "Step 966: val loss: 2.422638416290283\n",
      "Step 967: train loss: 2.471698522567749\n",
      "Step 967: val loss: 2.4185428619384766\n",
      "Step 968: train loss: 2.467576026916504\n",
      "Step 968: val loss: 2.414454698562622\n",
      "Step 969: train loss: 2.463460683822632\n",
      "Step 969: val loss: 2.4103753566741943\n",
      "Step 970: train loss: 2.45935320854187\n",
      "Step 970: val loss: 2.4063024520874023\n",
      "Step 971: train loss: 2.4552533626556396\n",
      "Step 971: val loss: 2.402237892150879\n",
      "Step 972: train loss: 2.4511613845825195\n",
      "Step 972: val loss: 2.3981809616088867\n",
      "Step 973: train loss: 2.4470765590667725\n",
      "Step 973: val loss: 2.3941314220428467\n",
      "Step 974: train loss: 2.443000316619873\n",
      "Step 974: val loss: 2.3900890350341797\n",
      "Step 975: train loss: 2.4389312267303467\n",
      "Step 975: val loss: 2.3860552310943604\n",
      "Step 976: train loss: 2.4348697662353516\n",
      "Step 976: val loss: 2.382028579711914\n",
      "Step 977: train loss: 2.4308159351348877\n",
      "Step 977: val loss: 2.378009557723999\n",
      "Step 978: train loss: 2.426769971847534\n",
      "Step 978: val loss: 2.3739981651306152\n",
      "Step 979: train loss: 2.422731399536133\n",
      "Step 979: val loss: 2.3699939250946045\n",
      "Step 980: train loss: 2.4187002182006836\n",
      "Step 980: val loss: 2.365997552871704\n",
      "Step 981: train loss: 2.4146766662597656\n",
      "Step 981: val loss: 2.362008810043335\n",
      "Step 982: train loss: 2.410661220550537\n",
      "Step 982: val loss: 2.358027458190918\n",
      "Step 983: train loss: 2.4066529273986816\n",
      "Step 983: val loss: 2.354053258895874\n",
      "Step 984: train loss: 2.4026520252227783\n",
      "Step 984: val loss: 2.3500864505767822\n",
      "Step 985: train loss: 2.398658514022827\n",
      "Step 985: val loss: 2.34612774848938\n",
      "Step 986: train loss: 2.3946726322174072\n",
      "Step 986: val loss: 2.3421761989593506\n",
      "Step 987: train loss: 2.3906946182250977\n",
      "Step 987: val loss: 2.3382318019866943\n",
      "Step 988: train loss: 2.386723518371582\n",
      "Step 988: val loss: 2.3342957496643066\n",
      "Step 989: train loss: 2.382760524749756\n",
      "Step 989: val loss: 2.3303661346435547\n",
      "Step 990: train loss: 2.3788044452667236\n",
      "Step 990: val loss: 2.326444387435913\n",
      "Step 991: train loss: 2.3748559951782227\n",
      "Step 991: val loss: 2.3225295543670654\n",
      "Step 992: train loss: 2.3709144592285156\n",
      "Step 992: val loss: 2.3186228275299072\n",
      "Step 993: train loss: 2.366981267929077\n",
      "Step 993: val loss: 2.3147225379943848\n",
      "Step 994: train loss: 2.3630547523498535\n",
      "Step 994: val loss: 2.3108303546905518\n",
      "Step 995: train loss: 2.359135389328003\n",
      "Step 995: val loss: 2.3069450855255127\n",
      "Step 996: train loss: 2.3552238941192627\n",
      "Step 996: val loss: 2.303067207336426\n",
      "Step 997: train loss: 2.3513197898864746\n",
      "Step 997: val loss: 2.2991960048675537\n",
      "Step 998: train loss: 2.3474223613739014\n",
      "Step 998: val loss: 2.29533314704895\n",
      "Step 999: train loss: 2.3435332775115967\n",
      "Step 999: val loss: 2.2914769649505615\n",
      "Step 1000: train loss: 2.3396503925323486\n",
      "Step 1000: val loss: 2.287628173828125\n",
      "Step 1001: train loss: 2.33577561378479\n",
      "Step 1001: val loss: 2.2837867736816406\n",
      "Step 1002: train loss: 2.3319077491760254\n",
      "Step 1002: val loss: 2.2799525260925293\n",
      "Step 1003: train loss: 2.328047275543213\n",
      "Step 1003: val loss: 2.276125192642212\n",
      "Step 1004: train loss: 2.3241939544677734\n",
      "Step 1004: val loss: 2.272305727005005\n",
      "Step 1005: train loss: 2.320347785949707\n",
      "Step 1005: val loss: 2.2684926986694336\n",
      "Step 1006: train loss: 2.3165087699890137\n",
      "Step 1006: val loss: 2.2646868228912354\n",
      "Step 1007: train loss: 2.3126771450042725\n",
      "Step 1007: val loss: 2.2608883380889893\n",
      "Step 1008: train loss: 2.308852434158325\n",
      "Step 1008: val loss: 2.2570972442626953\n",
      "Step 1009: train loss: 2.305035352706909\n",
      "Step 1009: val loss: 2.2533135414123535\n",
      "Step 1010: train loss: 2.301225423812866\n",
      "Step 1010: val loss: 2.2495357990264893\n",
      "Step 1011: train loss: 2.297422170639038\n",
      "Step 1011: val loss: 2.2457659244537354\n",
      "Step 1012: train loss: 2.293626070022583\n",
      "Step 1012: val loss: 2.2420034408569336\n",
      "Step 1013: train loss: 2.28983736038208\n",
      "Step 1013: val loss: 2.2382476329803467\n",
      "Step 1014: train loss: 2.286055326461792\n",
      "Step 1014: val loss: 2.2344985008239746\n",
      "Step 1015: train loss: 2.282280683517456\n",
      "Step 1015: val loss: 2.230756998062134\n",
      "Step 1016: train loss: 2.278513193130493\n",
      "Step 1016: val loss: 2.227022171020508\n",
      "Step 1017: train loss: 2.274752616882324\n",
      "Step 1017: val loss: 2.223294734954834\n",
      "Step 1018: train loss: 2.2709994316101074\n",
      "Step 1018: val loss: 2.219574213027954\n",
      "Step 1019: train loss: 2.2672526836395264\n",
      "Step 1019: val loss: 2.21586012840271\n",
      "Step 1020: train loss: 2.2635130882263184\n",
      "Step 1020: val loss: 2.212153911590576\n",
      "Step 1021: train loss: 2.2597811222076416\n",
      "Step 1021: val loss: 2.208453893661499\n",
      "Step 1022: train loss: 2.2560553550720215\n",
      "Step 1022: val loss: 2.204761505126953\n",
      "Step 1023: train loss: 2.2523372173309326\n",
      "Step 1023: val loss: 2.201075553894043\n",
      "Step 1024: train loss: 2.2486250400543213\n",
      "Step 1024: val loss: 2.1973960399627686\n",
      "Step 1025: train loss: 2.244920492172241\n",
      "Step 1025: val loss: 2.1937243938446045\n",
      "Step 1026: train loss: 2.241222858428955\n",
      "Step 1026: val loss: 2.190059185028076\n",
      "Step 1027: train loss: 2.237532138824463\n",
      "Step 1027: val loss: 2.1864006519317627\n",
      "Step 1028: train loss: 2.2338478565216064\n",
      "Step 1028: val loss: 2.1827492713928223\n",
      "Step 1029: train loss: 2.2301714420318604\n",
      "Step 1029: val loss: 2.179104804992676\n",
      "Step 1030: train loss: 2.226501226425171\n",
      "Step 1030: val loss: 2.175466775894165\n",
      "Step 1031: train loss: 2.2228376865386963\n",
      "Step 1031: val loss: 2.1718361377716064\n",
      "Step 1032: train loss: 2.219181537628174\n",
      "Step 1032: val loss: 2.1682119369506836\n",
      "Step 1033: train loss: 2.215531826019287\n",
      "Step 1033: val loss: 2.1645944118499756\n",
      "Step 1034: train loss: 2.2118892669677734\n",
      "Step 1034: val loss: 2.1609838008880615\n",
      "Step 1035: train loss: 2.2082529067993164\n",
      "Step 1035: val loss: 2.1573803424835205\n",
      "Step 1036: train loss: 2.2046241760253906\n",
      "Step 1036: val loss: 2.1537840366363525\n",
      "Step 1037: train loss: 2.2010021209716797\n",
      "Step 1037: val loss: 2.150193452835083\n",
      "Step 1038: train loss: 2.1973862648010254\n",
      "Step 1038: val loss: 2.1466097831726074\n",
      "Step 1039: train loss: 2.193777322769165\n",
      "Step 1039: val loss: 2.143033027648926\n",
      "Step 1040: train loss: 2.1901750564575195\n",
      "Step 1040: val loss: 2.139462947845459\n",
      "Step 1041: train loss: 2.186580181121826\n",
      "Step 1041: val loss: 2.1359002590179443\n",
      "Step 1042: train loss: 2.1829917430877686\n",
      "Step 1042: val loss: 2.1323435306549072\n",
      "Step 1043: train loss: 2.1794097423553467\n",
      "Step 1043: val loss: 2.128793239593506\n",
      "Step 1044: train loss: 2.1758344173431396\n",
      "Step 1044: val loss: 2.1252501010894775\n",
      "Step 1045: train loss: 2.1722657680511475\n",
      "Step 1045: val loss: 2.121713161468506\n",
      "Step 1046: train loss: 2.168703556060791\n",
      "Step 1046: val loss: 2.118183135986328\n",
      "Step 1047: train loss: 2.1651484966278076\n",
      "Step 1047: val loss: 2.1146600246429443\n",
      "Step 1048: train loss: 2.16159987449646\n",
      "Step 1048: val loss: 2.111143112182617\n",
      "Step 1049: train loss: 2.1580584049224854\n",
      "Step 1049: val loss: 2.107633113861084\n",
      "Step 1050: train loss: 2.1545228958129883\n",
      "Step 1050: val loss: 2.1041290760040283\n",
      "Step 1051: train loss: 2.150993585586548\n",
      "Step 1051: val loss: 2.100632429122925\n",
      "Step 1052: train loss: 2.1474721431732178\n",
      "Step 1052: val loss: 2.097141742706299\n",
      "Step 1053: train loss: 2.143955945968628\n",
      "Step 1053: val loss: 2.0936577320098877\n",
      "Step 1054: train loss: 2.140446662902832\n",
      "Step 1054: val loss: 2.0901801586151123\n",
      "Step 1055: train loss: 2.136944532394409\n",
      "Step 1055: val loss: 2.0867090225219727\n",
      "Step 1056: train loss: 2.1334478855133057\n",
      "Step 1056: val loss: 2.0832443237304688\n",
      "Step 1057: train loss: 2.129958391189575\n",
      "Step 1057: val loss: 2.079786777496338\n",
      "Step 1058: train loss: 2.1264760494232178\n",
      "Step 1058: val loss: 2.0763354301452637\n",
      "Step 1059: train loss: 2.122999429702759\n",
      "Step 1059: val loss: 2.072890520095825\n",
      "Step 1060: train loss: 2.1195297241210938\n",
      "Step 1060: val loss: 2.0694522857666016\n",
      "Step 1061: train loss: 2.1160664558410645\n",
      "Step 1061: val loss: 2.0660200119018555\n",
      "Step 1062: train loss: 2.112609624862671\n",
      "Step 1062: val loss: 2.0625948905944824\n",
      "Step 1063: train loss: 2.109159231185913\n",
      "Step 1063: val loss: 2.059175729751587\n",
      "Step 1064: train loss: 2.105715036392212\n",
      "Step 1064: val loss: 2.055762767791748\n",
      "Step 1065: train loss: 2.1022772789001465\n",
      "Step 1065: val loss: 2.052356719970703\n",
      "Step 1066: train loss: 2.098846197128296\n",
      "Step 1066: val loss: 2.048957109451294\n",
      "Step 1067: train loss: 2.095421552658081\n",
      "Step 1067: val loss: 2.0455634593963623\n",
      "Step 1068: train loss: 2.0920026302337646\n",
      "Step 1068: val loss: 2.0421762466430664\n",
      "Step 1069: train loss: 2.0885908603668213\n",
      "Step 1069: val loss: 2.0387954711914062\n",
      "Step 1070: train loss: 2.0851850509643555\n",
      "Step 1070: val loss: 2.035421133041382\n",
      "Step 1071: train loss: 2.0817861557006836\n",
      "Step 1071: val loss: 2.032052993774414\n",
      "Step 1072: train loss: 2.078392744064331\n",
      "Step 1072: val loss: 2.0286905765533447\n",
      "Step 1073: train loss: 2.0750062465667725\n",
      "Step 1073: val loss: 2.0253355503082275\n",
      "Step 1074: train loss: 2.0716261863708496\n",
      "Step 1074: val loss: 2.021986484527588\n",
      "Step 1075: train loss: 2.068251848220825\n",
      "Step 1075: val loss: 2.0186431407928467\n",
      "Step 1076: train loss: 2.0648844242095947\n",
      "Step 1076: val loss: 2.0153067111968994\n",
      "Step 1077: train loss: 2.0615227222442627\n",
      "Step 1077: val loss: 2.0119755268096924\n",
      "Step 1078: train loss: 2.0581672191619873\n",
      "Step 1078: val loss: 2.0086517333984375\n",
      "Step 1079: train loss: 2.0548181533813477\n",
      "Step 1079: val loss: 2.005333423614502\n",
      "Step 1080: train loss: 2.0514750480651855\n",
      "Step 1080: val loss: 2.002021551132202\n",
      "Step 1081: train loss: 2.0481386184692383\n",
      "Step 1081: val loss: 1.9987155199050903\n",
      "Step 1082: train loss: 2.0448081493377686\n",
      "Step 1082: val loss: 1.995416522026062\n",
      "Step 1083: train loss: 2.0414841175079346\n",
      "Step 1083: val loss: 1.9921232461929321\n",
      "Step 1084: train loss: 2.038165807723999\n",
      "Step 1084: val loss: 1.9888359308242798\n",
      "Step 1085: train loss: 2.0348546504974365\n",
      "Step 1085: val loss: 1.9855555295944214\n",
      "Step 1086: train loss: 2.0315487384796143\n",
      "Step 1086: val loss: 1.9822804927825928\n",
      "Step 1087: train loss: 2.028249502182007\n",
      "Step 1087: val loss: 1.9790118932724\n",
      "Step 1088: train loss: 2.0249557495117188\n",
      "Step 1088: val loss: 1.9757490158081055\n",
      "Step 1089: train loss: 2.0216689109802246\n",
      "Step 1089: val loss: 1.9724925756454468\n",
      "Step 1090: train loss: 2.018387794494629\n",
      "Step 1090: val loss: 1.9692422151565552\n",
      "Step 1091: train loss: 2.01511287689209\n",
      "Step 1091: val loss: 1.9659984111785889\n",
      "Step 1092: train loss: 2.0118441581726074\n",
      "Step 1092: val loss: 1.9627599716186523\n",
      "Step 1093: train loss: 2.0085813999176025\n",
      "Step 1093: val loss: 1.9595280885696411\n",
      "Step 1094: train loss: 2.005324602127075\n",
      "Step 1094: val loss: 1.9563020467758179\n",
      "Step 1095: train loss: 2.0020740032196045\n",
      "Step 1095: val loss: 1.9530819654464722\n",
      "Step 1096: train loss: 1.9988293647766113\n",
      "Step 1096: val loss: 1.9498684406280518\n",
      "Step 1097: train loss: 1.9955910444259644\n",
      "Step 1097: val loss: 1.9466603994369507\n",
      "Step 1098: train loss: 1.9923588037490845\n",
      "Step 1098: val loss: 1.9434587955474854\n",
      "Step 1099: train loss: 1.989132285118103\n",
      "Step 1099: val loss: 1.9402624368667603\n",
      "Step 1100: train loss: 1.9859116077423096\n",
      "Step 1100: val loss: 1.9370726346969604\n",
      "Step 1101: train loss: 1.9826973676681519\n",
      "Step 1101: val loss: 1.9338886737823486\n",
      "Step 1102: train loss: 1.9794888496398926\n",
      "Step 1102: val loss: 1.9307106733322144\n",
      "Step 1103: train loss: 1.9762861728668213\n",
      "Step 1103: val loss: 1.9275387525558472\n",
      "Step 1104: train loss: 1.973089575767517\n",
      "Step 1104: val loss: 1.9243724346160889\n",
      "Step 1105: train loss: 1.9698988199234009\n",
      "Step 1105: val loss: 1.9212123155593872\n",
      "Step 1106: train loss: 1.9667142629623413\n",
      "Step 1106: val loss: 1.918058156967163\n",
      "Step 1107: train loss: 1.9635356664657593\n",
      "Step 1107: val loss: 1.9149096012115479\n",
      "Step 1108: train loss: 1.960362434387207\n",
      "Step 1108: val loss: 1.9117672443389893\n",
      "Step 1109: train loss: 1.95719575881958\n",
      "Step 1109: val loss: 1.9086307287216187\n",
      "Step 1110: train loss: 1.9540349245071411\n",
      "Step 1110: val loss: 1.905500054359436\n",
      "Step 1111: train loss: 1.950879693031311\n",
      "Step 1111: val loss: 1.9023752212524414\n",
      "Step 1112: train loss: 1.9477301836013794\n",
      "Step 1112: val loss: 1.8992563486099243\n",
      "Step 1113: train loss: 1.9445871114730835\n",
      "Step 1113: val loss: 1.8961431980133057\n",
      "Step 1114: train loss: 1.9414489269256592\n",
      "Step 1114: val loss: 1.893035650253296\n",
      "Step 1115: train loss: 1.9383175373077393\n",
      "Step 1115: val loss: 1.8899344205856323\n",
      "Step 1116: train loss: 1.9351915121078491\n",
      "Step 1116: val loss: 1.8868385553359985\n",
      "Step 1117: train loss: 1.9320718050003052\n",
      "Step 1117: val loss: 1.8837487697601318\n",
      "Step 1118: train loss: 1.928957223892212\n",
      "Step 1118: val loss: 1.8806647062301636\n",
      "Step 1119: train loss: 1.9258488416671753\n",
      "Step 1119: val loss: 1.8775866031646729\n",
      "Step 1120: train loss: 1.922745943069458\n",
      "Step 1120: val loss: 1.874513864517212\n",
      "Step 1121: train loss: 1.9196491241455078\n",
      "Step 1121: val loss: 1.8714473247528076\n",
      "Step 1122: train loss: 1.9165579080581665\n",
      "Step 1122: val loss: 1.868385910987854\n",
      "Step 1123: train loss: 1.9134721755981445\n",
      "Step 1123: val loss: 1.8653308153152466\n",
      "Step 1124: train loss: 1.9103926420211792\n",
      "Step 1124: val loss: 1.8622807264328003\n",
      "Step 1125: train loss: 1.9073184728622437\n",
      "Step 1125: val loss: 1.8592373132705688\n",
      "Step 1126: train loss: 1.9042502641677856\n",
      "Step 1126: val loss: 1.856199026107788\n",
      "Step 1127: train loss: 1.9011878967285156\n",
      "Step 1127: val loss: 1.853166937828064\n",
      "Step 1128: train loss: 1.8981311321258545\n",
      "Step 1128: val loss: 1.8501399755477905\n",
      "Step 1129: train loss: 1.8950802087783813\n",
      "Step 1129: val loss: 1.8471189737319946\n",
      "Step 1130: train loss: 1.8920345306396484\n",
      "Step 1130: val loss: 1.8441035747528076\n",
      "Step 1131: train loss: 1.888994574546814\n",
      "Step 1131: val loss: 1.8410937786102295\n",
      "Step 1132: train loss: 1.8859604597091675\n",
      "Step 1132: val loss: 1.8380892276763916\n",
      "Step 1133: train loss: 1.8829318284988403\n",
      "Step 1133: val loss: 1.8350909948349\n",
      "Step 1134: train loss: 1.8799089193344116\n",
      "Step 1134: val loss: 1.8320978879928589\n",
      "Step 1135: train loss: 1.87689208984375\n",
      "Step 1135: val loss: 1.829110860824585\n",
      "Step 1136: train loss: 1.87388014793396\n",
      "Step 1136: val loss: 1.8261289596557617\n",
      "Step 1137: train loss: 1.8708741664886475\n",
      "Step 1137: val loss: 1.8231525421142578\n",
      "Step 1138: train loss: 1.8678735494613647\n",
      "Step 1138: val loss: 1.8201823234558105\n",
      "Step 1139: train loss: 1.8648786544799805\n",
      "Step 1139: val loss: 1.817217230796814\n",
      "Step 1140: train loss: 1.861889362335205\n",
      "Step 1140: val loss: 1.8142576217651367\n",
      "Step 1141: train loss: 1.8589054346084595\n",
      "Step 1141: val loss: 1.8113036155700684\n",
      "Step 1142: train loss: 1.8559273481369019\n",
      "Step 1142: val loss: 1.808355689048767\n",
      "Step 1143: train loss: 1.8529547452926636\n",
      "Step 1143: val loss: 1.8054126501083374\n",
      "Step 1144: train loss: 1.8499870300292969\n",
      "Step 1144: val loss: 1.8024752140045166\n",
      "Step 1145: train loss: 1.8470256328582764\n",
      "Step 1145: val loss: 1.7995433807373047\n",
      "Step 1146: train loss: 1.8440693616867065\n",
      "Step 1146: val loss: 1.7966169118881226\n",
      "Step 1147: train loss: 1.8411186933517456\n",
      "Step 1147: val loss: 1.7936960458755493\n",
      "Step 1148: train loss: 1.838173508644104\n",
      "Step 1148: val loss: 1.790780782699585\n",
      "Step 1149: train loss: 1.8352336883544922\n",
      "Step 1149: val loss: 1.7878705263137817\n",
      "Step 1150: train loss: 1.8322995901107788\n",
      "Step 1150: val loss: 1.7849665880203247\n",
      "Step 1151: train loss: 1.8293708562850952\n",
      "Step 1151: val loss: 1.7820671796798706\n",
      "Step 1152: train loss: 1.8264472484588623\n",
      "Step 1152: val loss: 1.779173493385315\n",
      "Step 1153: train loss: 1.8235292434692383\n",
      "Step 1153: val loss: 1.7762852907180786\n",
      "Step 1154: train loss: 1.8206169605255127\n",
      "Step 1154: val loss: 1.7734023332595825\n",
      "Step 1155: train loss: 1.8177096843719482\n",
      "Step 1155: val loss: 1.7705252170562744\n",
      "Step 1156: train loss: 1.8148081302642822\n",
      "Step 1156: val loss: 1.7676526308059692\n",
      "Step 1157: train loss: 1.8119113445281982\n",
      "Step 1157: val loss: 1.7647862434387207\n",
      "Step 1158: train loss: 1.8090206384658813\n",
      "Step 1158: val loss: 1.7619251012802124\n",
      "Step 1159: train loss: 1.8061352968215942\n",
      "Step 1159: val loss: 1.7590692043304443\n",
      "Step 1160: train loss: 1.8032547235488892\n",
      "Step 1160: val loss: 1.7562185525894165\n",
      "Step 1161: train loss: 1.800379991531372\n",
      "Step 1161: val loss: 1.7533730268478394\n",
      "Step 1162: train loss: 1.7975101470947266\n",
      "Step 1162: val loss: 1.7505332231521606\n",
      "Step 1163: train loss: 1.7946462631225586\n",
      "Step 1163: val loss: 1.7476987838745117\n",
      "Step 1164: train loss: 1.7917873859405518\n",
      "Step 1164: val loss: 1.744869351387024\n",
      "Step 1165: train loss: 1.7889336347579956\n",
      "Step 1165: val loss: 1.7420450448989868\n",
      "Step 1166: train loss: 1.7860851287841797\n",
      "Step 1166: val loss: 1.739226222038269\n",
      "Step 1167: train loss: 1.783242106437683\n",
      "Step 1167: val loss: 1.7364131212234497\n",
      "Step 1168: train loss: 1.7804045677185059\n",
      "Step 1168: val loss: 1.7336050271987915\n",
      "Step 1169: train loss: 1.7775721549987793\n",
      "Step 1169: val loss: 1.7308015823364258\n",
      "Step 1170: train loss: 1.7747445106506348\n",
      "Step 1170: val loss: 1.7280042171478271\n",
      "Step 1171: train loss: 1.7719228267669678\n",
      "Step 1171: val loss: 1.7252118587493896\n",
      "Step 1172: train loss: 1.769106388092041\n",
      "Step 1172: val loss: 1.7224247455596924\n",
      "Step 1173: train loss: 1.7662949562072754\n",
      "Step 1173: val loss: 1.7196427583694458\n",
      "Step 1174: train loss: 1.763488531112671\n",
      "Step 1174: val loss: 1.716866374015808\n",
      "Step 1175: train loss: 1.7606874704360962\n",
      "Step 1175: val loss: 1.714094638824463\n",
      "Step 1176: train loss: 1.7578918933868408\n",
      "Step 1176: val loss: 1.711328148841858\n",
      "Step 1177: train loss: 1.755100965499878\n",
      "Step 1177: val loss: 1.7085670232772827\n",
      "Step 1178: train loss: 1.7523152828216553\n",
      "Step 1178: val loss: 1.7058109045028687\n",
      "Step 1179: train loss: 1.749535083770752\n",
      "Step 1179: val loss: 1.7030596733093262\n",
      "Step 1180: train loss: 1.7467596530914307\n",
      "Step 1180: val loss: 1.7003140449523926\n",
      "Step 1181: train loss: 1.7439898252487183\n",
      "Step 1181: val loss: 1.697573184967041\n",
      "Step 1182: train loss: 1.7412251234054565\n",
      "Step 1182: val loss: 1.694838047027588\n",
      "Step 1183: train loss: 1.7384651899337769\n",
      "Step 1183: val loss: 1.6921076774597168\n",
      "Step 1184: train loss: 1.735710620880127\n",
      "Step 1184: val loss: 1.6893823146820068\n",
      "Step 1185: train loss: 1.7329607009887695\n",
      "Step 1185: val loss: 1.6866623163223267\n",
      "Step 1186: train loss: 1.7302165031433105\n",
      "Step 1186: val loss: 1.6839474439620972\n",
      "Step 1187: train loss: 1.7274774312973022\n",
      "Step 1187: val loss: 1.6812371015548706\n",
      "Step 1188: train loss: 1.7247426509857178\n",
      "Step 1188: val loss: 1.678532600402832\n",
      "Step 1189: train loss: 1.7220138311386108\n",
      "Step 1189: val loss: 1.6758325099945068\n",
      "Step 1190: train loss: 1.7192895412445068\n",
      "Step 1190: val loss: 1.6731376647949219\n",
      "Step 1191: train loss: 1.7165706157684326\n",
      "Step 1191: val loss: 1.6704479455947876\n",
      "Step 1192: train loss: 1.7138563394546509\n",
      "Step 1192: val loss: 1.667763113975525\n",
      "Step 1193: train loss: 1.7111469507217407\n",
      "Step 1193: val loss: 1.6650835275650024\n",
      "Step 1194: train loss: 1.708443522453308\n",
      "Step 1194: val loss: 1.6624090671539307\n",
      "Step 1195: train loss: 1.7057446241378784\n",
      "Step 1195: val loss: 1.659739375114441\n",
      "Step 1196: train loss: 1.7030504941940308\n",
      "Step 1196: val loss: 1.6570746898651123\n",
      "Step 1197: train loss: 1.7003616094589233\n",
      "Step 1197: val loss: 1.654414415359497\n",
      "Step 1198: train loss: 1.6976772546768188\n",
      "Step 1198: val loss: 1.6517599821090698\n",
      "Step 1199: train loss: 1.6949985027313232\n",
      "Step 1199: val loss: 1.6491107940673828\n",
      "Step 1200: train loss: 1.6923245191574097\n",
      "Step 1200: val loss: 1.6464658975601196\n",
      "Step 1201: train loss: 1.6896556615829468\n",
      "Step 1201: val loss: 1.6438260078430176\n",
      "Step 1202: train loss: 1.6869913339614868\n",
      "Step 1202: val loss: 1.6411908864974976\n",
      "Step 1203: train loss: 1.6843321323394775\n",
      "Step 1203: val loss: 1.6385611295700073\n",
      "Step 1204: train loss: 1.6816776990890503\n",
      "Step 1204: val loss: 1.63593590259552\n",
      "Step 1205: train loss: 1.6790282726287842\n",
      "Step 1205: val loss: 1.6333162784576416\n",
      "Step 1206: train loss: 1.6763842105865479\n",
      "Step 1206: val loss: 1.6307010650634766\n",
      "Step 1207: train loss: 1.673744797706604\n",
      "Step 1207: val loss: 1.628090500831604\n",
      "Step 1208: train loss: 1.6711100339889526\n",
      "Step 1208: val loss: 1.6254850625991821\n",
      "Step 1209: train loss: 1.668480396270752\n",
      "Step 1209: val loss: 1.622884750366211\n",
      "Step 1210: train loss: 1.6658557653427124\n",
      "Step 1210: val loss: 1.6202888488769531\n",
      "Step 1211: train loss: 1.6632351875305176\n",
      "Step 1211: val loss: 1.6176979541778564\n",
      "Step 1212: train loss: 1.6606202125549316\n",
      "Step 1212: val loss: 1.615112066268921\n",
      "Step 1213: train loss: 1.658009648323059\n",
      "Step 1213: val loss: 1.6125305891036987\n",
      "Step 1214: train loss: 1.6554042100906372\n",
      "Step 1214: val loss: 1.609954595565796\n",
      "Step 1215: train loss: 1.652803659439087\n",
      "Step 1215: val loss: 1.607383131980896\n",
      "Step 1216: train loss: 1.6502081155776978\n",
      "Step 1216: val loss: 1.6048167943954468\n",
      "Step 1217: train loss: 1.647617220878601\n",
      "Step 1217: val loss: 1.60225510597229\n",
      "Step 1218: train loss: 1.6450310945510864\n",
      "Step 1218: val loss: 1.5996978282928467\n",
      "Step 1219: train loss: 1.6424497365951538\n",
      "Step 1219: val loss: 1.5971461534500122\n",
      "Step 1220: train loss: 1.6398732662200928\n",
      "Step 1220: val loss: 1.594598650932312\n",
      "Step 1221: train loss: 1.6373016834259033\n",
      "Step 1221: val loss: 1.5920559167861938\n",
      "Step 1222: train loss: 1.6347346305847168\n",
      "Step 1222: val loss: 1.5895179510116577\n",
      "Step 1223: train loss: 1.6321722269058228\n",
      "Step 1223: val loss: 1.5869848728179932\n",
      "Step 1224: train loss: 1.6296147108078003\n",
      "Step 1224: val loss: 1.584456443786621\n",
      "Step 1225: train loss: 1.6270619630813599\n",
      "Step 1225: val loss: 1.581932783126831\n",
      "Step 1226: train loss: 1.6245136260986328\n",
      "Step 1226: val loss: 1.579413652420044\n",
      "Step 1227: train loss: 1.621970295906067\n",
      "Step 1227: val loss: 1.5768991708755493\n",
      "Step 1228: train loss: 1.6194316148757935\n",
      "Step 1228: val loss: 1.5743896961212158\n",
      "Step 1229: train loss: 1.6168979406356812\n",
      "Step 1229: val loss: 1.571885108947754\n",
      "Step 1230: train loss: 1.6143686771392822\n",
      "Step 1230: val loss: 1.5693848133087158\n",
      "Step 1231: train loss: 1.6118439435958862\n",
      "Step 1231: val loss: 1.5668891668319702\n",
      "Step 1232: train loss: 1.6093242168426514\n",
      "Step 1232: val loss: 1.5643986463546753\n",
      "Step 1233: train loss: 1.6068092584609985\n",
      "Step 1233: val loss: 1.5619122982025146\n",
      "Step 1234: train loss: 1.6042983531951904\n",
      "Step 1234: val loss: 1.5594308376312256\n",
      "Step 1235: train loss: 1.601792812347412\n",
      "Step 1235: val loss: 1.5569543838500977\n",
      "Step 1236: train loss: 1.5992913246154785\n",
      "Step 1236: val loss: 1.554481863975525\n",
      "Step 1237: train loss: 1.596794843673706\n",
      "Step 1237: val loss: 1.5520143508911133\n",
      "Step 1238: train loss: 1.594303011894226\n",
      "Step 1238: val loss: 1.5495516061782837\n",
      "Step 1239: train loss: 1.591815710067749\n",
      "Step 1239: val loss: 1.547093152999878\n",
      "Step 1240: train loss: 1.5893330574035645\n",
      "Step 1240: val loss: 1.5446397066116333\n",
      "Step 1241: train loss: 1.5868548154830933\n",
      "Step 1241: val loss: 1.542190432548523\n",
      "Step 1242: train loss: 1.584381103515625\n",
      "Step 1242: val loss: 1.5397453308105469\n",
      "Step 1243: train loss: 1.5819119215011597\n",
      "Step 1243: val loss: 1.5373057126998901\n",
      "Step 1244: train loss: 1.5794475078582764\n",
      "Step 1244: val loss: 1.5348701477050781\n",
      "Step 1245: train loss: 1.576987385749817\n",
      "Step 1245: val loss: 1.5324389934539795\n",
      "Step 1246: train loss: 1.5745320320129395\n",
      "Step 1246: val loss: 1.530012845993042\n",
      "Step 1247: train loss: 1.572081446647644\n",
      "Step 1247: val loss: 1.5275912284851074\n",
      "Step 1248: train loss: 1.569635272026062\n",
      "Step 1248: val loss: 1.5251737833023071\n",
      "Step 1249: train loss: 1.5671935081481934\n",
      "Step 1249: val loss: 1.5227612257003784\n",
      "Step 1250: train loss: 1.5647563934326172\n",
      "Step 1250: val loss: 1.5203529596328735\n",
      "Step 1251: train loss: 1.562323808670044\n",
      "Step 1251: val loss: 1.517949104309082\n",
      "Step 1252: train loss: 1.5598955154418945\n",
      "Step 1252: val loss: 1.515549898147583\n",
      "Step 1253: train loss: 1.557471752166748\n",
      "Step 1253: val loss: 1.5131549835205078\n",
      "Step 1254: train loss: 1.5550525188446045\n",
      "Step 1254: val loss: 1.510764718055725\n",
      "Step 1255: train loss: 1.5526378154754639\n",
      "Step 1255: val loss: 1.5083791017532349\n",
      "Step 1256: train loss: 1.5502276420593262\n",
      "Step 1256: val loss: 1.505997896194458\n",
      "Step 1257: train loss: 1.547822117805481\n",
      "Step 1257: val loss: 1.503620982170105\n",
      "Step 1258: train loss: 1.5454206466674805\n",
      "Step 1258: val loss: 1.5012484788894653\n",
      "Step 1259: train loss: 1.543023705482483\n",
      "Step 1259: val loss: 1.4988808631896973\n",
      "Step 1260: train loss: 1.5406312942504883\n",
      "Step 1260: val loss: 1.4965168237686157\n",
      "Step 1261: train loss: 1.5382434129714966\n",
      "Step 1261: val loss: 1.4941582679748535\n",
      "Step 1262: train loss: 1.5358598232269287\n",
      "Step 1262: val loss: 1.4918031692504883\n",
      "Step 1263: train loss: 1.5334806442260742\n",
      "Step 1263: val loss: 1.4894530773162842\n",
      "Step 1264: train loss: 1.5311057567596436\n",
      "Step 1264: val loss: 1.4871069192886353\n",
      "Step 1265: train loss: 1.5287355184555054\n",
      "Step 1265: val loss: 1.484765887260437\n",
      "Step 1266: train loss: 1.5263694524765015\n",
      "Step 1266: val loss: 1.4824285507202148\n",
      "Step 1267: train loss: 1.524007797241211\n",
      "Step 1267: val loss: 1.4800957441329956\n",
      "Step 1268: train loss: 1.5216504335403442\n",
      "Step 1268: val loss: 1.4777674674987793\n",
      "Step 1269: train loss: 1.5192975997924805\n",
      "Step 1269: val loss: 1.4754436016082764\n",
      "Step 1270: train loss: 1.51694917678833\n",
      "Step 1270: val loss: 1.4731240272521973\n",
      "Step 1271: train loss: 1.5146050453186035\n",
      "Step 1271: val loss: 1.4708086252212524\n",
      "Step 1272: train loss: 1.5122652053833008\n",
      "Step 1272: val loss: 1.468497395515442\n",
      "Step 1273: train loss: 1.5099295377731323\n",
      "Step 1273: val loss: 1.4661909341812134\n",
      "Step 1274: train loss: 1.5075982809066772\n",
      "Step 1274: val loss: 1.4638887643814087\n",
      "Step 1275: train loss: 1.5052717924118042\n",
      "Step 1275: val loss: 1.4615905284881592\n",
      "Step 1276: train loss: 1.5029488801956177\n",
      "Step 1276: val loss: 1.4592968225479126\n",
      "Step 1277: train loss: 1.5006308555603027\n",
      "Step 1277: val loss: 1.4570072889328003\n",
      "Step 1278: train loss: 1.4983166456222534\n",
      "Step 1278: val loss: 1.4547224044799805\n",
      "Step 1279: train loss: 1.4960070848464966\n",
      "Step 1279: val loss: 1.452441692352295\n",
      "Step 1280: train loss: 1.493701696395874\n",
      "Step 1280: val loss: 1.450165033340454\n",
      "Step 1281: train loss: 1.4914004802703857\n",
      "Step 1281: val loss: 1.4478925466537476\n",
      "Step 1282: train loss: 1.4891036748886108\n",
      "Step 1282: val loss: 1.4456247091293335\n",
      "Step 1283: train loss: 1.4868111610412598\n",
      "Step 1283: val loss: 1.4433608055114746\n",
      "Step 1284: train loss: 1.4845227003097534\n",
      "Step 1284: val loss: 1.441101312637329\n",
      "Step 1285: train loss: 1.482238531112671\n",
      "Step 1285: val loss: 1.4388457536697388\n",
      "Step 1286: train loss: 1.4799586534500122\n",
      "Step 1286: val loss: 1.436594843864441\n",
      "Step 1287: train loss: 1.4776828289031982\n",
      "Step 1287: val loss: 1.4343475103378296\n",
      "Step 1288: train loss: 1.4754114151000977\n",
      "Step 1288: val loss: 1.4321048259735107\n",
      "Step 1289: train loss: 1.4731436967849731\n",
      "Step 1289: val loss: 1.4298664331436157\n",
      "Step 1290: train loss: 1.4708807468414307\n",
      "Step 1290: val loss: 1.4276323318481445\n",
      "Step 1291: train loss: 1.4686219692230225\n",
      "Step 1291: val loss: 1.425402045249939\n",
      "Step 1292: train loss: 1.4663670063018799\n",
      "Step 1292: val loss: 1.4231760501861572\n",
      "Step 1293: train loss: 1.4641165733337402\n",
      "Step 1293: val loss: 1.4209543466567993\n",
      "Step 1294: train loss: 1.4618700742721558\n",
      "Step 1294: val loss: 1.4187368154525757\n",
      "Step 1295: train loss: 1.4596279859542847\n",
      "Step 1295: val loss: 1.4165233373641968\n",
      "Step 1296: train loss: 1.4573895931243896\n",
      "Step 1296: val loss: 1.4143136739730835\n",
      "Step 1297: train loss: 1.455155849456787\n",
      "Step 1297: val loss: 1.4121088981628418\n",
      "Step 1298: train loss: 1.4529261589050293\n",
      "Step 1298: val loss: 1.4099078178405762\n",
      "Step 1299: train loss: 1.4507004022598267\n",
      "Step 1299: val loss: 1.4077106714248657\n",
      "Step 1300: train loss: 1.4484789371490479\n",
      "Step 1300: val loss: 1.405517816543579\n",
      "Step 1301: train loss: 1.4462612867355347\n",
      "Step 1301: val loss: 1.4033294916152954\n",
      "Step 1302: train loss: 1.4440479278564453\n",
      "Step 1302: val loss: 1.4011447429656982\n",
      "Step 1303: train loss: 1.4418387413024902\n",
      "Step 1303: val loss: 1.398964285850525\n",
      "Step 1304: train loss: 1.439633846282959\n",
      "Step 1304: val loss: 1.3967876434326172\n",
      "Step 1305: train loss: 1.4374324083328247\n",
      "Step 1305: val loss: 1.3946154117584229\n",
      "Step 1306: train loss: 1.435235619544983\n",
      "Step 1306: val loss: 1.3924469947814941\n",
      "Step 1307: train loss: 1.4330424070358276\n",
      "Step 1307: val loss: 1.3902827501296997\n",
      "Step 1308: train loss: 1.4308533668518066\n",
      "Step 1308: val loss: 1.3881226778030396\n",
      "Step 1309: train loss: 1.428668737411499\n",
      "Step 1309: val loss: 1.3859666585922241\n",
      "Step 1310: train loss: 1.426487922668457\n",
      "Step 1310: val loss: 1.3838146924972534\n",
      "Step 1311: train loss: 1.4243112802505493\n",
      "Step 1311: val loss: 1.3816664218902588\n",
      "Step 1312: train loss: 1.4221384525299072\n",
      "Step 1312: val loss: 1.3795225620269775\n",
      "Step 1313: train loss: 1.4199694395065308\n",
      "Step 1313: val loss: 1.3773820400238037\n",
      "Step 1314: train loss: 1.4178048372268677\n",
      "Step 1314: val loss: 1.375246524810791\n",
      "Step 1315: train loss: 1.4156441688537598\n",
      "Step 1315: val loss: 1.3731144666671753\n",
      "Step 1316: train loss: 1.4134873151779175\n",
      "Step 1316: val loss: 1.3709862232208252\n",
      "Step 1317: train loss: 1.41133451461792\n",
      "Step 1317: val loss: 1.368862271308899\n",
      "Step 1318: train loss: 1.4091856479644775\n",
      "Step 1318: val loss: 1.3667422533035278\n",
      "Step 1319: train loss: 1.4070411920547485\n",
      "Step 1319: val loss: 1.3646262884140015\n",
      "Step 1320: train loss: 1.4049001932144165\n",
      "Step 1320: val loss: 1.3625140190124512\n",
      "Step 1321: train loss: 1.4027634859085083\n",
      "Step 1321: val loss: 1.3604059219360352\n",
      "Step 1322: train loss: 1.4006303548812866\n",
      "Step 1322: val loss: 1.3583014011383057\n",
      "Step 1323: train loss: 1.3985013961791992\n",
      "Step 1323: val loss: 1.3562010526657104\n",
      "Step 1324: train loss: 1.3963762521743774\n",
      "Step 1324: val loss: 1.354104995727539\n",
      "Step 1325: train loss: 1.3942551612854004\n",
      "Step 1325: val loss: 1.3520123958587646\n",
      "Step 1326: train loss: 1.3921382427215576\n",
      "Step 1326: val loss: 1.3499243259429932\n",
      "Step 1327: train loss: 1.3900247812271118\n",
      "Step 1327: val loss: 1.34783935546875\n",
      "Step 1328: train loss: 1.3879154920578003\n",
      "Step 1328: val loss: 1.3457589149475098\n",
      "Step 1329: train loss: 1.3858100175857544\n",
      "Step 1329: val loss: 1.343682050704956\n",
      "Step 1330: train loss: 1.3837084770202637\n",
      "Step 1330: val loss: 1.341609239578247\n",
      "Step 1331: train loss: 1.3816109895706177\n",
      "Step 1331: val loss: 1.3395400047302246\n",
      "Step 1332: train loss: 1.3795170783996582\n",
      "Step 1332: val loss: 1.3374748229980469\n",
      "Step 1333: train loss: 1.3774269819259644\n",
      "Step 1333: val loss: 1.3354134559631348\n",
      "Step 1334: train loss: 1.3753408193588257\n",
      "Step 1334: val loss: 1.3333563804626465\n",
      "Step 1335: train loss: 1.3732588291168213\n",
      "Step 1335: val loss: 1.3313030004501343\n",
      "Step 1336: train loss: 1.371180534362793\n",
      "Step 1336: val loss: 1.329253077507019\n",
      "Step 1337: train loss: 1.3691059350967407\n",
      "Step 1337: val loss: 1.327207088470459\n",
      "Step 1338: train loss: 1.367035150527954\n",
      "Step 1338: val loss: 1.3251652717590332\n",
      "Step 1339: train loss: 1.3649682998657227\n",
      "Step 1339: val loss: 1.3231267929077148\n",
      "Step 1340: train loss: 1.3629051446914673\n",
      "Step 1340: val loss: 1.3210924863815308\n",
      "Step 1341: train loss: 1.3608461618423462\n",
      "Step 1341: val loss: 1.3190621137619019\n",
      "Step 1342: train loss: 1.358790636062622\n",
      "Step 1342: val loss: 1.31703519821167\n",
      "Step 1343: train loss: 1.3567390441894531\n",
      "Step 1343: val loss: 1.315011978149414\n",
      "Step 1344: train loss: 1.3546912670135498\n",
      "Step 1344: val loss: 1.312993049621582\n",
      "Step 1345: train loss: 1.352647304534912\n",
      "Step 1345: val loss: 1.310977578163147\n",
      "Step 1346: train loss: 1.3506066799163818\n",
      "Step 1346: val loss: 1.308965802192688\n",
      "Step 1347: train loss: 1.348570466041565\n",
      "Step 1347: val loss: 1.3069579601287842\n",
      "Step 1348: train loss: 1.346537470817566\n",
      "Step 1348: val loss: 1.304953932762146\n",
      "Step 1349: train loss: 1.3445087671279907\n",
      "Step 1349: val loss: 1.3029537200927734\n",
      "Step 1350: train loss: 1.3424837589263916\n",
      "Step 1350: val loss: 1.3009570837020874\n",
      "Step 1351: train loss: 1.3404620885849\n",
      "Step 1351: val loss: 1.298964262008667\n",
      "Step 1352: train loss: 1.3384443521499634\n",
      "Step 1352: val loss: 1.2969752550125122\n",
      "Step 1353: train loss: 1.3364306688308716\n",
      "Step 1353: val loss: 1.294989824295044\n",
      "Step 1354: train loss: 1.3344204425811768\n",
      "Step 1354: val loss: 1.2930082082748413\n",
      "Step 1355: train loss: 1.3324137926101685\n",
      "Step 1355: val loss: 1.2910304069519043\n",
      "Step 1356: train loss: 1.3304109573364258\n",
      "Step 1356: val loss: 1.2890560626983643\n",
      "Step 1357: train loss: 1.3284118175506592\n",
      "Step 1357: val loss: 1.2870854139328003\n",
      "Step 1358: train loss: 1.3264161348342896\n",
      "Step 1358: val loss: 1.285118579864502\n",
      "Step 1359: train loss: 1.3244242668151855\n",
      "Step 1359: val loss: 1.2831552028656006\n",
      "Step 1360: train loss: 1.3224364519119263\n",
      "Step 1360: val loss: 1.2811956405639648\n",
      "Step 1361: train loss: 1.3204518556594849\n",
      "Step 1361: val loss: 1.2792400121688843\n",
      "Step 1362: train loss: 1.3184711933135986\n",
      "Step 1362: val loss: 1.2772878408432007\n",
      "Step 1363: train loss: 1.316494107246399\n",
      "Step 1363: val loss: 1.2753394842147827\n",
      "Step 1364: train loss: 1.3145205974578857\n",
      "Step 1364: val loss: 1.2733943462371826\n",
      "Step 1365: train loss: 1.3125505447387695\n",
      "Step 1365: val loss: 1.2714530229568481\n",
      "Step 1366: train loss: 1.3105844259262085\n",
      "Step 1366: val loss: 1.2695156335830688\n",
      "Step 1367: train loss: 1.3086223602294922\n",
      "Step 1367: val loss: 1.267581820487976\n",
      "Step 1368: train loss: 1.3066630363464355\n",
      "Step 1368: val loss: 1.265650987625122\n",
      "Step 1369: train loss: 1.3047077655792236\n",
      "Step 1369: val loss: 1.263724446296692\n",
      "Step 1370: train loss: 1.3027559518814087\n",
      "Step 1370: val loss: 1.2618014812469482\n",
      "Step 1371: train loss: 1.3008081912994385\n",
      "Step 1371: val loss: 1.2598819732666016\n",
      "Step 1372: train loss: 1.2988637685775757\n",
      "Step 1372: val loss: 1.2579660415649414\n",
      "Step 1373: train loss: 1.2969226837158203\n",
      "Step 1373: val loss: 1.2560536861419678\n",
      "Step 1374: train loss: 1.2949856519699097\n",
      "Step 1374: val loss: 1.2541450262069702\n",
      "Step 1375: train loss: 1.2930519580841064\n",
      "Step 1375: val loss: 1.25223970413208\n",
      "Step 1376: train loss: 1.2911217212677002\n",
      "Step 1376: val loss: 1.250338077545166\n",
      "Step 1377: train loss: 1.28919517993927\n",
      "Step 1377: val loss: 1.2484402656555176\n",
      "Step 1378: train loss: 1.2872719764709473\n",
      "Step 1378: val loss: 1.246545672416687\n",
      "Step 1379: train loss: 1.2853527069091797\n",
      "Step 1379: val loss: 1.2446550130844116\n",
      "Step 1380: train loss: 1.283436894416809\n",
      "Step 1380: val loss: 1.2427676916122437\n",
      "Step 1381: train loss: 1.281524419784546\n",
      "Step 1381: val loss: 1.2408835887908936\n",
      "Step 1382: train loss: 1.2796156406402588\n",
      "Step 1382: val loss: 1.2390034198760986\n",
      "Step 1383: train loss: 1.2777100801467896\n",
      "Step 1383: val loss: 1.2371267080307007\n",
      "Step 1384: train loss: 1.2758084535598755\n",
      "Step 1384: val loss: 1.2352536916732788\n",
      "Step 1385: train loss: 1.273910403251648\n",
      "Step 1385: val loss: 1.2333836555480957\n",
      "Step 1386: train loss: 1.2720154523849487\n",
      "Step 1386: val loss: 1.2315174341201782\n",
      "Step 1387: train loss: 1.2701241970062256\n",
      "Step 1387: val loss: 1.2296545505523682\n",
      "Step 1388: train loss: 1.2682362794876099\n",
      "Step 1388: val loss: 1.2277953624725342\n",
      "Step 1389: train loss: 1.2663519382476807\n",
      "Step 1389: val loss: 1.2259392738342285\n",
      "Step 1390: train loss: 1.264471173286438\n",
      "Step 1390: val loss: 1.2240869998931885\n",
      "Step 1391: train loss: 1.2625938653945923\n",
      "Step 1391: val loss: 1.222238540649414\n",
      "Step 1392: train loss: 1.2607202529907227\n",
      "Step 1392: val loss: 1.220393180847168\n",
      "Step 1393: train loss: 1.2588496208190918\n",
      "Step 1393: val loss: 1.2185512781143188\n",
      "Step 1394: train loss: 1.2569829225540161\n",
      "Step 1394: val loss: 1.2167127132415771\n",
      "Step 1395: train loss: 1.2551190853118896\n",
      "Step 1395: val loss: 1.214877963066101\n",
      "Step 1396: train loss: 1.2532590627670288\n",
      "Step 1396: val loss: 1.2130461931228638\n",
      "Step 1397: train loss: 1.2514023780822754\n",
      "Step 1397: val loss: 1.2112178802490234\n",
      "Step 1398: train loss: 1.2495490312576294\n",
      "Step 1398: val loss: 1.2093931436538696\n",
      "Step 1399: train loss: 1.247699499130249\n",
      "Step 1399: val loss: 1.2075719833374023\n",
      "Step 1400: train loss: 1.2458531856536865\n",
      "Step 1400: val loss: 1.205754280090332\n",
      "Step 1401: train loss: 1.244010329246521\n",
      "Step 1401: val loss: 1.20393967628479\n",
      "Step 1402: train loss: 1.242170810699463\n",
      "Step 1402: val loss: 1.2021287679672241\n",
      "Step 1403: train loss: 1.2403347492218018\n",
      "Step 1403: val loss: 1.200321078300476\n",
      "Step 1404: train loss: 1.238502025604248\n",
      "Step 1404: val loss: 1.198516607284546\n",
      "Step 1405: train loss: 1.2366725206375122\n",
      "Step 1405: val loss: 1.196716070175171\n",
      "Step 1406: train loss: 1.2348464727401733\n",
      "Step 1406: val loss: 1.1949185132980347\n",
      "Step 1407: train loss: 1.2330238819122314\n",
      "Step 1407: val loss: 1.1931244134902954\n",
      "Step 1408: train loss: 1.231204628944397\n",
      "Step 1408: val loss: 1.1913331747055054\n",
      "Step 1409: train loss: 1.2293883562088013\n",
      "Step 1409: val loss: 1.1895456314086914\n",
      "Step 1410: train loss: 1.2275755405426025\n",
      "Step 1410: val loss: 1.1877613067626953\n",
      "Step 1411: train loss: 1.225766658782959\n",
      "Step 1411: val loss: 1.1859806776046753\n",
      "Step 1412: train loss: 1.2239603996276855\n",
      "Step 1412: val loss: 1.1842031478881836\n",
      "Step 1413: train loss: 1.2221577167510986\n",
      "Step 1413: val loss: 1.1824290752410889\n",
      "Step 1414: train loss: 1.2203587293624878\n",
      "Step 1414: val loss: 1.1806584596633911\n",
      "Step 1415: train loss: 1.2185629606246948\n",
      "Step 1415: val loss: 1.1788910627365112\n",
      "Step 1416: train loss: 1.2167706489562988\n",
      "Step 1416: val loss: 1.1771273612976074\n",
      "Step 1417: train loss: 1.2149814367294312\n",
      "Step 1417: val loss: 1.1753664016723633\n",
      "Step 1418: train loss: 1.213195562362671\n",
      "Step 1418: val loss: 1.1736090183258057\n",
      "Step 1419: train loss: 1.2114133834838867\n",
      "Step 1419: val loss: 1.1718549728393555\n",
      "Step 1420: train loss: 1.2096340656280518\n",
      "Step 1420: val loss: 1.1701043844223022\n",
      "Step 1421: train loss: 1.2078582048416138\n",
      "Step 1421: val loss: 1.1683567762374878\n",
      "Step 1422: train loss: 1.206085205078125\n",
      "Step 1422: val loss: 1.1666125059127808\n",
      "Step 1423: train loss: 1.2043159008026123\n",
      "Step 1423: val loss: 1.1648715734481812\n",
      "Step 1424: train loss: 1.2025498151779175\n",
      "Step 1424: val loss: 1.1631336212158203\n",
      "Step 1425: train loss: 1.2007865905761719\n",
      "Step 1425: val loss: 1.161399006843567\n",
      "Step 1426: train loss: 1.1990268230438232\n",
      "Step 1426: val loss: 1.159667730331421\n",
      "Step 1427: train loss: 1.1972702741622925\n",
      "Step 1427: val loss: 1.1579394340515137\n",
      "Step 1428: train loss: 1.19551682472229\n",
      "Step 1428: val loss: 1.1562143564224243\n",
      "Step 1429: train loss: 1.193766713142395\n",
      "Step 1429: val loss: 1.1544928550720215\n",
      "Step 1430: train loss: 1.192020058631897\n",
      "Step 1430: val loss: 1.1527743339538574\n",
      "Step 1431: train loss: 1.1902762651443481\n",
      "Step 1431: val loss: 1.1510590314865112\n",
      "Step 1432: train loss: 1.1885359287261963\n",
      "Step 1432: val loss: 1.1493473052978516\n",
      "Step 1433: train loss: 1.1867986917495728\n",
      "Step 1433: val loss: 1.1476383209228516\n",
      "Step 1434: train loss: 1.185064673423767\n",
      "Step 1434: val loss: 1.1459331512451172\n",
      "Step 1435: train loss: 1.1833341121673584\n",
      "Step 1435: val loss: 1.144230604171753\n",
      "Step 1436: train loss: 1.1816065311431885\n",
      "Step 1436: val loss: 1.1425316333770752\n",
      "Step 1437: train loss: 1.1798824071884155\n",
      "Step 1437: val loss: 1.1408356428146362\n",
      "Step 1438: train loss: 1.178161382675171\n",
      "Step 1438: val loss: 1.1391432285308838\n",
      "Step 1439: train loss: 1.176443338394165\n",
      "Step 1439: val loss: 1.137453556060791\n",
      "Step 1440: train loss: 1.1747287511825562\n",
      "Step 1440: val loss: 1.1357671022415161\n",
      "Step 1441: train loss: 1.1730167865753174\n",
      "Step 1441: val loss: 1.1340837478637695\n",
      "Step 1442: train loss: 1.1713083982467651\n",
      "Step 1442: val loss: 1.1324034929275513\n",
      "Step 1443: train loss: 1.1696029901504517\n",
      "Step 1443: val loss: 1.1307265758514404\n",
      "Step 1444: train loss: 1.167900800704956\n",
      "Step 1444: val loss: 1.129052758216858\n",
      "Step 1445: train loss: 1.1662017107009888\n",
      "Step 1445: val loss: 1.1273819208145142\n",
      "Step 1446: train loss: 1.1645056009292603\n",
      "Step 1446: val loss: 1.1257145404815674\n",
      "Step 1447: train loss: 1.1628130674362183\n",
      "Step 1447: val loss: 1.1240501403808594\n",
      "Step 1448: train loss: 1.161123514175415\n",
      "Step 1448: val loss: 1.1223891973495483\n",
      "Step 1449: train loss: 1.1594372987747192\n",
      "Step 1449: val loss: 1.120730996131897\n",
      "Step 1450: train loss: 1.157753825187683\n",
      "Step 1450: val loss: 1.1190757751464844\n",
      "Step 1451: train loss: 1.1560733318328857\n",
      "Step 1451: val loss: 1.117423415184021\n",
      "Step 1452: train loss: 1.1543959379196167\n",
      "Step 1452: val loss: 1.1157748699188232\n",
      "Step 1453: train loss: 1.1527220010757446\n",
      "Step 1453: val loss: 1.1141291856765747\n",
      "Step 1454: train loss: 1.1510510444641113\n",
      "Step 1454: val loss: 1.1124868392944336\n",
      "Step 1455: train loss: 1.1493831872940063\n",
      "Step 1455: val loss: 1.1108468770980835\n",
      "Step 1456: train loss: 1.1477183103561401\n",
      "Step 1456: val loss: 1.1092103719711304\n",
      "Step 1457: train loss: 1.1460564136505127\n",
      "Step 1457: val loss: 1.107576847076416\n",
      "Step 1458: train loss: 1.144397497177124\n",
      "Step 1458: val loss: 1.1059465408325195\n",
      "Step 1459: train loss: 1.1427419185638428\n",
      "Step 1459: val loss: 1.1043193340301514\n",
      "Step 1460: train loss: 1.1410894393920898\n",
      "Step 1460: val loss: 1.1026948690414429\n",
      "Step 1461: train loss: 1.1394399404525757\n",
      "Step 1461: val loss: 1.1010738611221313\n",
      "Step 1462: train loss: 1.1377934217453003\n",
      "Step 1462: val loss: 1.09945547580719\n",
      "Step 1463: train loss: 1.1361498832702637\n",
      "Step 1463: val loss: 1.0978403091430664\n",
      "Step 1464: train loss: 1.134509563446045\n",
      "Step 1464: val loss: 1.0962283611297607\n",
      "Step 1465: train loss: 1.1328719854354858\n",
      "Step 1465: val loss: 1.0946192741394043\n",
      "Step 1466: train loss: 1.1312378644943237\n",
      "Step 1466: val loss: 1.0930132865905762\n",
      "Step 1467: train loss: 1.1296064853668213\n",
      "Step 1467: val loss: 1.0914102792739868\n",
      "Step 1468: train loss: 1.1279780864715576\n",
      "Step 1468: val loss: 1.0898100137710571\n",
      "Step 1469: train loss: 1.1263526678085327\n",
      "Step 1469: val loss: 1.0882130861282349\n",
      "Step 1470: train loss: 1.1247303485870361\n",
      "Step 1470: val loss: 1.0866190195083618\n",
      "Step 1471: train loss: 1.1231110095977783\n",
      "Step 1471: val loss: 1.085027813911438\n",
      "Step 1472: train loss: 1.1214946508407593\n",
      "Step 1472: val loss: 1.083439826965332\n",
      "Step 1473: train loss: 1.119881272315979\n",
      "Step 1473: val loss: 1.0818549394607544\n",
      "Step 1474: train loss: 1.118270993232727\n",
      "Step 1474: val loss: 1.0802727937698364\n",
      "Step 1475: train loss: 1.1166635751724243\n",
      "Step 1475: val loss: 1.0786935091018677\n",
      "Step 1476: train loss: 1.1150590181350708\n",
      "Step 1476: val loss: 1.0771172046661377\n",
      "Step 1477: train loss: 1.1134573221206665\n",
      "Step 1477: val loss: 1.0755443572998047\n",
      "Step 1478: train loss: 1.1118589639663696\n",
      "Step 1478: val loss: 1.0739737749099731\n",
      "Step 1479: train loss: 1.1102632284164429\n",
      "Step 1479: val loss: 1.0724064111709595\n",
      "Step 1480: train loss: 1.108670711517334\n",
      "Step 1480: val loss: 1.0708421468734741\n",
      "Step 1481: train loss: 1.1070809364318848\n",
      "Step 1481: val loss: 1.0692803859710693\n",
      "Step 1482: train loss: 1.1054939031600952\n",
      "Step 1482: val loss: 1.0677220821380615\n",
      "Step 1483: train loss: 1.103909969329834\n",
      "Step 1483: val loss: 1.0661664009094238\n",
      "Step 1484: train loss: 1.1023292541503906\n",
      "Step 1484: val loss: 1.064613938331604\n",
      "Step 1485: train loss: 1.1007511615753174\n",
      "Step 1485: val loss: 1.0630638599395752\n",
      "Step 1486: train loss: 1.099176049232483\n",
      "Step 1486: val loss: 1.0615168809890747\n",
      "Step 1487: train loss: 1.0976037979125977\n",
      "Step 1487: val loss: 1.0599730014801025\n",
      "Step 1488: train loss: 1.0960344076156616\n",
      "Step 1488: val loss: 1.05843186378479\n",
      "Step 1489: train loss: 1.0944679975509644\n",
      "Step 1489: val loss: 1.0568938255310059\n",
      "Step 1490: train loss: 1.0929045677185059\n",
      "Step 1490: val loss: 1.0553585290908813\n",
      "Step 1491: train loss: 1.0913439989089966\n",
      "Step 1491: val loss: 1.0538259744644165\n",
      "Step 1492: train loss: 1.0897859334945679\n",
      "Step 1492: val loss: 1.0522964000701904\n",
      "Step 1493: train loss: 1.0882312059402466\n",
      "Step 1493: val loss: 1.0507699251174927\n",
      "Step 1494: train loss: 1.086679220199585\n",
      "Step 1494: val loss: 1.049245834350586\n",
      "Step 1495: train loss: 1.0851298570632935\n",
      "Step 1495: val loss: 1.0477250814437866\n",
      "Step 1496: train loss: 1.0835834741592407\n",
      "Step 1496: val loss: 1.0462068319320679\n",
      "Step 1497: train loss: 1.0820401906967163\n",
      "Step 1497: val loss: 1.0446916818618774\n",
      "Step 1498: train loss: 1.0804994106292725\n",
      "Step 1498: val loss: 1.0431787967681885\n",
      "Step 1499: train loss: 1.0789616107940674\n",
      "Step 1499: val loss: 1.0416699647903442\n",
      "Step 1500: train loss: 1.0774269104003906\n",
      "Step 1500: val loss: 1.0401630401611328\n",
      "Step 1501: train loss: 1.0758945941925049\n",
      "Step 1501: val loss: 1.0386593341827393\n",
      "Step 1502: train loss: 1.074365496635437\n",
      "Step 1502: val loss: 1.0371578931808472\n",
      "Step 1503: train loss: 1.0728389024734497\n",
      "Step 1503: val loss: 1.035659670829773\n",
      "Step 1504: train loss: 1.0713152885437012\n",
      "Step 1504: val loss: 1.034164309501648\n",
      "Step 1505: train loss: 1.0697945356369019\n",
      "Step 1505: val loss: 1.0326714515686035\n",
      "Step 1506: train loss: 1.068276286125183\n",
      "Step 1506: val loss: 1.0311815738677979\n",
      "Step 1507: train loss: 1.0667611360549927\n",
      "Step 1507: val loss: 1.0296945571899414\n",
      "Step 1508: train loss: 1.065248727798462\n",
      "Step 1508: val loss: 1.0282104015350342\n",
      "Step 1509: train loss: 1.06373929977417\n",
      "Step 1509: val loss: 1.0267289876937866\n",
      "Step 1510: train loss: 1.0622321367263794\n",
      "Step 1510: val loss: 1.0252503156661987\n",
      "Step 1511: train loss: 1.0607283115386963\n",
      "Step 1511: val loss: 1.0237743854522705\n",
      "Step 1512: train loss: 1.0592267513275146\n",
      "Step 1512: val loss: 1.022301197052002\n",
      "Step 1513: train loss: 1.0577282905578613\n",
      "Step 1513: val loss: 1.0208308696746826\n",
      "Step 1514: train loss: 1.0562324523925781\n",
      "Step 1514: val loss: 1.0193631649017334\n",
      "Step 1515: train loss: 1.0547395944595337\n",
      "Step 1515: val loss: 1.017898440361023\n",
      "Step 1516: train loss: 1.0532492399215698\n",
      "Step 1516: val loss: 1.0164363384246826\n",
      "Step 1517: train loss: 1.0517619848251343\n",
      "Step 1517: val loss: 1.0149768590927124\n",
      "Step 1518: train loss: 1.0502771139144897\n",
      "Step 1518: val loss: 1.0135204792022705\n",
      "Step 1519: train loss: 1.0487948656082153\n",
      "Step 1519: val loss: 1.01206636428833\n",
      "Step 1520: train loss: 1.0473157167434692\n",
      "Step 1520: val loss: 1.010615348815918\n",
      "Step 1521: train loss: 1.0458390712738037\n",
      "Step 1521: val loss: 1.0091663599014282\n",
      "Step 1522: train loss: 1.0443650484085083\n",
      "Step 1522: val loss: 1.007720708847046\n",
      "Step 1523: train loss: 1.042893886566162\n",
      "Step 1523: val loss: 1.0062779188156128\n",
      "Step 1524: train loss: 1.0414257049560547\n",
      "Step 1524: val loss: 1.0048378705978394\n",
      "Step 1525: train loss: 1.0399599075317383\n",
      "Step 1525: val loss: 1.0033999681472778\n",
      "Step 1526: train loss: 1.038496971130371\n",
      "Step 1526: val loss: 1.001965045928955\n",
      "Step 1527: train loss: 1.037036657333374\n",
      "Step 1527: val loss: 1.000533103942871\n",
      "Step 1528: train loss: 1.0355790853500366\n",
      "Step 1528: val loss: 0.9991036057472229\n",
      "Step 1529: train loss: 1.0341241359710693\n",
      "Step 1529: val loss: 0.9976767897605896\n",
      "Step 1530: train loss: 1.0326721668243408\n",
      "Step 1530: val loss: 0.9962524771690369\n",
      "Step 1531: train loss: 1.0312224626541138\n",
      "Step 1531: val loss: 0.9948309659957886\n",
      "Step 1532: train loss: 1.029775619506836\n",
      "Step 1532: val loss: 0.9934123754501343\n",
      "Step 1533: train loss: 1.0283313989639282\n",
      "Step 1533: val loss: 0.9919965863227844\n",
      "Step 1534: train loss: 1.0268900394439697\n",
      "Step 1534: val loss: 0.9905828833580017\n",
      "Step 1535: train loss: 1.0254510641098022\n",
      "Step 1535: val loss: 0.9891722202301025\n",
      "Step 1536: train loss: 1.024015188217163\n",
      "Step 1536: val loss: 0.9877640008926392\n",
      "Step 1537: train loss: 1.0225813388824463\n",
      "Step 1537: val loss: 0.9863585233688354\n",
      "Step 1538: train loss: 1.0211503505706787\n",
      "Step 1538: val loss: 0.9849556088447571\n",
      "Step 1539: train loss: 1.0197221040725708\n",
      "Step 1539: val loss: 0.9835553169250488\n",
      "Step 1540: train loss: 1.0182963609695435\n",
      "Step 1540: val loss: 0.9821578860282898\n",
      "Step 1541: train loss: 1.0168733596801758\n",
      "Step 1541: val loss: 0.9807626008987427\n",
      "Step 1542: train loss: 1.0154526233673096\n",
      "Step 1542: val loss: 0.9793702960014343\n",
      "Step 1543: train loss: 1.0140349864959717\n",
      "Step 1543: val loss: 0.9779804348945618\n",
      "Step 1544: train loss: 1.0126196146011353\n",
      "Step 1544: val loss: 0.9765931367874146\n",
      "Step 1545: train loss: 1.0112069845199585\n",
      "Step 1545: val loss: 0.9752086997032166\n",
      "Step 1546: train loss: 1.0097970962524414\n",
      "Step 1546: val loss: 0.9738268852233887\n",
      "Step 1547: train loss: 1.0083897113800049\n",
      "Step 1547: val loss: 0.9724473357200623\n",
      "Step 1548: train loss: 1.0069849491119385\n",
      "Step 1548: val loss: 0.971070408821106\n",
      "Step 1549: train loss: 1.0055826902389526\n",
      "Step 1549: val loss: 0.9696965217590332\n",
      "Step 1550: train loss: 1.004183053970337\n",
      "Step 1550: val loss: 0.9683248400688171\n",
      "Step 1551: train loss: 1.0027860403060913\n",
      "Step 1551: val loss: 0.9669559597969055\n",
      "Step 1552: train loss: 1.0013917684555054\n",
      "Step 1552: val loss: 0.965589165687561\n",
      "Step 1553: train loss: 0.9999995231628418\n",
      "Step 1553: val loss: 0.964225172996521\n",
      "Step 1554: train loss: 0.9986101388931274\n",
      "Step 1554: val loss: 0.9628640413284302\n",
      "Step 1555: train loss: 0.9972233176231384\n",
      "Step 1555: val loss: 0.961505115032196\n",
      "Step 1556: train loss: 0.9958391189575195\n",
      "Step 1556: val loss: 0.9601489305496216\n",
      "Step 1557: train loss: 0.9944575428962708\n",
      "Step 1557: val loss: 0.9587953090667725\n",
      "Step 1558: train loss: 0.9930784702301025\n",
      "Step 1558: val loss: 0.9574439525604248\n",
      "Step 1559: train loss: 0.991701602935791\n",
      "Step 1559: val loss: 0.9560955762863159\n",
      "Step 1560: train loss: 0.990327775478363\n",
      "Step 1560: val loss: 0.954749345779419\n",
      "Step 1561: train loss: 0.9889560341835022\n",
      "Step 1561: val loss: 0.9534058570861816\n",
      "Step 1562: train loss: 0.9875872731208801\n",
      "Step 1562: val loss: 0.9520649909973145\n",
      "Step 1563: train loss: 0.9862208366394043\n",
      "Step 1563: val loss: 0.9507263898849487\n",
      "Step 1564: train loss: 0.9848567843437195\n",
      "Step 1564: val loss: 0.9493903517723083\n",
      "Step 1565: train loss: 0.9834954738616943\n",
      "Step 1565: val loss: 0.9480569362640381\n",
      "Step 1566: train loss: 0.9821364879608154\n",
      "Step 1566: val loss: 0.9467259049415588\n",
      "Step 1567: train loss: 0.9807800054550171\n",
      "Step 1567: val loss: 0.9453974366188049\n",
      "Step 1568: train loss: 0.9794262051582336\n",
      "Step 1568: val loss: 0.9440714716911316\n",
      "Step 1569: train loss: 0.978074848651886\n",
      "Step 1569: val loss: 0.9427481293678284\n",
      "Step 1570: train loss: 0.976725697517395\n",
      "Step 1570: val loss: 0.9414270520210266\n",
      "Step 1571: train loss: 0.9753794074058533\n",
      "Step 1571: val loss: 0.9401086568832397\n",
      "Step 1572: train loss: 0.9740354418754578\n",
      "Step 1572: val loss: 0.9387922883033752\n",
      "Step 1573: train loss: 0.9726937413215637\n",
      "Step 1573: val loss: 0.9374788999557495\n",
      "Step 1574: train loss: 0.9713548421859741\n",
      "Step 1574: val loss: 0.9361677765846252\n",
      "Step 1575: train loss: 0.9700182676315308\n",
      "Step 1575: val loss: 0.934859037399292\n",
      "Step 1576: train loss: 0.9686842560768127\n",
      "Step 1576: val loss: 0.9335528612136841\n",
      "Step 1577: train loss: 0.9673523902893066\n",
      "Step 1577: val loss: 0.9322493076324463\n",
      "Step 1578: train loss: 0.966023325920105\n",
      "Step 1578: val loss: 0.93094801902771\n",
      "Step 1579: train loss: 0.9646967053413391\n",
      "Step 1579: val loss: 0.9296491742134094\n",
      "Step 1580: train loss: 0.9633724093437195\n",
      "Step 1580: val loss: 0.9283527135848999\n",
      "Step 1581: train loss: 0.9620506167411804\n",
      "Step 1581: val loss: 0.927058756351471\n",
      "Step 1582: train loss: 0.9607312679290771\n",
      "Step 1582: val loss: 0.9257670640945435\n",
      "Step 1583: train loss: 0.9594140648841858\n",
      "Step 1583: val loss: 0.9244781136512756\n",
      "Step 1584: train loss: 0.9580996036529541\n",
      "Step 1584: val loss: 0.9231915473937988\n",
      "Step 1585: train loss: 0.9567874670028687\n",
      "Step 1585: val loss: 0.9219071269035339\n",
      "Step 1586: train loss: 0.9554778337478638\n",
      "Step 1586: val loss: 0.920625627040863\n",
      "Step 1587: train loss: 0.9541705846786499\n",
      "Step 1587: val loss: 0.9193461537361145\n",
      "Step 1588: train loss: 0.9528657793998718\n",
      "Step 1588: val loss: 0.9180688858032227\n",
      "Step 1589: train loss: 0.9515633583068848\n",
      "Step 1589: val loss: 0.9167943000793457\n",
      "Step 1590: train loss: 0.9502632021903992\n",
      "Step 1590: val loss: 0.9155221581459045\n",
      "Step 1591: train loss: 0.9489654302597046\n",
      "Step 1591: val loss: 0.9142523407936096\n",
      "Step 1592: train loss: 0.9476704597473145\n",
      "Step 1592: val loss: 0.9129849672317505\n",
      "Step 1593: train loss: 0.9463774561882019\n",
      "Step 1593: val loss: 0.911719799041748\n",
      "Step 1594: train loss: 0.9450868964195251\n",
      "Step 1594: val loss: 0.9104572534561157\n",
      "Step 1595: train loss: 0.9437987804412842\n",
      "Step 1595: val loss: 0.9091969132423401\n",
      "Step 1596: train loss: 0.9425130486488342\n",
      "Step 1596: val loss: 0.9079388380050659\n",
      "Step 1597: train loss: 0.9412294626235962\n",
      "Step 1597: val loss: 0.9066833853721619\n",
      "Step 1598: train loss: 0.939948558807373\n",
      "Step 1598: val loss: 0.9054298996925354\n",
      "Step 1599: train loss: 0.938669741153717\n",
      "Step 1599: val loss: 0.9041791558265686\n",
      "Step 1600: train loss: 0.9373934864997864\n",
      "Step 1600: val loss: 0.9029306173324585\n",
      "Step 1601: train loss: 0.936119556427002\n",
      "Step 1601: val loss: 0.901684582233429\n",
      "Step 1602: train loss: 0.934847891330719\n",
      "Step 1602: val loss: 0.9004406332969666\n",
      "Step 1603: train loss: 0.933578610420227\n",
      "Step 1603: val loss: 0.8991992473602295\n",
      "Step 1604: train loss: 0.9323116540908813\n",
      "Step 1604: val loss: 0.8979598879814148\n",
      "Step 1605: train loss: 0.9310470819473267\n",
      "Step 1605: val loss: 0.8967231512069702\n",
      "Step 1606: train loss: 0.9297847747802734\n",
      "Step 1606: val loss: 0.8954886794090271\n",
      "Step 1607: train loss: 0.9285248517990112\n",
      "Step 1607: val loss: 0.8942564725875854\n",
      "Step 1608: train loss: 0.9272670149803162\n",
      "Step 1608: val loss: 0.8930262923240662\n",
      "Step 1609: train loss: 0.9260116815567017\n",
      "Step 1609: val loss: 0.8917990326881409\n",
      "Step 1610: train loss: 0.9247589111328125\n",
      "Step 1610: val loss: 0.8905737400054932\n",
      "Step 1611: train loss: 0.9235080480575562\n",
      "Step 1611: val loss: 0.8893506526947021\n",
      "Step 1612: train loss: 0.9222596883773804\n",
      "Step 1612: val loss: 0.8881301879882812\n",
      "Step 1613: train loss: 0.921013593673706\n",
      "Step 1613: val loss: 0.8869116902351379\n",
      "Step 1614: train loss: 0.9197698831558228\n",
      "Step 1614: val loss: 0.8856956362724304\n",
      "Step 1615: train loss: 0.9185282588005066\n",
      "Step 1615: val loss: 0.8844819068908691\n",
      "Step 1616: train loss: 0.9172890782356262\n",
      "Step 1616: val loss: 0.8832704424858093\n",
      "Step 1617: train loss: 0.9160521626472473\n",
      "Step 1617: val loss: 0.8820613026618958\n",
      "Step 1618: train loss: 0.9148175716400146\n",
      "Step 1618: val loss: 0.8808541297912598\n",
      "Step 1619: train loss: 0.9135850071907043\n",
      "Step 1619: val loss: 0.8796495199203491\n",
      "Step 1620: train loss: 0.9123551845550537\n",
      "Step 1620: val loss: 0.8784470558166504\n",
      "Step 1621: train loss: 0.9111270904541016\n",
      "Step 1621: val loss: 0.8772466778755188\n",
      "Step 1622: train loss: 0.9099012613296509\n",
      "Step 1622: val loss: 0.8760489821434021\n",
      "Step 1623: train loss: 0.9086782336235046\n",
      "Step 1623: val loss: 0.8748534917831421\n",
      "Step 1624: train loss: 0.907457172870636\n",
      "Step 1624: val loss: 0.8736600875854492\n",
      "Step 1625: train loss: 0.9062382578849792\n",
      "Step 1625: val loss: 0.8724690079689026\n",
      "Step 1626: train loss: 0.9050218462944031\n",
      "Step 1626: val loss: 0.8712798953056335\n",
      "Step 1627: train loss: 0.9038074016571045\n",
      "Step 1627: val loss: 0.8700933456420898\n",
      "Step 1628: train loss: 0.9025953412055969\n",
      "Step 1628: val loss: 0.8689088225364685\n",
      "Step 1629: train loss: 0.901385486125946\n",
      "Step 1629: val loss: 0.8677266240119934\n",
      "Step 1630: train loss: 0.9001776576042175\n",
      "Step 1630: val loss: 0.8665463328361511\n",
      "Step 1631: train loss: 0.8989721536636353\n",
      "Step 1631: val loss: 0.8653685450553894\n",
      "Step 1632: train loss: 0.8977689146995544\n",
      "Step 1632: val loss: 0.8641930222511292\n",
      "Step 1633: train loss: 0.8965679407119751\n",
      "Step 1633: val loss: 0.8630196452140808\n",
      "Step 1634: train loss: 0.8953693509101868\n",
      "Step 1634: val loss: 0.861848771572113\n",
      "Step 1635: train loss: 0.894172728061676\n",
      "Step 1635: val loss: 0.8606796264648438\n",
      "Step 1636: train loss: 0.892978310585022\n",
      "Step 1636: val loss: 0.8595129251480103\n",
      "Step 1637: train loss: 0.8917862772941589\n",
      "Step 1637: val loss: 0.8583484888076782\n",
      "Step 1638: train loss: 0.890596330165863\n",
      "Step 1638: val loss: 0.857185959815979\n",
      "Step 1639: train loss: 0.889408528804779\n",
      "Step 1639: val loss: 0.8560258746147156\n",
      "Step 1640: train loss: 0.8882230520248413\n",
      "Step 1640: val loss: 0.8548678755760193\n",
      "Step 1641: train loss: 0.8870393633842468\n",
      "Step 1641: val loss: 0.8537118434906006\n",
      "Step 1642: train loss: 0.8858581781387329\n",
      "Step 1642: val loss: 0.8525583744049072\n",
      "Step 1643: train loss: 0.8846790194511414\n",
      "Step 1643: val loss: 0.8514066338539124\n",
      "Step 1644: train loss: 0.8835020661354065\n",
      "Step 1644: val loss: 0.8502572774887085\n",
      "Step 1645: train loss: 0.8823271989822388\n",
      "Step 1645: val loss: 0.8491101264953613\n",
      "Step 1646: train loss: 0.8811547756195068\n",
      "Step 1646: val loss: 0.8479650616645813\n",
      "Step 1647: train loss: 0.8799841403961182\n",
      "Step 1647: val loss: 0.8468219637870789\n",
      "Step 1648: train loss: 0.8788157105445862\n",
      "Step 1648: val loss: 0.845681369304657\n",
      "Step 1649: train loss: 0.8776496052742004\n",
      "Step 1649: val loss: 0.8445425033569336\n",
      "Step 1650: train loss: 0.8764854073524475\n",
      "Step 1650: val loss: 0.8434059619903564\n",
      "Step 1651: train loss: 0.8753235340118408\n",
      "Step 1651: val loss: 0.8422718644142151\n",
      "Step 1652: train loss: 0.8741639256477356\n",
      "Step 1652: val loss: 0.8411397337913513\n",
      "Step 1653: train loss: 0.8730067014694214\n",
      "Step 1653: val loss: 0.840009868144989\n",
      "Step 1654: train loss: 0.8718513250350952\n",
      "Step 1654: val loss: 0.8388820886611938\n",
      "Step 1655: train loss: 0.870698094367981\n",
      "Step 1655: val loss: 0.8377565741539001\n",
      "Step 1656: train loss: 0.8695472478866577\n",
      "Step 1656: val loss: 0.8366329669952393\n",
      "Step 1657: train loss: 0.8683982491493225\n",
      "Step 1657: val loss: 0.8355114459991455\n",
      "Step 1658: train loss: 0.8672513961791992\n",
      "Step 1658: val loss: 0.8343920707702637\n",
      "Step 1659: train loss: 0.8661065697669983\n",
      "Step 1659: val loss: 0.8332747220993042\n",
      "Step 1660: train loss: 0.8649636507034302\n",
      "Step 1660: val loss: 0.8321592211723328\n",
      "Step 1661: train loss: 0.8638231158256531\n",
      "Step 1661: val loss: 0.8310462236404419\n",
      "Step 1662: train loss: 0.8626843690872192\n",
      "Step 1662: val loss: 0.8299350738525391\n",
      "Step 1663: train loss: 0.8615479469299316\n",
      "Step 1663: val loss: 0.8288261294364929\n",
      "Step 1664: train loss: 0.8604134321212769\n",
      "Step 1664: val loss: 0.8277191519737244\n",
      "Step 1665: train loss: 0.8592812418937683\n",
      "Step 1665: val loss: 0.8266142010688782\n",
      "Step 1666: train loss: 0.8581508994102478\n",
      "Step 1666: val loss: 0.8255112171173096\n",
      "Step 1667: train loss: 0.8570229411125183\n",
      "Step 1667: val loss: 0.8244110345840454\n",
      "Step 1668: train loss: 0.8558968901634216\n",
      "Step 1668: val loss: 0.8233122229576111\n",
      "Step 1669: train loss: 0.8547729849815369\n",
      "Step 1669: val loss: 0.8222158551216125\n",
      "Step 1670: train loss: 0.8536510467529297\n",
      "Step 1670: val loss: 0.8211213946342468\n",
      "Step 1671: train loss: 0.8525311946868896\n",
      "Step 1671: val loss: 0.8200289011001587\n",
      "Step 1672: train loss: 0.8514136075973511\n",
      "Step 1672: val loss: 0.8189387321472168\n",
      "Step 1673: train loss: 0.8502979278564453\n",
      "Step 1673: val loss: 0.8178505897521973\n",
      "Step 1674: train loss: 0.8491843342781067\n",
      "Step 1674: val loss: 0.8167644143104553\n",
      "Step 1675: train loss: 0.8480728268623352\n",
      "Step 1675: val loss: 0.8156800270080566\n",
      "Step 1676: train loss: 0.8469634056091309\n",
      "Step 1676: val loss: 0.8145982027053833\n",
      "Step 1677: train loss: 0.8458560705184937\n",
      "Step 1677: val loss: 0.8135184049606323\n",
      "Step 1678: train loss: 0.8447507619857788\n",
      "Step 1678: val loss: 0.8124401569366455\n",
      "Step 1679: train loss: 0.843647301197052\n",
      "Step 1679: val loss: 0.811363935470581\n",
      "Step 1680: train loss: 0.8425458669662476\n",
      "Step 1680: val loss: 0.8102899193763733\n",
      "Step 1681: train loss: 0.8414463400840759\n",
      "Step 1681: val loss: 0.8092179298400879\n",
      "Step 1682: train loss: 0.840349018573761\n",
      "Step 1682: val loss: 0.8081480264663696\n",
      "Step 1683: train loss: 0.8392537236213684\n",
      "Step 1683: val loss: 0.8070800304412842\n",
      "Step 1684: train loss: 0.8381603360176086\n",
      "Step 1684: val loss: 0.8060140013694763\n",
      "Step 1685: train loss: 0.8370692133903503\n",
      "Step 1685: val loss: 0.8049501776695251\n",
      "Step 1686: train loss: 0.8359799385070801\n",
      "Step 1686: val loss: 0.8038884401321411\n",
      "Step 1687: train loss: 0.8348928093910217\n",
      "Step 1687: val loss: 0.8028283715248108\n",
      "Step 1688: train loss: 0.8338077664375305\n",
      "Step 1688: val loss: 0.8017706274986267\n",
      "Step 1689: train loss: 0.8327245116233826\n",
      "Step 1689: val loss: 0.8007146120071411\n",
      "Step 1690: train loss: 0.8316431641578674\n",
      "Step 1690: val loss: 0.7996606230735779\n",
      "Step 1691: train loss: 0.8305637836456299\n",
      "Step 1691: val loss: 0.798608660697937\n",
      "Step 1692: train loss: 0.8294864892959595\n",
      "Step 1692: val loss: 0.797558605670929\n",
      "Step 1693: train loss: 0.8284112811088562\n",
      "Step 1693: val loss: 0.7965108156204224\n",
      "Step 1694: train loss: 0.8273380398750305\n",
      "Step 1694: val loss: 0.7954646944999695\n",
      "Step 1695: train loss: 0.8262667059898376\n",
      "Step 1695: val loss: 0.7944205403327942\n",
      "Step 1696: train loss: 0.8251973390579224\n",
      "Step 1696: val loss: 0.7933784127235413\n",
      "Step 1697: train loss: 0.8241298794746399\n",
      "Step 1697: val loss: 0.7923382520675659\n",
      "Step 1698: train loss: 0.8230644464492798\n",
      "Step 1698: val loss: 0.7913002371788025\n",
      "Step 1699: train loss: 0.8220009207725525\n",
      "Step 1699: val loss: 0.7902639508247375\n",
      "Step 1700: train loss: 0.820939302444458\n",
      "Step 1700: val loss: 0.789229691028595\n",
      "Step 1701: train loss: 0.819879949092865\n",
      "Step 1701: val loss: 0.7881973385810852\n",
      "Step 1702: train loss: 0.8188222050666809\n",
      "Step 1702: val loss: 0.7871668338775635\n",
      "Step 1703: train loss: 0.8177664279937744\n",
      "Step 1703: val loss: 0.7861384153366089\n",
      "Step 1704: train loss: 0.8167127370834351\n",
      "Step 1704: val loss: 0.7851119041442871\n",
      "Step 1705: train loss: 0.8156608939170837\n",
      "Step 1705: val loss: 0.7840874791145325\n",
      "Step 1706: train loss: 0.8146111965179443\n",
      "Step 1706: val loss: 0.7830649018287659\n",
      "Step 1707: train loss: 0.813563346862793\n",
      "Step 1707: val loss: 0.7820439338684082\n",
      "Step 1708: train loss: 0.8125172257423401\n",
      "Step 1708: val loss: 0.7810252904891968\n",
      "Step 1709: train loss: 0.8114731907844543\n",
      "Step 1709: val loss: 0.7800084352493286\n",
      "Step 1710: train loss: 0.8104310631752014\n",
      "Step 1710: val loss: 0.7789933681488037\n",
      "Step 1711: train loss: 0.8093907833099365\n",
      "Step 1711: val loss: 0.7779802083969116\n",
      "Step 1712: train loss: 0.8083525896072388\n",
      "Step 1712: val loss: 0.7769691944122314\n",
      "Step 1713: train loss: 0.8073161244392395\n",
      "Step 1713: val loss: 0.7759597897529602\n",
      "Step 1714: train loss: 0.806281566619873\n",
      "Step 1714: val loss: 0.7749525308609009\n",
      "Step 1715: train loss: 0.805249035358429\n",
      "Step 1715: val loss: 0.7739473581314087\n",
      "Step 1716: train loss: 0.8042183518409729\n",
      "Step 1716: val loss: 0.7729435563087463\n",
      "Step 1717: train loss: 0.8031893968582153\n",
      "Step 1717: val loss: 0.7719419598579407\n",
      "Step 1718: train loss: 0.8021625876426697\n",
      "Step 1718: val loss: 0.7709421515464783\n",
      "Step 1719: train loss: 0.8011375665664673\n",
      "Step 1719: val loss: 0.7699441909790039\n",
      "Step 1720: train loss: 0.8001144528388977\n",
      "Step 1720: val loss: 0.7689482569694519\n",
      "Step 1721: train loss: 0.7990933060646057\n",
      "Step 1721: val loss: 0.7679541707038879\n",
      "Step 1722: train loss: 0.7980737686157227\n",
      "Step 1722: val loss: 0.7669618129730225\n",
      "Step 1723: train loss: 0.7970563769340515\n",
      "Step 1723: val loss: 0.7659714818000793\n",
      "Step 1724: train loss: 0.7960408926010132\n",
      "Step 1724: val loss: 0.764983057975769\n",
      "Step 1725: train loss: 0.7950271964073181\n",
      "Step 1725: val loss: 0.7639965415000916\n",
      "Step 1726: train loss: 0.7940152883529663\n",
      "Step 1726: val loss: 0.7630113363265991\n",
      "Step 1727: train loss: 0.7930050492286682\n",
      "Step 1727: val loss: 0.7620285153388977\n",
      "Step 1728: train loss: 0.791996955871582\n",
      "Step 1728: val loss: 0.7610472440719604\n",
      "Step 1729: train loss: 0.7909904718399048\n",
      "Step 1729: val loss: 0.7600681185722351\n",
      "Step 1730: train loss: 0.7899860143661499\n",
      "Step 1730: val loss: 0.759090781211853\n",
      "Step 1731: train loss: 0.7889834642410278\n",
      "Step 1731: val loss: 0.7581151723861694\n",
      "Step 1732: train loss: 0.7879827618598938\n",
      "Step 1732: val loss: 0.7571414709091187\n",
      "Step 1733: train loss: 0.7869839072227478\n",
      "Step 1733: val loss: 0.7561697959899902\n",
      "Step 1734: train loss: 0.7859869599342346\n",
      "Step 1734: val loss: 0.7551996111869812\n",
      "Step 1735: train loss: 0.7849916219711304\n",
      "Step 1735: val loss: 0.7542314529418945\n",
      "Step 1736: train loss: 0.7839980721473694\n",
      "Step 1736: val loss: 0.7532647252082825\n",
      "Step 1737: train loss: 0.7830063104629517\n",
      "Step 1737: val loss: 0.7523001432418823\n",
      "Step 1738: train loss: 0.7820165157318115\n",
      "Step 1738: val loss: 0.7513374090194702\n",
      "Step 1739: train loss: 0.7810285687446594\n",
      "Step 1739: val loss: 0.7503765225410461\n",
      "Step 1740: train loss: 0.7800427079200745\n",
      "Step 1740: val loss: 0.7494176626205444\n",
      "Step 1741: train loss: 0.7790586352348328\n",
      "Step 1741: val loss: 0.748460054397583\n",
      "Step 1742: train loss: 0.7780758142471313\n",
      "Step 1742: val loss: 0.7475044131278992\n",
      "Step 1743: train loss: 0.7770951390266418\n",
      "Step 1743: val loss: 0.7465508580207825\n",
      "Step 1744: train loss: 0.7761161923408508\n",
      "Step 1744: val loss: 0.7455987930297852\n",
      "Step 1745: train loss: 0.7751390933990479\n",
      "Step 1745: val loss: 0.7446489930152893\n",
      "Step 1746: train loss: 0.7741639614105225\n",
      "Step 1746: val loss: 0.7437008023262024\n",
      "Step 1747: train loss: 0.7731907963752747\n",
      "Step 1747: val loss: 0.7427540421485901\n",
      "Step 1748: train loss: 0.7722188234329224\n",
      "Step 1748: val loss: 0.7418091297149658\n",
      "Step 1749: train loss: 0.7712486982345581\n",
      "Step 1749: val loss: 0.7408660650253296\n",
      "Step 1750: train loss: 0.770280659198761\n",
      "Step 1750: val loss: 0.7399246692657471\n",
      "Step 1751: train loss: 0.7693142294883728\n",
      "Step 1751: val loss: 0.7389854788780212\n",
      "Step 1752: train loss: 0.7683497071266174\n",
      "Step 1752: val loss: 0.7380479574203491\n",
      "Step 1753: train loss: 0.767387330532074\n",
      "Step 1753: val loss: 0.7371119260787964\n",
      "Step 1754: train loss: 0.7664260864257812\n",
      "Step 1754: val loss: 0.7361775636672974\n",
      "Step 1755: train loss: 0.765466570854187\n",
      "Step 1755: val loss: 0.7352452874183655\n",
      "Step 1756: train loss: 0.7645091414451599\n",
      "Step 1756: val loss: 0.7343147397041321\n",
      "Step 1757: train loss: 0.7635533809661865\n",
      "Step 1757: val loss: 0.7333859801292419\n",
      "Step 1758: train loss: 0.7625995874404907\n",
      "Step 1758: val loss: 0.7324586510658264\n",
      "Step 1759: train loss: 0.7616472840309143\n",
      "Step 1759: val loss: 0.7315331101417542\n",
      "Step 1760: train loss: 0.7606964111328125\n",
      "Step 1760: val loss: 0.7306092381477356\n",
      "Step 1761: train loss: 0.7597476840019226\n",
      "Step 1761: val loss: 0.7296875715255737\n",
      "Step 1762: train loss: 0.7588006854057312\n",
      "Step 1762: val loss: 0.7287675738334656\n",
      "Step 1763: train loss: 0.7578557133674622\n",
      "Step 1763: val loss: 0.7278491258621216\n",
      "Step 1764: train loss: 0.756912112236023\n",
      "Step 1764: val loss: 0.7269322276115417\n",
      "Step 1765: train loss: 0.7559702396392822\n",
      "Step 1765: val loss: 0.7260171175003052\n",
      "Step 1766: train loss: 0.7550300359725952\n",
      "Step 1766: val loss: 0.7251037359237671\n",
      "Step 1767: train loss: 0.7540916204452515\n",
      "Step 1767: val loss: 0.7241923809051514\n",
      "Step 1768: train loss: 0.7531551718711853\n",
      "Step 1768: val loss: 0.7232823371887207\n",
      "Step 1769: train loss: 0.7522200345993042\n",
      "Step 1769: val loss: 0.7223740816116333\n",
      "Step 1770: train loss: 0.7512868046760559\n",
      "Step 1770: val loss: 0.7214678525924683\n",
      "Step 1771: train loss: 0.7503553628921509\n",
      "Step 1771: val loss: 0.7205631136894226\n",
      "Step 1772: train loss: 0.7494257092475891\n",
      "Step 1772: val loss: 0.7196604609489441\n",
      "Step 1773: train loss: 0.7484978437423706\n",
      "Step 1773: val loss: 0.7187589406967163\n",
      "Step 1774: train loss: 0.7475712299346924\n",
      "Step 1774: val loss: 0.717859148979187\n",
      "Step 1775: train loss: 0.746646523475647\n",
      "Step 1775: val loss: 0.7169613838195801\n",
      "Step 1776: train loss: 0.7457237839698792\n",
      "Step 1776: val loss: 0.7160654067993164\n",
      "Step 1777: train loss: 0.7448025941848755\n",
      "Step 1777: val loss: 0.7151708602905273\n",
      "Step 1778: train loss: 0.7438831329345703\n",
      "Step 1778: val loss: 0.7142780423164368\n",
      "Step 1779: train loss: 0.7429652214050293\n",
      "Step 1779: val loss: 0.7133867144584656\n",
      "Step 1780: train loss: 0.742048978805542\n",
      "Step 1780: val loss: 0.7124972939491272\n",
      "Step 1781: train loss: 0.7411347031593323\n",
      "Step 1781: val loss: 0.7116096615791321\n",
      "Step 1782: train loss: 0.7402219176292419\n",
      "Step 1782: val loss: 0.7107234597206116\n",
      "Step 1783: train loss: 0.7393105030059814\n",
      "Step 1783: val loss: 0.7098388671875\n",
      "Step 1784: train loss: 0.7384010553359985\n",
      "Step 1784: val loss: 0.7089560031890869\n",
      "Step 1785: train loss: 0.7374931573867798\n",
      "Step 1785: val loss: 0.7080748677253723\n",
      "Step 1786: train loss: 0.7365873456001282\n",
      "Step 1786: val loss: 0.7071959376335144\n",
      "Step 1787: train loss: 0.735683023929596\n",
      "Step 1787: val loss: 0.7063179612159729\n",
      "Step 1788: train loss: 0.7347801923751831\n",
      "Step 1788: val loss: 0.7054417133331299\n",
      "Step 1789: train loss: 0.7338790893554688\n",
      "Step 1789: val loss: 0.7045672535896301\n",
      "Step 1790: train loss: 0.7329797148704529\n",
      "Step 1790: val loss: 0.7036945819854736\n",
      "Step 1791: train loss: 0.7320820093154907\n",
      "Step 1791: val loss: 0.7028234004974365\n",
      "Step 1792: train loss: 0.7311856746673584\n",
      "Step 1792: val loss: 0.701953649520874\n",
      "Step 1793: train loss: 0.7302911877632141\n",
      "Step 1793: val loss: 0.7010858058929443\n",
      "Step 1794: train loss: 0.7293984293937683\n",
      "Step 1794: val loss: 0.7002199292182922\n",
      "Step 1795: train loss: 0.7285074591636658\n",
      "Step 1795: val loss: 0.6993551254272461\n",
      "Step 1796: train loss: 0.7276176810264587\n",
      "Step 1796: val loss: 0.6984919309616089\n",
      "Step 1797: train loss: 0.7267295122146606\n",
      "Step 1797: val loss: 0.6976308226585388\n",
      "Step 1798: train loss: 0.7258433103561401\n",
      "Step 1798: val loss: 0.6967711448669434\n",
      "Step 1799: train loss: 0.7249588370323181\n",
      "Step 1799: val loss: 0.6959128975868225\n",
      "Step 1800: train loss: 0.7240756154060364\n",
      "Step 1800: val loss: 0.6950562596321106\n",
      "Step 1801: train loss: 0.7231941819190979\n",
      "Step 1801: val loss: 0.6942015290260315\n",
      "Step 1802: train loss: 0.7223145365715027\n",
      "Step 1802: val loss: 0.6933483481407166\n",
      "Step 1803: train loss: 0.7214364409446716\n",
      "Step 1803: val loss: 0.6924965381622314\n",
      "Step 1804: train loss: 0.7205597162246704\n",
      "Step 1804: val loss: 0.6916465163230896\n",
      "Step 1805: train loss: 0.7196847200393677\n",
      "Step 1805: val loss: 0.6907982230186462\n",
      "Step 1806: train loss: 0.7188115119934082\n",
      "Step 1806: val loss: 0.6899514198303223\n",
      "Step 1807: train loss: 0.7179398536682129\n",
      "Step 1807: val loss: 0.6891061067581177\n",
      "Step 1808: train loss: 0.7170696258544922\n",
      "Step 1808: val loss: 0.6882623434066772\n",
      "Step 1809: train loss: 0.71620112657547\n",
      "Step 1809: val loss: 0.6874204874038696\n",
      "Step 1810: train loss: 0.7153342962265015\n",
      "Step 1810: val loss: 0.6865798234939575\n",
      "Step 1811: train loss: 0.7144687175750732\n",
      "Step 1811: val loss: 0.6857408881187439\n",
      "Step 1812: train loss: 0.713604748249054\n",
      "Step 1812: val loss: 0.6849036812782288\n",
      "Step 1813: train loss: 0.712742805480957\n",
      "Step 1813: val loss: 0.6840682029724121\n",
      "Step 1814: train loss: 0.7118823528289795\n",
      "Step 1814: val loss: 0.6832335591316223\n",
      "Step 1815: train loss: 0.7110230922698975\n",
      "Step 1815: val loss: 0.6824012398719788\n",
      "Step 1816: train loss: 0.7101656198501587\n",
      "Step 1816: val loss: 0.6815701127052307\n",
      "Step 1817: train loss: 0.7093098759651184\n",
      "Step 1817: val loss: 0.6807408928871155\n",
      "Step 1818: train loss: 0.7084558010101318\n",
      "Step 1818: val loss: 0.6799129843711853\n",
      "Step 1819: train loss: 0.7076029777526855\n",
      "Step 1819: val loss: 0.6790867447853088\n",
      "Step 1820: train loss: 0.706751823425293\n",
      "Step 1820: val loss: 0.678261935710907\n",
      "Step 1821: train loss: 0.7059023380279541\n",
      "Step 1821: val loss: 0.6774386167526245\n",
      "Step 1822: train loss: 0.7050541639328003\n",
      "Step 1822: val loss: 0.6766170263290405\n",
      "Step 1823: train loss: 0.7042077779769897\n",
      "Step 1823: val loss: 0.6757969856262207\n",
      "Step 1824: train loss: 0.7033629417419434\n",
      "Step 1824: val loss: 0.6749783158302307\n",
      "Step 1825: train loss: 0.7025193572044373\n",
      "Step 1825: val loss: 0.6741611957550049\n",
      "Step 1826: train loss: 0.7016774415969849\n",
      "Step 1826: val loss: 0.6733459234237671\n",
      "Step 1827: train loss: 0.7008373141288757\n",
      "Step 1827: val loss: 0.6725320219993591\n",
      "Step 1828: train loss: 0.6999987959861755\n",
      "Step 1828: val loss: 0.6717194318771362\n",
      "Step 1829: train loss: 0.6991613507270813\n",
      "Step 1829: val loss: 0.6709088087081909\n",
      "Step 1830: train loss: 0.6983257532119751\n",
      "Step 1830: val loss: 0.6700994372367859\n",
      "Step 1831: train loss: 0.6974918246269226\n",
      "Step 1831: val loss: 0.6692916750907898\n",
      "Step 1832: train loss: 0.6966590285301208\n",
      "Step 1832: val loss: 0.6684852838516235\n",
      "Step 1833: train loss: 0.6958280205726624\n",
      "Step 1833: val loss: 0.6676806211471558\n",
      "Step 1834: train loss: 0.6949986219406128\n",
      "Step 1834: val loss: 0.6668772101402283\n",
      "Step 1835: train loss: 0.694170355796814\n",
      "Step 1835: val loss: 0.6660754680633545\n",
      "Step 1836: train loss: 0.6933437585830688\n",
      "Step 1836: val loss: 0.6652753353118896\n",
      "Step 1837: train loss: 0.6925190687179565\n",
      "Step 1837: val loss: 0.6644764542579651\n",
      "Step 1838: train loss: 0.691695511341095\n",
      "Step 1838: val loss: 0.6636791825294495\n",
      "Step 1839: train loss: 0.6908735036849976\n",
      "Step 1839: val loss: 0.662883460521698\n",
      "Step 1840: train loss: 0.6900529861450195\n",
      "Step 1840: val loss: 0.662089467048645\n",
      "Step 1841: train loss: 0.6892343163490295\n",
      "Step 1841: val loss: 0.6612967252731323\n",
      "Step 1842: train loss: 0.6884168982505798\n",
      "Step 1842: val loss: 0.6605055928230286\n",
      "Step 1843: train loss: 0.6876010894775391\n",
      "Step 1843: val loss: 0.6597161293029785\n",
      "Step 1844: train loss: 0.6867867708206177\n",
      "Step 1844: val loss: 0.6589277982711792\n",
      "Step 1845: train loss: 0.6859738230705261\n",
      "Step 1845: val loss: 0.6581413149833679\n",
      "Step 1846: train loss: 0.685162365436554\n",
      "Step 1846: val loss: 0.6573561429977417\n",
      "Step 1847: train loss: 0.6843525767326355\n",
      "Step 1847: val loss: 0.6565724015235901\n",
      "Step 1848: train loss: 0.6835442185401917\n",
      "Step 1848: val loss: 0.6557899713516235\n",
      "Step 1849: train loss: 0.6827372908592224\n",
      "Step 1849: val loss: 0.655009388923645\n",
      "Step 1850: train loss: 0.6819319128990173\n",
      "Step 1850: val loss: 0.6542298793792725\n",
      "Step 1851: train loss: 0.6811277866363525\n",
      "Step 1851: val loss: 0.6534520387649536\n",
      "Step 1852: train loss: 0.6803252100944519\n",
      "Step 1852: val loss: 0.6526759266853333\n",
      "Step 1853: train loss: 0.6795243620872498\n",
      "Step 1853: val loss: 0.6519009470939636\n",
      "Step 1854: train loss: 0.6787247061729431\n",
      "Step 1854: val loss: 0.6511275768280029\n",
      "Step 1855: train loss: 0.6779268980026245\n",
      "Step 1855: val loss: 0.650355875492096\n",
      "Step 1856: train loss: 0.6771304607391357\n",
      "Step 1856: val loss: 0.6495853066444397\n",
      "Step 1857: train loss: 0.6763351559638977\n",
      "Step 1857: val loss: 0.6488161683082581\n",
      "Step 1858: train loss: 0.6755414605140686\n",
      "Step 1858: val loss: 0.648048460483551\n",
      "Step 1859: train loss: 0.6747490763664246\n",
      "Step 1859: val loss: 0.6472823619842529\n",
      "Step 1860: train loss: 0.6739581823348999\n",
      "Step 1860: val loss: 0.6465175747871399\n",
      "Step 1861: train loss: 0.6731690168380737\n",
      "Step 1861: val loss: 0.6457542777061462\n",
      "Step 1862: train loss: 0.672380805015564\n",
      "Step 1862: val loss: 0.6449921727180481\n",
      "Step 1863: train loss: 0.6715943217277527\n",
      "Step 1863: val loss: 0.6442320942878723\n",
      "Step 1864: train loss: 0.6708095073699951\n",
      "Step 1864: val loss: 0.6434727907180786\n",
      "Step 1865: train loss: 0.6700258255004883\n",
      "Step 1865: val loss: 0.642715573310852\n",
      "Step 1866: train loss: 0.6692436337471008\n",
      "Step 1866: val loss: 0.6419594883918762\n",
      "Step 1867: train loss: 0.6684632301330566\n",
      "Step 1867: val loss: 0.6412047147750854\n",
      "Step 1868: train loss: 0.6676837205886841\n",
      "Step 1868: val loss: 0.6404513716697693\n",
      "Step 1869: train loss: 0.66690593957901\n",
      "Step 1869: val loss: 0.6396997570991516\n",
      "Step 1870: train loss: 0.6661296486854553\n",
      "Step 1870: val loss: 0.6389492750167847\n",
      "Step 1871: train loss: 0.6653546094894409\n",
      "Step 1871: val loss: 0.6382004618644714\n",
      "Step 1872: train loss: 0.6645812392234802\n",
      "Step 1872: val loss: 0.6374527215957642\n",
      "Step 1873: train loss: 0.6638089418411255\n",
      "Step 1873: val loss: 0.636706531047821\n",
      "Step 1874: train loss: 0.6630382537841797\n",
      "Step 1874: val loss: 0.6359617710113525\n",
      "Step 1875: train loss: 0.662269115447998\n",
      "Step 1875: val loss: 0.6352183222770691\n",
      "Step 1876: train loss: 0.6615011096000671\n",
      "Step 1876: val loss: 0.6344765424728394\n",
      "Step 1877: train loss: 0.6607346534729004\n",
      "Step 1877: val loss: 0.6337360739707947\n",
      "Step 1878: train loss: 0.6599697470664978\n",
      "Step 1878: val loss: 0.6329970359802246\n",
      "Step 1879: train loss: 0.6592060327529907\n",
      "Step 1879: val loss: 0.6322592496871948\n",
      "Step 1880: train loss: 0.658443808555603\n",
      "Step 1880: val loss: 0.6315227746963501\n",
      "Step 1881: train loss: 0.6576827764511108\n",
      "Step 1881: val loss: 0.63078773021698\n",
      "Step 1882: train loss: 0.6569234132766724\n",
      "Step 1882: val loss: 0.630054235458374\n",
      "Step 1883: train loss: 0.656165361404419\n",
      "Step 1883: val loss: 0.6293219923973083\n",
      "Step 1884: train loss: 0.6554085612297058\n",
      "Step 1884: val loss: 0.6285913586616516\n",
      "Step 1885: train loss: 0.6546535491943359\n",
      "Step 1885: val loss: 0.627861738204956\n",
      "Step 1886: train loss: 0.6538993716239929\n",
      "Step 1886: val loss: 0.6271336674690247\n",
      "Step 1887: train loss: 0.6531468629837036\n",
      "Step 1887: val loss: 0.6264069676399231\n",
      "Step 1888: train loss: 0.6523957848548889\n",
      "Step 1888: val loss: 0.6256816387176514\n",
      "Step 1889: train loss: 0.6516458988189697\n",
      "Step 1889: val loss: 0.6249578595161438\n",
      "Step 1890: train loss: 0.6508976221084595\n",
      "Step 1890: val loss: 0.6242354512214661\n",
      "Step 1891: train loss: 0.6501508355140686\n",
      "Step 1891: val loss: 0.6235142946243286\n",
      "Step 1892: train loss: 0.6494051218032837\n",
      "Step 1892: val loss: 0.6227944493293762\n",
      "Step 1893: train loss: 0.6486610174179077\n",
      "Step 1893: val loss: 0.6220757365226746\n",
      "Step 1894: train loss: 0.6479179263114929\n",
      "Step 1894: val loss: 0.6213587522506714\n",
      "Step 1895: train loss: 0.6471763849258423\n",
      "Step 1895: val loss: 0.620643138885498\n",
      "Step 1896: train loss: 0.6464365124702454\n",
      "Step 1896: val loss: 0.6199289560317993\n",
      "Step 1897: train loss: 0.6456977128982544\n",
      "Step 1897: val loss: 0.6192157864570618\n",
      "Step 1898: train loss: 0.6449602246284485\n",
      "Step 1898: val loss: 0.6185041666030884\n",
      "Step 1899: train loss: 0.6442241668701172\n",
      "Step 1899: val loss: 0.6177936792373657\n",
      "Step 1900: train loss: 0.643489420413971\n",
      "Step 1900: val loss: 0.6170845627784729\n",
      "Step 1901: train loss: 0.6427558660507202\n",
      "Step 1901: val loss: 0.6163769364356995\n",
      "Step 1902: train loss: 0.6420238614082336\n",
      "Step 1902: val loss: 0.6156708002090454\n",
      "Step 1903: train loss: 0.6412933468818665\n",
      "Step 1903: val loss: 0.614965558052063\n",
      "Step 1904: train loss: 0.6405638456344604\n",
      "Step 1904: val loss: 0.614262044429779\n",
      "Step 1905: train loss: 0.6398358345031738\n",
      "Step 1905: val loss: 0.6135594844818115\n",
      "Step 1906: train loss: 0.6391090750694275\n",
      "Step 1906: val loss: 0.6128586530685425\n",
      "Step 1907: train loss: 0.6383838057518005\n",
      "Step 1907: val loss: 0.612159013748169\n",
      "Step 1908: train loss: 0.6376598477363586\n",
      "Step 1908: val loss: 0.6114605069160461\n",
      "Step 1909: train loss: 0.636937141418457\n",
      "Step 1909: val loss: 0.6107637286186218\n",
      "Step 1910: train loss: 0.6362160444259644\n",
      "Step 1910: val loss: 0.6100679039955139\n",
      "Step 1911: train loss: 0.6354957818984985\n",
      "Step 1911: val loss: 0.6093735694885254\n",
      "Step 1912: train loss: 0.6347771883010864\n",
      "Step 1912: val loss: 0.6086803674697876\n",
      "Step 1913: train loss: 0.6340596079826355\n",
      "Step 1913: val loss: 0.6079887747764587\n",
      "Step 1914: train loss: 0.6333435773849487\n",
      "Step 1914: val loss: 0.6072982549667358\n",
      "Step 1915: train loss: 0.6326290965080261\n",
      "Step 1915: val loss: 0.6066092848777771\n",
      "Step 1916: train loss: 0.6319156289100647\n",
      "Step 1916: val loss: 0.6059215068817139\n",
      "Step 1917: train loss: 0.6312033534049988\n",
      "Step 1917: val loss: 0.6052347421646118\n",
      "Step 1918: train loss: 0.6304925680160522\n",
      "Step 1918: val loss: 0.6045497059822083\n",
      "Step 1919: train loss: 0.6297831535339355\n",
      "Step 1919: val loss: 0.6038654446601868\n",
      "Step 1920: train loss: 0.6290746331214905\n",
      "Step 1920: val loss: 0.603182852268219\n",
      "Step 1921: train loss: 0.6283677220344543\n",
      "Step 1921: val loss: 0.602501392364502\n",
      "Step 1922: train loss: 0.6276621222496033\n",
      "Step 1922: val loss: 0.6018213033676147\n",
      "Step 1923: train loss: 0.6269577741622925\n",
      "Step 1923: val loss: 0.6011426448822021\n",
      "Step 1924: train loss: 0.6262548565864563\n",
      "Step 1924: val loss: 0.6004651784896851\n",
      "Step 1925: train loss: 0.6255532503128052\n",
      "Step 1925: val loss: 0.599789023399353\n",
      "Step 1926: train loss: 0.6248528361320496\n",
      "Step 1926: val loss: 0.599113941192627\n",
      "Step 1927: train loss: 0.6241535544395447\n",
      "Step 1927: val loss: 0.5984404683113098\n",
      "Step 1928: train loss: 0.623455822467804\n",
      "Step 1928: val loss: 0.5977679491043091\n",
      "Step 1929: train loss: 0.6227589249610901\n",
      "Step 1929: val loss: 0.5970967411994934\n",
      "Step 1930: train loss: 0.6220636963844299\n",
      "Step 1930: val loss: 0.5964269042015076\n",
      "Step 1931: train loss: 0.6213695406913757\n",
      "Step 1931: val loss: 0.5957582592964172\n",
      "Step 1932: train loss: 0.6206768155097961\n",
      "Step 1932: val loss: 0.5950907468795776\n",
      "Step 1933: train loss: 0.619985044002533\n",
      "Step 1933: val loss: 0.5944246649742126\n",
      "Step 1934: train loss: 0.6192948222160339\n",
      "Step 1934: val loss: 0.5937599539756775\n",
      "Step 1935: train loss: 0.6186059713363647\n",
      "Step 1935: val loss: 0.5930964350700378\n",
      "Step 1936: train loss: 0.6179182529449463\n",
      "Step 1936: val loss: 0.5924343466758728\n",
      "Step 1937: train loss: 0.6172319054603577\n",
      "Step 1937: val loss: 0.5917732119560242\n",
      "Step 1938: train loss: 0.6165466904640198\n",
      "Step 1938: val loss: 0.5911135077476501\n",
      "Step 1939: train loss: 0.6158629059791565\n",
      "Step 1939: val loss: 0.5904548764228821\n",
      "Step 1940: train loss: 0.6151801943778992\n",
      "Step 1940: val loss: 0.5897976756095886\n",
      "Step 1941: train loss: 0.6144987940788269\n",
      "Step 1941: val loss: 0.5891413688659668\n",
      "Step 1942: train loss: 0.6138184070587158\n",
      "Step 1942: val loss: 0.5884866714477539\n",
      "Step 1943: train loss: 0.6131395101547241\n",
      "Step 1943: val loss: 0.5878329277038574\n",
      "Step 1944: train loss: 0.6124618053436279\n",
      "Step 1944: val loss: 0.5871807336807251\n",
      "Step 1945: train loss: 0.6117854714393616\n",
      "Step 1945: val loss: 0.5865294933319092\n",
      "Step 1946: train loss: 0.6111100912094116\n",
      "Step 1946: val loss: 0.5858797430992126\n",
      "Step 1947: train loss: 0.6104362607002258\n",
      "Step 1947: val loss: 0.5852308869361877\n",
      "Step 1948: train loss: 0.6097632646560669\n",
      "Step 1948: val loss: 0.5845834016799927\n",
      "Step 1949: train loss: 0.6090918183326721\n",
      "Step 1949: val loss: 0.5839370489120483\n",
      "Step 1950: train loss: 0.6084215044975281\n",
      "Step 1950: val loss: 0.5832923054695129\n",
      "Step 1951: train loss: 0.6077526211738586\n",
      "Step 1951: val loss: 0.5826483368873596\n",
      "Step 1952: train loss: 0.6070845127105713\n",
      "Step 1952: val loss: 0.5820059776306152\n",
      "Step 1953: train loss: 0.6064180731773376\n",
      "Step 1953: val loss: 0.5813643932342529\n",
      "Step 1954: train loss: 0.6057524681091309\n",
      "Step 1954: val loss: 0.58072429895401\n",
      "Step 1955: train loss: 0.6050883531570435\n",
      "Step 1955: val loss: 0.5800853967666626\n",
      "Step 1956: train loss: 0.604425311088562\n",
      "Step 1956: val loss: 0.5794474482536316\n",
      "Step 1957: train loss: 0.6037635207176208\n",
      "Step 1957: val loss: 0.5788110494613647\n",
      "Step 1958: train loss: 0.60310298204422\n",
      "Step 1958: val loss: 0.5781757831573486\n",
      "Step 1959: train loss: 0.6024435758590698\n",
      "Step 1959: val loss: 0.5775414705276489\n",
      "Step 1960: train loss: 0.6017854809761047\n",
      "Step 1960: val loss: 0.5769085884094238\n",
      "Step 1961: train loss: 0.6011284589767456\n",
      "Step 1961: val loss: 0.5762766599655151\n",
      "Step 1962: train loss: 0.6004726886749268\n",
      "Step 1962: val loss: 0.5756462812423706\n",
      "Step 1963: train loss: 0.5998183488845825\n",
      "Step 1963: val loss: 0.5750167965888977\n",
      "Step 1964: train loss: 0.5991649031639099\n",
      "Step 1964: val loss: 0.574388861656189\n",
      "Step 1965: train loss: 0.5985128879547119\n",
      "Step 1965: val loss: 0.5737617611885071\n",
      "Step 1966: train loss: 0.5978619456291199\n",
      "Step 1966: val loss: 0.573136031627655\n",
      "Step 1967: train loss: 0.5972121953964233\n",
      "Step 1967: val loss: 0.5725115537643433\n",
      "Step 1968: train loss: 0.5965635776519775\n",
      "Step 1968: val loss: 0.5718880891799927\n",
      "Step 1969: train loss: 0.5959163308143616\n",
      "Step 1969: val loss: 0.5712657570838928\n",
      "Step 1970: train loss: 0.5952702164649963\n",
      "Step 1970: val loss: 0.570644736289978\n",
      "Step 1971: train loss: 0.5946252346038818\n",
      "Step 1971: val loss: 0.570024847984314\n",
      "Step 1972: train loss: 0.5939814448356628\n",
      "Step 1972: val loss: 0.569406270980835\n",
      "Step 1973: train loss: 0.5933390259742737\n",
      "Step 1973: val loss: 0.5687886476516724\n",
      "Step 1974: train loss: 0.5926975011825562\n",
      "Step 1974: val loss: 0.5681723952293396\n",
      "Step 1975: train loss: 0.5920573472976685\n",
      "Step 1975: val loss: 0.567557156085968\n",
      "Step 1976: train loss: 0.5914182066917419\n",
      "Step 1976: val loss: 0.5669431686401367\n",
      "Step 1977: train loss: 0.5907804369926453\n",
      "Step 1977: val loss: 0.5663304328918457\n",
      "Step 1978: train loss: 0.5901437401771545\n",
      "Step 1978: val loss: 0.5657188296318054\n",
      "Step 1979: train loss: 0.5895082950592041\n",
      "Step 1979: val loss: 0.5651082992553711\n",
      "Step 1980: train loss: 0.5888738036155701\n",
      "Step 1980: val loss: 0.564499020576477\n",
      "Step 1981: train loss: 0.5882408618927002\n",
      "Step 1981: val loss: 0.5638909339904785\n",
      "Step 1982: train loss: 0.5876087546348572\n",
      "Step 1982: val loss: 0.5632838606834412\n",
      "Step 1983: train loss: 0.5869779586791992\n",
      "Step 1983: val loss: 0.5626779794692993\n",
      "Step 1984: train loss: 0.5863483548164368\n",
      "Step 1984: val loss: 0.5620734095573425\n",
      "Step 1985: train loss: 0.5857198238372803\n",
      "Step 1985: val loss: 0.5614697933197021\n",
      "Step 1986: train loss: 0.5850925445556641\n",
      "Step 1986: val loss: 0.5608673691749573\n",
      "Step 1987: train loss: 0.5844661593437195\n",
      "Step 1987: val loss: 0.5602661371231079\n",
      "Step 1988: train loss: 0.58384108543396\n",
      "Step 1988: val loss: 0.559665858745575\n",
      "Step 1989: train loss: 0.5832169651985168\n",
      "Step 1989: val loss: 0.5590668320655823\n",
      "Step 1990: train loss: 0.5825942754745483\n",
      "Step 1990: val loss: 0.5584688782691956\n",
      "Step 1991: train loss: 0.5819724798202515\n",
      "Step 1991: val loss: 0.5578720569610596\n",
      "Step 1992: train loss: 0.5813521146774292\n",
      "Step 1992: val loss: 0.5572764873504639\n",
      "Step 1993: train loss: 0.5807325839996338\n",
      "Step 1993: val loss: 0.5566819906234741\n",
      "Step 1994: train loss: 0.580114483833313\n",
      "Step 1994: val loss: 0.5560886859893799\n",
      "Step 1995: train loss: 0.5794973969459534\n",
      "Step 1995: val loss: 0.5554964542388916\n",
      "Step 1996: train loss: 0.5788814425468445\n",
      "Step 1996: val loss: 0.5549054145812988\n",
      "Step 1997: train loss: 0.5782665610313416\n",
      "Step 1997: val loss: 0.5543153285980225\n",
      "Step 1998: train loss: 0.577652633190155\n",
      "Step 1998: val loss: 0.553726315498352\n",
      "Step 1999: train loss: 0.5770401358604431\n",
      "Step 1999: val loss: 0.5531384348869324\n",
      "Step 2000: train loss: 0.5764285922050476\n",
      "Step 2000: val loss: 0.5525517463684082\n",
      "Step 2001: train loss: 0.5758181214332581\n",
      "Step 2001: val loss: 0.5519661903381348\n",
      "Step 2002: train loss: 0.5752087831497192\n",
      "Step 2002: val loss: 0.5513818860054016\n",
      "Step 2003: train loss: 0.5746008157730103\n",
      "Step 2003: val loss: 0.5507984757423401\n",
      "Step 2004: train loss: 0.5739938020706177\n",
      "Step 2004: val loss: 0.5502164363861084\n",
      "Step 2005: train loss: 0.5733879804611206\n",
      "Step 2005: val loss: 0.5496352314949036\n",
      "Step 2006: train loss: 0.5727832913398743\n",
      "Step 2006: val loss: 0.5490550994873047\n",
      "Step 2007: train loss: 0.5721793174743652\n",
      "Step 2007: val loss: 0.5484762191772461\n",
      "Step 2008: train loss: 0.5715768933296204\n",
      "Step 2008: val loss: 0.5478982925415039\n",
      "Step 2009: train loss: 0.5709751844406128\n",
      "Step 2009: val loss: 0.5473214983940125\n",
      "Step 2010: train loss: 0.5703749060630798\n",
      "Step 2010: val loss: 0.546746015548706\n",
      "Step 2011: train loss: 0.5697757005691528\n",
      "Step 2011: val loss: 0.5461715459823608\n",
      "Step 2012: train loss: 0.5691776275634766\n",
      "Step 2012: val loss: 0.545598030090332\n",
      "Step 2013: train loss: 0.5685804486274719\n",
      "Step 2013: val loss: 0.5450255274772644\n",
      "Step 2014: train loss: 0.5679845213890076\n",
      "Step 2014: val loss: 0.5444542765617371\n",
      "Step 2015: train loss: 0.5673895478248596\n",
      "Step 2015: val loss: 0.5438839197158813\n",
      "Step 2016: train loss: 0.5667957067489624\n",
      "Step 2016: val loss: 0.5433149933815002\n",
      "Step 2017: train loss: 0.5662031173706055\n",
      "Step 2017: val loss: 0.5427468419075012\n",
      "Step 2018: train loss: 0.5656114220619202\n",
      "Step 2018: val loss: 0.5421800017356873\n",
      "Step 2019: train loss: 0.5650210380554199\n",
      "Step 2019: val loss: 0.5416140556335449\n",
      "Step 2020: train loss: 0.5644316077232361\n",
      "Step 2020: val loss: 0.5410493016242981\n",
      "Step 2021: train loss: 0.5638431310653687\n",
      "Step 2021: val loss: 0.5404856204986572\n",
      "Step 2022: train loss: 0.5632559061050415\n",
      "Step 2022: val loss: 0.5399228930473328\n",
      "Step 2023: train loss: 0.5626697540283203\n",
      "Step 2023: val loss: 0.5393614172935486\n",
      "Step 2024: train loss: 0.5620847344398499\n",
      "Step 2024: val loss: 0.538800835609436\n",
      "Step 2025: train loss: 0.5615006685256958\n",
      "Step 2025: val loss: 0.5382410883903503\n",
      "Step 2026: train loss: 0.5609174370765686\n",
      "Step 2026: val loss: 0.5376827716827393\n",
      "Step 2027: train loss: 0.5603355169296265\n",
      "Step 2027: val loss: 0.537125289440155\n",
      "Step 2028: train loss: 0.5597546100616455\n",
      "Step 2028: val loss: 0.5365691184997559\n",
      "Step 2029: train loss: 0.5591748952865601\n",
      "Step 2029: val loss: 0.5360137224197388\n",
      "Step 2030: train loss: 0.5585960149765015\n",
      "Step 2030: val loss: 0.5354596376419067\n",
      "Step 2031: train loss: 0.5580185651779175\n",
      "Step 2031: val loss: 0.5349066257476807\n",
      "Step 2032: train loss: 0.5574420094490051\n",
      "Step 2032: val loss: 0.5343545079231262\n",
      "Step 2033: train loss: 0.5568663477897644\n",
      "Step 2033: val loss: 0.533803403377533\n",
      "Step 2034: train loss: 0.556291937828064\n",
      "Step 2034: val loss: 0.53325355052948\n",
      "Step 2035: train loss: 0.5557183623313904\n",
      "Step 2035: val loss: 0.5327045917510986\n",
      "Step 2036: train loss: 0.5551461577415466\n",
      "Step 2036: val loss: 0.5321568846702576\n",
      "Step 2037: train loss: 0.5545749068260193\n",
      "Step 2037: val loss: 0.5316098928451538\n",
      "Step 2038: train loss: 0.5540044903755188\n",
      "Step 2038: val loss: 0.5310640931129456\n",
      "Step 2039: train loss: 0.5534354448318481\n",
      "Step 2039: val loss: 0.530519425868988\n",
      "Step 2040: train loss: 0.5528671741485596\n",
      "Step 2040: val loss: 0.529975414276123\n",
      "Step 2041: train loss: 0.5522997975349426\n",
      "Step 2041: val loss: 0.5294325947761536\n",
      "Step 2042: train loss: 0.5517337918281555\n",
      "Step 2042: val loss: 0.5288907885551453\n",
      "Step 2043: train loss: 0.5511685609817505\n",
      "Step 2043: val loss: 0.5283503532409668\n",
      "Step 2044: train loss: 0.5506046414375305\n",
      "Step 2044: val loss: 0.5278105139732361\n",
      "Step 2045: train loss: 0.5500414967536926\n",
      "Step 2045: val loss: 0.5272717475891113\n",
      "Step 2046: train loss: 0.5494794249534607\n",
      "Step 2046: val loss: 0.5267343521118164\n",
      "Step 2047: train loss: 0.548918604850769\n",
      "Step 2047: val loss: 0.5261977314949036\n",
      "Step 2048: train loss: 0.5483585000038147\n",
      "Step 2048: val loss: 0.5256623029708862\n",
      "Step 2049: train loss: 0.5477997660636902\n",
      "Step 2049: val loss: 0.5251277089118958\n",
      "Step 2050: train loss: 0.5472419857978821\n",
      "Step 2050: val loss: 0.5245941281318665\n",
      "Step 2051: train loss: 0.546684980392456\n",
      "Step 2051: val loss: 0.5240616202354431\n",
      "Step 2052: train loss: 0.5461292862892151\n",
      "Step 2052: val loss: 0.523530125617981\n",
      "Step 2053: train loss: 0.5455743670463562\n",
      "Step 2053: val loss: 0.5229994654655457\n",
      "Step 2054: train loss: 0.5450204610824585\n",
      "Step 2054: val loss: 0.5224701166152954\n",
      "Step 2055: train loss: 0.5444678068161011\n",
      "Step 2055: val loss: 0.5219414234161377\n",
      "Step 2056: train loss: 0.5439158082008362\n",
      "Step 2056: val loss: 0.5214141011238098\n",
      "Step 2057: train loss: 0.5433653593063354\n",
      "Step 2057: val loss: 0.5208876132965088\n",
      "Step 2058: train loss: 0.542815625667572\n",
      "Step 2058: val loss: 0.520362138748169\n",
      "Step 2059: train loss: 0.542266845703125\n",
      "Step 2059: val loss: 0.5198377966880798\n",
      "Step 2060: train loss: 0.5417192578315735\n",
      "Step 2060: val loss: 0.5193142294883728\n",
      "Step 2061: train loss: 0.541172444820404\n",
      "Step 2061: val loss: 0.5187916159629822\n",
      "Step 2062: train loss: 0.540626585483551\n",
      "Step 2062: val loss: 0.5182701945304871\n",
      "Step 2063: train loss: 0.5400819182395935\n",
      "Step 2063: val loss: 0.517749547958374\n",
      "Step 2064: train loss: 0.5395381450653076\n",
      "Step 2064: val loss: 0.5172299742698669\n",
      "Step 2065: train loss: 0.5389952063560486\n",
      "Step 2065: val loss: 0.516711413860321\n",
      "Step 2066: train loss: 0.5384533405303955\n",
      "Step 2066: val loss: 0.5161938071250916\n",
      "Step 2067: train loss: 0.5379127264022827\n",
      "Step 2067: val loss: 0.515677273273468\n",
      "Step 2068: train loss: 0.5373731255531311\n",
      "Step 2068: val loss: 0.5151616930961609\n",
      "Step 2069: train loss: 0.5368342399597168\n",
      "Step 2069: val loss: 0.5146470069885254\n",
      "Step 2070: train loss: 0.5362964272499084\n",
      "Step 2070: val loss: 0.5141333937644958\n",
      "Step 2071: train loss: 0.5357597470283508\n",
      "Step 2071: val loss: 0.5136208534240723\n",
      "Step 2072: train loss: 0.5352241396903992\n",
      "Step 2072: val loss: 0.5131091475486755\n",
      "Step 2073: train loss: 0.5346889495849609\n",
      "Step 2073: val loss: 0.5125985145568848\n",
      "Step 2074: train loss: 0.5341553092002869\n",
      "Step 2074: val loss: 0.5120887160301208\n",
      "Step 2075: train loss: 0.5336223244667053\n",
      "Step 2075: val loss: 0.5115797519683838\n",
      "Step 2076: train loss: 0.5330904722213745\n",
      "Step 2076: val loss: 0.5110721588134766\n",
      "Step 2077: train loss: 0.5325595736503601\n",
      "Step 2077: val loss: 0.5105652809143066\n",
      "Step 2078: train loss: 0.5320295691490173\n",
      "Step 2078: val loss: 0.5100592374801636\n",
      "Step 2079: train loss: 0.5315005779266357\n",
      "Step 2079: val loss: 0.5095543265342712\n",
      "Step 2080: train loss: 0.5309725999832153\n",
      "Step 2080: val loss: 0.5090503692626953\n",
      "Step 2081: train loss: 0.5304456353187561\n",
      "Step 2081: val loss: 0.5085472464561462\n",
      "Step 2082: train loss: 0.5299192667007446\n",
      "Step 2082: val loss: 0.5080453753471375\n",
      "Step 2083: train loss: 0.529394268989563\n",
      "Step 2083: val loss: 0.5075441598892212\n",
      "Step 2084: train loss: 0.5288701057434082\n",
      "Step 2084: val loss: 0.5070438981056213\n",
      "Step 2085: train loss: 0.5283466577529907\n",
      "Step 2085: val loss: 0.5065447092056274\n",
      "Step 2086: train loss: 0.5278245210647583\n",
      "Step 2086: val loss: 0.5060464143753052\n",
      "Step 2087: train loss: 0.5273032188415527\n",
      "Step 2087: val loss: 0.5055490732192993\n",
      "Step 2088: train loss: 0.5267828106880188\n",
      "Step 2088: val loss: 0.5050528645515442\n",
      "Step 2089: train loss: 0.5262635350227356\n",
      "Step 2089: val loss: 0.5045573711395264\n",
      "Step 2090: train loss: 0.5257450938224792\n",
      "Step 2090: val loss: 0.504062831401825\n",
      "Step 2091: train loss: 0.5252275466918945\n",
      "Step 2091: val loss: 0.5035692453384399\n",
      "Step 2092: train loss: 0.5247110724449158\n",
      "Step 2092: val loss: 0.5030766129493713\n",
      "Step 2093: train loss: 0.5241954326629639\n",
      "Step 2093: val loss: 0.5025849938392639\n",
      "Step 2094: train loss: 0.5236808061599731\n",
      "Step 2094: val loss: 0.5020943284034729\n",
      "Step 2095: train loss: 0.5231672525405884\n",
      "Step 2095: val loss: 0.5016044974327087\n",
      "Step 2096: train loss: 0.5226544141769409\n",
      "Step 2096: val loss: 0.5011155009269714\n",
      "Step 2097: train loss: 0.5221424698829651\n",
      "Step 2097: val loss: 0.5006276369094849\n",
      "Step 2098: train loss: 0.52163165807724\n",
      "Step 2098: val loss: 0.5001407861709595\n",
      "Step 2099: train loss: 0.5211217999458313\n",
      "Step 2099: val loss: 0.499654620885849\n",
      "Step 2100: train loss: 0.5206127762794495\n",
      "Step 2100: val loss: 0.4991694688796997\n",
      "Step 2101: train loss: 0.5201046466827393\n",
      "Step 2101: val loss: 0.49868521094322205\n",
      "Step 2102: train loss: 0.5195976495742798\n",
      "Step 2102: val loss: 0.4982019364833832\n",
      "Step 2103: train loss: 0.5190913081169128\n",
      "Step 2103: val loss: 0.497719407081604\n",
      "Step 2104: train loss: 0.5185858607292175\n",
      "Step 2104: val loss: 0.4972379803657532\n",
      "Step 2105: train loss: 0.5180816054344177\n",
      "Step 2105: val loss: 0.49675726890563965\n",
      "Step 2106: train loss: 0.5175780653953552\n",
      "Step 2106: val loss: 0.4962775707244873\n",
      "Step 2107: train loss: 0.5170754194259644\n",
      "Step 2107: val loss: 0.49579888582229614\n",
      "Step 2108: train loss: 0.5165738463401794\n",
      "Step 2108: val loss: 0.49532103538513184\n",
      "Step 2109: train loss: 0.5160732865333557\n",
      "Step 2109: val loss: 0.4948439300060272\n",
      "Step 2110: train loss: 0.5155735015869141\n",
      "Step 2110: val loss: 0.4943681061267853\n",
      "Step 2111: train loss: 0.5150747299194336\n",
      "Step 2111: val loss: 0.4938928782939911\n",
      "Step 2112: train loss: 0.5145766735076904\n",
      "Step 2112: val loss: 0.49341869354248047\n",
      "Step 2113: train loss: 0.5140795707702637\n",
      "Step 2113: val loss: 0.49294546246528625\n",
      "Step 2114: train loss: 0.5135836005210876\n",
      "Step 2114: val loss: 0.4924730360507965\n",
      "Step 2115: train loss: 0.5130885243415833\n",
      "Step 2115: val loss: 0.4920015037059784\n",
      "Step 2116: train loss: 0.5125941038131714\n",
      "Step 2116: val loss: 0.49153080582618713\n",
      "Step 2117: train loss: 0.5121006369590759\n",
      "Step 2117: val loss: 0.491061270236969\n",
      "Step 2118: train loss: 0.5116082429885864\n",
      "Step 2118: val loss: 0.49059221148490906\n",
      "Step 2119: train loss: 0.5111165642738342\n",
      "Step 2119: val loss: 0.49012434482574463\n",
      "Step 2120: train loss: 0.5106257200241089\n",
      "Step 2120: val loss: 0.48965734243392944\n",
      "Step 2121: train loss: 0.5101361274719238\n",
      "Step 2121: val loss: 0.4891912043094635\n",
      "Step 2122: train loss: 0.5096473097801208\n",
      "Step 2122: val loss: 0.4887259006500244\n",
      "Step 2123: train loss: 0.5091592669487\n",
      "Step 2123: val loss: 0.488261342048645\n",
      "Step 2124: train loss: 0.5086719989776611\n",
      "Step 2124: val loss: 0.48779797554016113\n",
      "Step 2125: train loss: 0.508185863494873\n",
      "Step 2125: val loss: 0.48733535408973694\n",
      "Step 2126: train loss: 0.5077004432678223\n",
      "Step 2126: val loss: 0.48687341809272766\n",
      "Step 2127: train loss: 0.5072159171104431\n",
      "Step 2127: val loss: 0.48641273379325867\n",
      "Step 2128: train loss: 0.5067325234413147\n",
      "Step 2128: val loss: 0.485952764749527\n",
      "Step 2129: train loss: 0.5062499642372131\n",
      "Step 2129: val loss: 0.48549360036849976\n",
      "Step 2130: train loss: 0.5057680606842041\n",
      "Step 2130: val loss: 0.4850353002548218\n",
      "Step 2131: train loss: 0.5052870512008667\n",
      "Step 2131: val loss: 0.484578013420105\n",
      "Step 2132: train loss: 0.50480717420578\n",
      "Step 2132: val loss: 0.4841214716434479\n",
      "Step 2133: train loss: 0.5043278932571411\n",
      "Step 2133: val loss: 0.4836658537387848\n",
      "Step 2134: train loss: 0.5038496255874634\n",
      "Step 2134: val loss: 0.48321107029914856\n",
      "Step 2135: train loss: 0.5033723711967468\n",
      "Step 2135: val loss: 0.4827573299407959\n",
      "Step 2136: train loss: 0.5028960108757019\n",
      "Step 2136: val loss: 0.4823043942451477\n",
      "Step 2137: train loss: 0.5024203062057495\n",
      "Step 2137: val loss: 0.4818519353866577\n",
      "Step 2138: train loss: 0.5019453763961792\n",
      "Step 2138: val loss: 0.4814007580280304\n",
      "Step 2139: train loss: 0.5014716982841492\n",
      "Step 2139: val loss: 0.4809504449367523\n",
      "Step 2140: train loss: 0.5009986758232117\n",
      "Step 2140: val loss: 0.480500727891922\n",
      "Step 2141: train loss: 0.500526487827301\n",
      "Step 2141: val loss: 0.4800519347190857\n",
      "Step 2142: train loss: 0.5000549554824829\n",
      "Step 2142: val loss: 0.4796040952205658\n",
      "Step 2143: train loss: 0.4995846152305603\n",
      "Step 2143: val loss: 0.4791569709777832\n",
      "Step 2144: train loss: 0.4991150498390198\n",
      "Step 2144: val loss: 0.47871074080467224\n",
      "Step 2145: train loss: 0.49864616990089417\n",
      "Step 2145: val loss: 0.4782653748989105\n",
      "Step 2146: train loss: 0.49817827343940735\n",
      "Step 2146: val loss: 0.4778209626674652\n",
      "Step 2147: train loss: 0.4977113902568817\n",
      "Step 2147: val loss: 0.47737735509872437\n",
      "Step 2148: train loss: 0.49724528193473816\n",
      "Step 2148: val loss: 0.47693467140197754\n",
      "Step 2149: train loss: 0.4967798888683319\n",
      "Step 2149: val loss: 0.4764925539493561\n",
      "Step 2150: train loss: 0.4963153004646301\n",
      "Step 2150: val loss: 0.476051390171051\n",
      "Step 2151: train loss: 0.4958516061306\n",
      "Step 2151: val loss: 0.4756111204624176\n",
      "Step 2152: train loss: 0.49538910388946533\n",
      "Step 2152: val loss: 0.4751717150211334\n",
      "Step 2153: train loss: 0.4949270486831665\n",
      "Step 2153: val loss: 0.474732905626297\n",
      "Step 2154: train loss: 0.49446573853492737\n",
      "Step 2154: val loss: 0.4742952883243561\n",
      "Step 2155: train loss: 0.4940057396888733\n",
      "Step 2155: val loss: 0.4738582670688629\n",
      "Step 2156: train loss: 0.49354636669158936\n",
      "Step 2156: val loss: 0.4734221398830414\n",
      "Step 2157: train loss: 0.4930877983570099\n",
      "Step 2157: val loss: 0.47298675775527954\n",
      "Step 2158: train loss: 0.49262985587120056\n",
      "Step 2158: val loss: 0.4725522994995117\n",
      "Step 2159: train loss: 0.49217310547828674\n",
      "Step 2159: val loss: 0.4721187651157379\n",
      "Step 2160: train loss: 0.4917171597480774\n",
      "Step 2160: val loss: 0.4716859459877014\n",
      "Step 2161: train loss: 0.49126189947128296\n",
      "Step 2161: val loss: 0.47125378251075745\n",
      "Step 2162: train loss: 0.49080729484558105\n",
      "Step 2162: val loss: 0.47082263231277466\n",
      "Step 2163: train loss: 0.4903539717197418\n",
      "Step 2163: val loss: 0.47039246559143066\n",
      "Step 2164: train loss: 0.4899013638496399\n",
      "Step 2164: val loss: 0.4699628949165344\n",
      "Step 2165: train loss: 0.48944932222366333\n",
      "Step 2165: val loss: 0.46953409910202026\n",
      "Step 2166: train loss: 0.4889981746673584\n",
      "Step 2166: val loss: 0.4691062569618225\n",
      "Step 2167: train loss: 0.48854807019233704\n",
      "Step 2167: val loss: 0.4686792492866516\n",
      "Step 2168: train loss: 0.488098680973053\n",
      "Step 2168: val loss: 0.4682530462741852\n",
      "Step 2169: train loss: 0.4876501262187958\n",
      "Step 2169: val loss: 0.4678274989128113\n",
      "Step 2170: train loss: 0.4872022569179535\n",
      "Step 2170: val loss: 0.46740299463272095\n",
      "Step 2171: train loss: 0.48675549030303955\n",
      "Step 2171: val loss: 0.46697932481765747\n",
      "Step 2172: train loss: 0.4863094985485077\n",
      "Step 2172: val loss: 0.46655622124671936\n",
      "Step 2173: train loss: 0.48586416244506836\n",
      "Step 2173: val loss: 0.46613407135009766\n",
      "Step 2174: train loss: 0.48541969060897827\n",
      "Step 2174: val loss: 0.4657125473022461\n",
      "Step 2175: train loss: 0.4849758446216583\n",
      "Step 2175: val loss: 0.4652920365333557\n",
      "Step 2176: train loss: 0.48453307151794434\n",
      "Step 2176: val loss: 0.4648723304271698\n",
      "Step 2177: train loss: 0.4840911328792572\n",
      "Step 2177: val loss: 0.4644533395767212\n",
      "Step 2178: train loss: 0.483649879693985\n",
      "Step 2178: val loss: 0.4640350937843323\n",
      "Step 2179: train loss: 0.48320919275283813\n",
      "Step 2179: val loss: 0.46361762285232544\n",
      "Step 2180: train loss: 0.4827696681022644\n",
      "Step 2180: val loss: 0.46320104598999023\n",
      "Step 2181: train loss: 0.4823310077190399\n",
      "Step 2181: val loss: 0.4627854526042938\n",
      "Step 2182: train loss: 0.4818929135799408\n",
      "Step 2182: val loss: 0.4623703062534332\n",
      "Step 2183: train loss: 0.4814556837081909\n",
      "Step 2183: val loss: 0.4619559943675995\n",
      "Step 2184: train loss: 0.48101910948753357\n",
      "Step 2184: val loss: 0.461542546749115\n",
      "Step 2185: train loss: 0.4805836081504822\n",
      "Step 2185: val loss: 0.4611299932003021\n",
      "Step 2186: train loss: 0.4801487326622009\n",
      "Step 2186: val loss: 0.4607181251049042\n",
      "Step 2187: train loss: 0.4797148108482361\n",
      "Step 2187: val loss: 0.4603070616722107\n",
      "Step 2188: train loss: 0.47928154468536377\n",
      "Step 2188: val loss: 0.45989665389060974\n",
      "Step 2189: train loss: 0.4788489043712616\n",
      "Step 2189: val loss: 0.4594871699810028\n",
      "Step 2190: train loss: 0.478417307138443\n",
      "Step 2190: val loss: 0.4590783715248108\n",
      "Step 2191: train loss: 0.47798651456832886\n",
      "Step 2191: val loss: 0.4586704671382904\n",
      "Step 2192: train loss: 0.47755634784698486\n",
      "Step 2192: val loss: 0.45826321840286255\n",
      "Step 2193: train loss: 0.4771270751953125\n",
      "Step 2193: val loss: 0.4578569829463959\n",
      "Step 2194: train loss: 0.4766986668109894\n",
      "Step 2194: val loss: 0.45745134353637695\n",
      "Step 2195: train loss: 0.4762709438800812\n",
      "Step 2195: val loss: 0.4570465683937073\n",
      "Step 2196: train loss: 0.4758441150188446\n",
      "Step 2196: val loss: 0.4566425681114197\n",
      "Step 2197: train loss: 0.47541794180870056\n",
      "Step 2197: val loss: 0.45623907446861267\n",
      "Step 2198: train loss: 0.47499242424964905\n",
      "Step 2198: val loss: 0.4558367133140564\n",
      "Step 2199: train loss: 0.47456806898117065\n",
      "Step 2199: val loss: 0.4554349482059479\n",
      "Step 2200: train loss: 0.4741442799568176\n",
      "Step 2200: val loss: 0.4550339877605438\n",
      "Step 2201: train loss: 0.4737212657928467\n",
      "Step 2201: val loss: 0.45463380217552185\n",
      "Step 2202: train loss: 0.4732988774776459\n",
      "Step 2202: val loss: 0.45423424243927\n",
      "Step 2203: train loss: 0.4728773236274719\n",
      "Step 2203: val loss: 0.45383545756340027\n",
      "Step 2204: train loss: 0.4724564850330353\n",
      "Step 2204: val loss: 0.45343753695487976\n",
      "Step 2205: train loss: 0.4720365107059479\n",
      "Step 2205: val loss: 0.4530401825904846\n",
      "Step 2206: train loss: 0.4716174006462097\n",
      "Step 2206: val loss: 0.45264384150505066\n",
      "Step 2207: train loss: 0.4711989760398865\n",
      "Step 2207: val loss: 0.4522481858730316\n",
      "Step 2208: train loss: 0.47078123688697815\n",
      "Step 2208: val loss: 0.4518532156944275\n",
      "Step 2209: train loss: 0.47036412358283997\n",
      "Step 2209: val loss: 0.451459139585495\n",
      "Step 2210: train loss: 0.4699481725692749\n",
      "Step 2210: val loss: 0.4510657489299774\n",
      "Step 2211: train loss: 0.4695328176021576\n",
      "Step 2211: val loss: 0.45067301392555237\n",
      "Step 2212: train loss: 0.46911823749542236\n",
      "Step 2212: val loss: 0.4502810537815094\n",
      "Step 2213: train loss: 0.4687042832374573\n",
      "Step 2213: val loss: 0.44988977909088135\n",
      "Step 2214: train loss: 0.46829113364219666\n",
      "Step 2214: val loss: 0.4494993984699249\n",
      "Step 2215: train loss: 0.46787890791893005\n",
      "Step 2215: val loss: 0.449109822511673\n",
      "Step 2216: train loss: 0.467467337846756\n",
      "Step 2216: val loss: 0.4487209916114807\n",
      "Step 2217: train loss: 0.46705663204193115\n",
      "Step 2217: val loss: 0.4483327567577362\n",
      "Step 2218: train loss: 0.4666464626789093\n",
      "Step 2218: val loss: 0.44794514775276184\n",
      "Step 2219: train loss: 0.46623703837394714\n",
      "Step 2219: val loss: 0.4475584030151367\n",
      "Step 2220: train loss: 0.46582838892936707\n",
      "Step 2220: val loss: 0.44717252254486084\n",
      "Step 2221: train loss: 0.46542051434516907\n",
      "Step 2221: val loss: 0.4467872679233551\n",
      "Step 2222: train loss: 0.4650135338306427\n",
      "Step 2222: val loss: 0.44640281796455383\n",
      "Step 2223: train loss: 0.4646071791648865\n",
      "Step 2223: val loss: 0.4460189640522003\n",
      "Step 2224: train loss: 0.4642016291618347\n",
      "Step 2224: val loss: 0.44563597440719604\n",
      "Step 2225: train loss: 0.4637966752052307\n",
      "Step 2225: val loss: 0.44525346159935\n",
      "Step 2226: train loss: 0.463392436504364\n",
      "Step 2226: val loss: 0.44487208127975464\n",
      "Step 2227: train loss: 0.46298906207084656\n",
      "Step 2227: val loss: 0.4444911777973175\n",
      "Step 2228: train loss: 0.46258655190467834\n",
      "Step 2228: val loss: 0.4441111087799072\n",
      "Step 2229: train loss: 0.4621845781803131\n",
      "Step 2229: val loss: 0.4437316954135895\n",
      "Step 2230: train loss: 0.46178334951400757\n",
      "Step 2230: val loss: 0.44335296750068665\n",
      "Step 2231: train loss: 0.4613828659057617\n",
      "Step 2231: val loss: 0.44297492504119873\n",
      "Step 2232: train loss: 0.460983008146286\n",
      "Step 2232: val loss: 0.44259777665138245\n",
      "Step 2233: train loss: 0.4605841636657715\n",
      "Step 2233: val loss: 0.4422213137149811\n",
      "Step 2234: train loss: 0.46018609404563904\n",
      "Step 2234: val loss: 0.44184550642967224\n",
      "Step 2235: train loss: 0.45978844165802\n",
      "Step 2235: val loss: 0.4414704442024231\n",
      "Step 2236: train loss: 0.4593915343284607\n",
      "Step 2236: val loss: 0.4410959780216217\n",
      "Step 2237: train loss: 0.4589954614639282\n",
      "Step 2237: val loss: 0.4407223165035248\n",
      "Step 2238: train loss: 0.45860007405281067\n",
      "Step 2238: val loss: 0.4403495192527771\n",
      "Step 2239: train loss: 0.45820558071136475\n",
      "Step 2239: val loss: 0.4399771988391876\n",
      "Step 2240: train loss: 0.45781177282333374\n",
      "Step 2240: val loss: 0.43960586190223694\n",
      "Step 2241: train loss: 0.4574185609817505\n",
      "Step 2241: val loss: 0.439235121011734\n",
      "Step 2242: train loss: 0.4570261240005493\n",
      "Step 2242: val loss: 0.4388648569583893\n",
      "Step 2243: train loss: 0.45663416385650635\n",
      "Step 2243: val loss: 0.43849554657936096\n",
      "Step 2244: train loss: 0.45624324679374695\n",
      "Step 2244: val loss: 0.43812671303749084\n",
      "Step 2245: train loss: 0.4558527171611786\n",
      "Step 2245: val loss: 0.4377588629722595\n",
      "Step 2246: train loss: 0.4554632604122162\n",
      "Step 2246: val loss: 0.4373916983604431\n",
      "Step 2247: train loss: 0.45507436990737915\n",
      "Step 2247: val loss: 0.4370249807834625\n",
      "Step 2248: train loss: 0.4546862244606018\n",
      "Step 2248: val loss: 0.4366592466831207\n",
      "Step 2249: train loss: 0.45429879426956177\n",
      "Step 2249: val loss: 0.4362940490245819\n",
      "Step 2250: train loss: 0.4539119005203247\n",
      "Step 2250: val loss: 0.43592944741249084\n",
      "Step 2251: train loss: 0.4535258710384369\n",
      "Step 2251: val loss: 0.43556568026542664\n",
      "Step 2252: train loss: 0.45314037799835205\n",
      "Step 2252: val loss: 0.4352026879787445\n",
      "Step 2253: train loss: 0.4527559280395508\n",
      "Step 2253: val loss: 0.4348403215408325\n",
      "Step 2254: train loss: 0.4523719251155853\n",
      "Step 2254: val loss: 0.43447864055633545\n",
      "Step 2255: train loss: 0.45198875665664673\n",
      "Step 2255: val loss: 0.4341176450252533\n",
      "Step 2256: train loss: 0.45160621404647827\n",
      "Step 2256: val loss: 0.43375739455223083\n",
      "Step 2257: train loss: 0.45122432708740234\n",
      "Step 2257: val loss: 0.4333977699279785\n",
      "Step 2258: train loss: 0.4508432149887085\n",
      "Step 2258: val loss: 0.43303874135017395\n",
      "Step 2259: train loss: 0.45046260952949524\n",
      "Step 2259: val loss: 0.43268051743507385\n",
      "Step 2260: train loss: 0.45008301734924316\n",
      "Step 2260: val loss: 0.43232303857803345\n",
      "Step 2261: train loss: 0.44970396161079407\n",
      "Step 2261: val loss: 0.43196621537208557\n",
      "Step 2262: train loss: 0.4493255913257599\n",
      "Step 2262: val loss: 0.43161001801490784\n",
      "Step 2263: train loss: 0.4489479660987854\n",
      "Step 2263: val loss: 0.43125444650650024\n",
      "Step 2264: train loss: 0.44857096672058105\n",
      "Step 2264: val loss: 0.4308995306491852\n",
      "Step 2265: train loss: 0.4481945335865021\n",
      "Step 2265: val loss: 0.4305454194545746\n",
      "Step 2266: train loss: 0.4478188753128052\n",
      "Step 2266: val loss: 0.4301917850971222\n",
      "Step 2267: train loss: 0.44744375348091125\n",
      "Step 2267: val loss: 0.4298390746116638\n",
      "Step 2268: train loss: 0.4470697343349457\n",
      "Step 2268: val loss: 0.429487019777298\n",
      "Step 2269: train loss: 0.4466961920261383\n",
      "Step 2269: val loss: 0.4291355609893799\n",
      "Step 2270: train loss: 0.44632354378700256\n",
      "Step 2270: val loss: 0.4287848472595215\n",
      "Step 2271: train loss: 0.4459512531757355\n",
      "Step 2271: val loss: 0.42843466997146606\n",
      "Step 2272: train loss: 0.44557976722717285\n",
      "Step 2272: val loss: 0.4280853867530823\n",
      "Step 2273: train loss: 0.44520893692970276\n",
      "Step 2273: val loss: 0.4277365207672119\n",
      "Step 2274: train loss: 0.44483867287635803\n",
      "Step 2274: val loss: 0.4273882210254669\n",
      "Step 2275: train loss: 0.44446900486946106\n",
      "Step 2275: val loss: 0.4270406663417816\n",
      "Step 2276: train loss: 0.44410017132759094\n",
      "Step 2276: val loss: 0.42669397592544556\n",
      "Step 2277: train loss: 0.4437321126461029\n",
      "Step 2277: val loss: 0.42634791135787964\n",
      "Step 2278: train loss: 0.4433647096157074\n",
      "Step 2278: val loss: 0.4260024428367615\n",
      "Step 2279: train loss: 0.4429980218410492\n",
      "Step 2279: val loss: 0.4256576895713806\n",
      "Step 2280: train loss: 0.4426318407058716\n",
      "Step 2280: val loss: 0.4253135919570923\n",
      "Step 2281: train loss: 0.44226640462875366\n",
      "Step 2281: val loss: 0.42496997117996216\n",
      "Step 2282: train loss: 0.44190162420272827\n",
      "Step 2282: val loss: 0.4246271252632141\n",
      "Step 2283: train loss: 0.4415373206138611\n",
      "Step 2283: val loss: 0.4242849051952362\n",
      "Step 2284: train loss: 0.44117382168769836\n",
      "Step 2284: val loss: 0.4239434003829956\n",
      "Step 2285: train loss: 0.44081103801727295\n",
      "Step 2285: val loss: 0.4236026406288147\n",
      "Step 2286: train loss: 0.44044893980026245\n",
      "Step 2286: val loss: 0.423262357711792\n",
      "Step 2287: train loss: 0.44008758664131165\n",
      "Step 2287: val loss: 0.42292284965515137\n",
      "Step 2288: train loss: 0.4397267997264862\n",
      "Step 2288: val loss: 0.42258405685424805\n",
      "Step 2289: train loss: 0.43936672806739807\n",
      "Step 2289: val loss: 0.42224574089050293\n",
      "Step 2290: train loss: 0.43900713324546814\n",
      "Step 2290: val loss: 0.4219081997871399\n",
      "Step 2291: train loss: 0.43864837288856506\n",
      "Step 2291: val loss: 0.42157110571861267\n",
      "Step 2292: train loss: 0.4382900595664978\n",
      "Step 2292: val loss: 0.42123469710350037\n",
      "Step 2293: train loss: 0.4379326105117798\n",
      "Step 2293: val loss: 0.42089909315109253\n",
      "Step 2294: train loss: 0.4375755488872528\n",
      "Step 2294: val loss: 0.42056381702423096\n",
      "Step 2295: train loss: 0.4372192323207855\n",
      "Step 2295: val loss: 0.4202296733856201\n",
      "Step 2296: train loss: 0.4368637502193451\n",
      "Step 2296: val loss: 0.4198959171772003\n",
      "Step 2297: train loss: 0.43650898337364197\n",
      "Step 2297: val loss: 0.4195629060268402\n",
      "Step 2298: train loss: 0.43615472316741943\n",
      "Step 2298: val loss: 0.41923031210899353\n",
      "Step 2299: train loss: 0.4358012080192566\n",
      "Step 2299: val loss: 0.4188986122608185\n",
      "Step 2300: train loss: 0.4354482889175415\n",
      "Step 2300: val loss: 0.4185674488544464\n",
      "Step 2301: train loss: 0.435095876455307\n",
      "Step 2301: val loss: 0.4182368814945221\n",
      "Step 2302: train loss: 0.4347442090511322\n",
      "Step 2302: val loss: 0.41790691018104553\n",
      "Step 2303: train loss: 0.4343929886817932\n",
      "Step 2303: val loss: 0.41757750511169434\n",
      "Step 2304: train loss: 0.4340426027774811\n",
      "Step 2304: val loss: 0.4172487258911133\n",
      "Step 2305: train loss: 0.43369272351264954\n",
      "Step 2305: val loss: 0.41692060232162476\n",
      "Step 2306: train loss: 0.43334347009658813\n",
      "Step 2306: val loss: 0.41659295558929443\n",
      "Step 2307: train loss: 0.43299487233161926\n",
      "Step 2307: val loss: 0.4162663519382477\n",
      "Step 2308: train loss: 0.43264713883399963\n",
      "Step 2308: val loss: 0.41594013571739197\n",
      "Step 2309: train loss: 0.4322999119758606\n",
      "Step 2309: val loss: 0.4156147241592407\n",
      "Step 2310: train loss: 0.43195343017578125\n",
      "Step 2310: val loss: 0.4152897596359253\n",
      "Step 2311: train loss: 0.43160757422447205\n",
      "Step 2311: val loss: 0.41496554017066956\n",
      "Step 2312: train loss: 0.43126216530799866\n",
      "Step 2312: val loss: 0.41464194655418396\n",
      "Step 2313: train loss: 0.43091756105422974\n",
      "Step 2313: val loss: 0.41431885957717896\n",
      "Step 2314: train loss: 0.4305734932422638\n",
      "Step 2314: val loss: 0.41399645805358887\n",
      "Step 2315: train loss: 0.4302300810813904\n",
      "Step 2315: val loss: 0.41367462277412415\n",
      "Step 2316: train loss: 0.42988720536231995\n",
      "Step 2316: val loss: 0.4133533239364624\n",
      "Step 2317: train loss: 0.42954492568969727\n",
      "Step 2317: val loss: 0.4130326509475708\n",
      "Step 2318: train loss: 0.42920324206352234\n",
      "Step 2318: val loss: 0.4127125144004822\n",
      "Step 2319: train loss: 0.4288621246814728\n",
      "Step 2319: val loss: 0.41239306330680847\n",
      "Step 2320: train loss: 0.4285218417644501\n",
      "Step 2320: val loss: 0.4120745360851288\n",
      "Step 2321: train loss: 0.4281822443008423\n",
      "Step 2321: val loss: 0.4117562472820282\n",
      "Step 2322: train loss: 0.42784327268600464\n",
      "Step 2322: val loss: 0.4114389419555664\n",
      "Step 2323: train loss: 0.42750489711761475\n",
      "Step 2323: val loss: 0.41112208366394043\n",
      "Step 2324: train loss: 0.4271670877933502\n",
      "Step 2324: val loss: 0.41080573201179504\n",
      "Step 2325: train loss: 0.4268300533294678\n",
      "Step 2325: val loss: 0.41049015522003174\n",
      "Step 2326: train loss: 0.4264933168888092\n",
      "Step 2326: val loss: 0.41017502546310425\n",
      "Step 2327: train loss: 0.42615747451782227\n",
      "Step 2327: val loss: 0.4098605215549469\n",
      "Step 2328: train loss: 0.42582204937934875\n",
      "Step 2328: val loss: 0.4095465838909149\n",
      "Step 2329: train loss: 0.42548733949661255\n",
      "Step 2329: val loss: 0.40923330187797546\n",
      "Step 2330: train loss: 0.425152987241745\n",
      "Step 2330: val loss: 0.4089205861091614\n",
      "Step 2331: train loss: 0.4248194992542267\n",
      "Step 2331: val loss: 0.40860840678215027\n",
      "Step 2332: train loss: 0.4244864583015442\n",
      "Step 2332: val loss: 0.4082967936992645\n",
      "Step 2333: train loss: 0.4241540729999542\n",
      "Step 2333: val loss: 0.4079858362674713\n",
      "Step 2334: train loss: 0.4238222539424896\n",
      "Step 2334: val loss: 0.40767550468444824\n",
      "Step 2335: train loss: 0.42349106073379517\n",
      "Step 2335: val loss: 0.4073655903339386\n",
      "Step 2336: train loss: 0.42316052317619324\n",
      "Step 2336: val loss: 0.4070565700531006\n",
      "Step 2337: train loss: 0.4228304624557495\n",
      "Step 2337: val loss: 0.4067479968070984\n",
      "Step 2338: train loss: 0.42250120639801025\n",
      "Step 2338: val loss: 0.40644019842147827\n",
      "Step 2339: train loss: 0.42217254638671875\n",
      "Step 2339: val loss: 0.40613287687301636\n",
      "Step 2340: train loss: 0.4218445420265198\n",
      "Step 2340: val loss: 0.4058261513710022\n",
      "Step 2341: train loss: 0.42151716351509094\n",
      "Step 2341: val loss: 0.4055200517177582\n",
      "Step 2342: train loss: 0.4211903214454651\n",
      "Step 2342: val loss: 0.4052146077156067\n",
      "Step 2343: train loss: 0.42086413502693176\n",
      "Step 2343: val loss: 0.4049096405506134\n",
      "Step 2344: train loss: 0.42053836584091187\n",
      "Step 2344: val loss: 0.4046051800251007\n",
      "Step 2345: train loss: 0.4202132821083069\n",
      "Step 2345: val loss: 0.4043014645576477\n",
      "Step 2346: train loss: 0.4198887050151825\n",
      "Step 2346: val loss: 0.4039981961250305\n",
      "Step 2347: train loss: 0.419564813375473\n",
      "Step 2347: val loss: 0.4036955237388611\n",
      "Step 2348: train loss: 0.4192415177822113\n",
      "Step 2348: val loss: 0.4033935070037842\n",
      "Step 2349: train loss: 0.4189188480377197\n",
      "Step 2349: val loss: 0.40309199690818787\n",
      "Step 2350: train loss: 0.41859662532806396\n",
      "Step 2350: val loss: 0.40279099345207214\n",
      "Step 2351: train loss: 0.41827496886253357\n",
      "Step 2351: val loss: 0.40249064564704895\n",
      "Step 2352: train loss: 0.4179539382457733\n",
      "Step 2352: val loss: 0.4021908938884735\n",
      "Step 2353: train loss: 0.4176335036754608\n",
      "Step 2353: val loss: 0.4018915593624115\n",
      "Step 2354: train loss: 0.4173135757446289\n",
      "Step 2354: val loss: 0.40159299969673157\n",
      "Step 2355: train loss: 0.41699424386024475\n",
      "Step 2355: val loss: 0.4012948274612427\n",
      "Step 2356: train loss: 0.4166755676269531\n",
      "Step 2356: val loss: 0.4009973108768463\n",
      "Step 2357: train loss: 0.4163573682308197\n",
      "Step 2357: val loss: 0.4007003903388977\n",
      "Step 2358: train loss: 0.4160398244857788\n",
      "Step 2358: val loss: 0.40040385723114014\n",
      "Step 2359: train loss: 0.4157227873802185\n",
      "Step 2359: val loss: 0.4001079797744751\n",
      "Step 2360: train loss: 0.41540631651878357\n",
      "Step 2360: val loss: 0.399812787771225\n",
      "Step 2361: train loss: 0.41509050130844116\n",
      "Step 2361: val loss: 0.39951807260513306\n",
      "Step 2362: train loss: 0.41477516293525696\n",
      "Step 2362: val loss: 0.39922380447387695\n",
      "Step 2363: train loss: 0.4144604802131653\n",
      "Step 2363: val loss: 0.3989304006099701\n",
      "Step 2364: train loss: 0.4141464829444885\n",
      "Step 2364: val loss: 0.3986375629901886\n",
      "Step 2365: train loss: 0.41383323073387146\n",
      "Step 2365: val loss: 0.3983452022075653\n",
      "Step 2366: train loss: 0.41352036595344543\n",
      "Step 2366: val loss: 0.39805352687835693\n",
      "Step 2367: train loss: 0.41320815682411194\n",
      "Step 2367: val loss: 0.39776238799095154\n",
      "Step 2368: train loss: 0.4128965437412262\n",
      "Step 2368: val loss: 0.3974718153476715\n",
      "Step 2369: train loss: 0.41258543729782104\n",
      "Step 2369: val loss: 0.3971817195415497\n",
      "Step 2370: train loss: 0.41227492690086365\n",
      "Step 2370: val loss: 0.39689210057258606\n",
      "Step 2371: train loss: 0.41196489334106445\n",
      "Step 2371: val loss: 0.39660319685935974\n",
      "Step 2372: train loss: 0.41165557503700256\n",
      "Step 2372: val loss: 0.3963148593902588\n",
      "Step 2373: train loss: 0.41134676337242126\n",
      "Step 2373: val loss: 0.3960270285606384\n",
      "Step 2374: train loss: 0.41103845834732056\n",
      "Step 2374: val loss: 0.3957396149635315\n",
      "Step 2375: train loss: 0.41073065996170044\n",
      "Step 2375: val loss: 0.39545297622680664\n",
      "Step 2376: train loss: 0.41042351722717285\n",
      "Step 2376: val loss: 0.39516681432724\n",
      "Step 2377: train loss: 0.41011691093444824\n",
      "Step 2377: val loss: 0.39488109946250916\n",
      "Step 2378: train loss: 0.4098109006881714\n",
      "Step 2378: val loss: 0.39459601044654846\n",
      "Step 2379: train loss: 0.40950533747673035\n",
      "Step 2379: val loss: 0.39431142807006836\n",
      "Step 2380: train loss: 0.40920042991638184\n",
      "Step 2380: val loss: 0.39402735233306885\n",
      "Step 2381: train loss: 0.40889596939086914\n",
      "Step 2381: val loss: 0.3937438726425171\n",
      "Step 2382: train loss: 0.408592164516449\n",
      "Step 2382: val loss: 0.3934610188007355\n",
      "Step 2383: train loss: 0.408288836479187\n",
      "Step 2383: val loss: 0.3931786119937897\n",
      "Step 2384: train loss: 0.40798619389533997\n",
      "Step 2384: val loss: 0.3928968608379364\n",
      "Step 2385: train loss: 0.40768399834632874\n",
      "Step 2385: val loss: 0.3926154673099518\n",
      "Step 2386: train loss: 0.4073822796344757\n",
      "Step 2386: val loss: 0.3923347592353821\n",
      "Step 2387: train loss: 0.40708127617836\n",
      "Step 2387: val loss: 0.3920544981956482\n",
      "Step 2388: train loss: 0.4067806005477905\n",
      "Step 2388: val loss: 0.39177483320236206\n",
      "Step 2389: train loss: 0.40648072957992554\n",
      "Step 2389: val loss: 0.39149561524391174\n",
      "Step 2390: train loss: 0.406181275844574\n",
      "Step 2390: val loss: 0.39121702313423157\n",
      "Step 2391: train loss: 0.40588241815567017\n",
      "Step 2391: val loss: 0.3909388780593872\n",
      "Step 2392: train loss: 0.4055840075016022\n",
      "Step 2392: val loss: 0.39066147804260254\n",
      "Step 2393: train loss: 0.40528619289398193\n",
      "Step 2393: val loss: 0.3903844952583313\n",
      "Step 2394: train loss: 0.4049889147281647\n",
      "Step 2394: val loss: 0.3901078402996063\n",
      "Step 2395: train loss: 0.4046922028064728\n",
      "Step 2395: val loss: 0.3898319900035858\n",
      "Step 2396: train loss: 0.40439605712890625\n",
      "Step 2396: val loss: 0.38955652713775635\n",
      "Step 2397: train loss: 0.4041003882884979\n",
      "Step 2397: val loss: 0.38928166031837463\n",
      "Step 2398: train loss: 0.40380528569221497\n",
      "Step 2398: val loss: 0.38900724053382874\n",
      "Step 2399: train loss: 0.4035106897354126\n",
      "Step 2399: val loss: 0.3887333869934082\n",
      "Step 2400: train loss: 0.40321677923202515\n",
      "Step 2400: val loss: 0.3884601891040802\n",
      "Step 2401: train loss: 0.40292322635650635\n",
      "Step 2401: val loss: 0.3881872892379761\n",
      "Step 2402: train loss: 0.4026302397251129\n",
      "Step 2402: val loss: 0.38791507482528687\n",
      "Step 2403: train loss: 0.4023378789424896\n",
      "Step 2403: val loss: 0.38764330744743347\n",
      "Step 2404: train loss: 0.4020460844039917\n",
      "Step 2404: val loss: 0.387372225522995\n",
      "Step 2405: train loss: 0.4017547070980072\n",
      "Step 2405: val loss: 0.38710153102874756\n",
      "Step 2406: train loss: 0.4014638662338257\n",
      "Step 2406: val loss: 0.3868314325809479\n",
      "Step 2407: train loss: 0.40117359161376953\n",
      "Step 2407: val loss: 0.3865618407726288\n",
      "Step 2408: train loss: 0.4008839428424835\n",
      "Step 2408: val loss: 0.3862927258014679\n",
      "Step 2409: train loss: 0.40059468150138855\n",
      "Step 2409: val loss: 0.38602414727211\n",
      "Step 2410: train loss: 0.40030598640441895\n",
      "Step 2410: val loss: 0.38575610518455505\n",
      "Step 2411: train loss: 0.4000178873538971\n",
      "Step 2411: val loss: 0.38548848032951355\n",
      "Step 2412: train loss: 0.39973023533821106\n",
      "Step 2412: val loss: 0.3852214515209198\n",
      "Step 2413: train loss: 0.399443119764328\n",
      "Step 2413: val loss: 0.3849550187587738\n",
      "Step 2414: train loss: 0.39915671944618225\n",
      "Step 2414: val loss: 0.38468900322914124\n",
      "Step 2415: train loss: 0.3988705575466156\n",
      "Step 2415: val loss: 0.38442355394363403\n",
      "Step 2416: train loss: 0.39858514070510864\n",
      "Step 2416: val loss: 0.3841584324836731\n",
      "Step 2417: train loss: 0.3982999324798584\n",
      "Step 2417: val loss: 0.38389378786087036\n",
      "Step 2418: train loss: 0.39801523089408875\n",
      "Step 2418: val loss: 0.3836296796798706\n",
      "Step 2419: train loss: 0.39773106575012207\n",
      "Step 2419: val loss: 0.3833659887313843\n",
      "Step 2420: train loss: 0.3974474370479584\n",
      "Step 2420: val loss: 0.3831029534339905\n",
      "Step 2421: train loss: 0.39716431498527527\n",
      "Step 2421: val loss: 0.38284045457839966\n",
      "Step 2422: train loss: 0.39688175916671753\n",
      "Step 2422: val loss: 0.38257837295532227\n",
      "Step 2423: train loss: 0.396599680185318\n",
      "Step 2423: val loss: 0.3823166489601135\n",
      "Step 2424: train loss: 0.39631810784339905\n",
      "Step 2424: val loss: 0.38205572962760925\n",
      "Step 2425: train loss: 0.39603710174560547\n",
      "Step 2425: val loss: 0.38179510831832886\n",
      "Step 2426: train loss: 0.39575663208961487\n",
      "Step 2426: val loss: 0.3815350830554962\n",
      "Step 2427: train loss: 0.3954766094684601\n",
      "Step 2427: val loss: 0.3812755346298218\n",
      "Step 2428: train loss: 0.39519715309143066\n",
      "Step 2428: val loss: 0.3810165524482727\n",
      "Step 2429: train loss: 0.39491814374923706\n",
      "Step 2429: val loss: 0.38075798749923706\n",
      "Step 2430: train loss: 0.39463984966278076\n",
      "Step 2430: val loss: 0.380499929189682\n",
      "Step 2431: train loss: 0.39436182379722595\n",
      "Step 2431: val loss: 0.38024237751960754\n",
      "Step 2432: train loss: 0.39408448338508606\n",
      "Step 2432: val loss: 0.37998542189598083\n",
      "Step 2433: train loss: 0.39380761981010437\n",
      "Step 2433: val loss: 0.3797288239002228\n",
      "Step 2434: train loss: 0.39353126287460327\n",
      "Step 2434: val loss: 0.379472941160202\n",
      "Step 2435: train loss: 0.39325541257858276\n",
      "Step 2435: val loss: 0.3792174160480499\n",
      "Step 2436: train loss: 0.39298003911972046\n",
      "Step 2436: val loss: 0.37896236777305603\n",
      "Step 2437: train loss: 0.3927052319049835\n",
      "Step 2437: val loss: 0.3787079453468323\n",
      "Step 2438: train loss: 0.3924309313297272\n",
      "Step 2438: val loss: 0.37845396995544434\n",
      "Step 2439: train loss: 0.39215704798698425\n",
      "Step 2439: val loss: 0.3782004117965698\n",
      "Step 2440: train loss: 0.3918837904930115\n",
      "Step 2440: val loss: 0.3779473900794983\n",
      "Step 2441: train loss: 0.3916109800338745\n",
      "Step 2441: val loss: 0.37769484519958496\n",
      "Step 2442: train loss: 0.39133861660957336\n",
      "Step 2442: val loss: 0.3774429261684418\n",
      "Step 2443: train loss: 0.39106693863868713\n",
      "Step 2443: val loss: 0.37719112634658813\n",
      "Step 2444: train loss: 0.39079537987709045\n",
      "Step 2444: val loss: 0.37693989276885986\n",
      "Step 2445: train loss: 0.39052441716194153\n",
      "Step 2445: val loss: 0.37668919563293457\n",
      "Step 2446: train loss: 0.3902539610862732\n",
      "Step 2446: val loss: 0.3764389753341675\n",
      "Step 2447: train loss: 0.3899838328361511\n",
      "Step 2447: val loss: 0.3761892020702362\n",
      "Step 2448: train loss: 0.38971447944641113\n",
      "Step 2448: val loss: 0.3759400546550751\n",
      "Step 2449: train loss: 0.389445424079895\n",
      "Step 2449: val loss: 0.3756912052631378\n",
      "Step 2450: train loss: 0.38917696475982666\n",
      "Step 2450: val loss: 0.3754430115222931\n",
      "Step 2451: train loss: 0.38890907168388367\n",
      "Step 2451: val loss: 0.37519514560699463\n",
      "Step 2452: train loss: 0.38864150643348694\n",
      "Step 2452: val loss: 0.3749479353427887\n",
      "Step 2453: train loss: 0.3883745074272156\n",
      "Step 2453: val loss: 0.3747010827064514\n",
      "Step 2454: train loss: 0.38810810446739197\n",
      "Step 2454: val loss: 0.3744547963142395\n",
      "Step 2455: train loss: 0.387842059135437\n",
      "Step 2455: val loss: 0.3742089569568634\n",
      "Step 2456: train loss: 0.3875766098499298\n",
      "Step 2456: val loss: 0.3739636242389679\n",
      "Step 2457: train loss: 0.3873116374015808\n",
      "Step 2457: val loss: 0.3737187385559082\n",
      "Step 2458: train loss: 0.3870471715927124\n",
      "Step 2458: val loss: 0.3734743297100067\n",
      "Step 2459: train loss: 0.3867831528186798\n",
      "Step 2459: val loss: 0.3732304573059082\n",
      "Step 2460: train loss: 0.3865196406841278\n",
      "Step 2460: val loss: 0.3729870021343231\n",
      "Step 2461: train loss: 0.38625669479370117\n",
      "Step 2461: val loss: 0.37274399399757385\n",
      "Step 2462: train loss: 0.38599392771720886\n",
      "Step 2462: val loss: 0.37250131368637085\n",
      "Step 2463: train loss: 0.3857317566871643\n",
      "Step 2463: val loss: 0.3722591996192932\n",
      "Step 2464: train loss: 0.3854700028896332\n",
      "Step 2464: val loss: 0.37201744318008423\n",
      "Step 2465: train loss: 0.3852088153362274\n",
      "Step 2465: val loss: 0.3717763125896454\n",
      "Step 2466: train loss: 0.38494792580604553\n",
      "Step 2466: val loss: 0.3715355396270752\n",
      "Step 2467: train loss: 0.3846876919269562\n",
      "Step 2467: val loss: 0.37129536271095276\n",
      "Step 2468: train loss: 0.3844279646873474\n",
      "Step 2468: val loss: 0.37105563282966614\n",
      "Step 2469: train loss: 0.3841685354709625\n",
      "Step 2469: val loss: 0.37081634998321533\n",
      "Step 2470: train loss: 0.38390985131263733\n",
      "Step 2470: val loss: 0.37057745456695557\n",
      "Step 2471: train loss: 0.3836514949798584\n",
      "Step 2471: val loss: 0.37033918499946594\n",
      "Step 2472: train loss: 0.38339367508888245\n",
      "Step 2472: val loss: 0.37010127305984497\n",
      "Step 2473: train loss: 0.3831363916397095\n",
      "Step 2473: val loss: 0.3698638379573822\n",
      "Step 2474: train loss: 0.3828795254230499\n",
      "Step 2474: val loss: 0.36962705850601196\n",
      "Step 2475: train loss: 0.3826231062412262\n",
      "Step 2475: val loss: 0.36939042806625366\n",
      "Step 2476: train loss: 0.38236701488494873\n",
      "Step 2476: val loss: 0.3691542446613312\n",
      "Step 2477: train loss: 0.38211140036582947\n",
      "Step 2477: val loss: 0.3689185380935669\n",
      "Step 2478: train loss: 0.38185620307922363\n",
      "Step 2478: val loss: 0.36868324875831604\n",
      "Step 2479: train loss: 0.38160160183906555\n",
      "Step 2479: val loss: 0.36844852566719055\n",
      "Step 2480: train loss: 0.38134732842445374\n",
      "Step 2480: val loss: 0.3682142198085785\n",
      "Step 2481: train loss: 0.3810936510562897\n",
      "Step 2481: val loss: 0.3679804801940918\n",
      "Step 2482: train loss: 0.3808404207229614\n",
      "Step 2482: val loss: 0.36774712800979614\n",
      "Step 2483: train loss: 0.38058769702911377\n",
      "Step 2483: val loss: 0.3675142228603363\n",
      "Step 2484: train loss: 0.38033542037010193\n",
      "Step 2484: val loss: 0.3672817647457123\n",
      "Step 2485: train loss: 0.38008368015289307\n",
      "Step 2485: val loss: 0.36704981327056885\n",
      "Step 2486: train loss: 0.37983229756355286\n",
      "Step 2486: val loss: 0.3668183386325836\n",
      "Step 2487: train loss: 0.3795815706253052\n",
      "Step 2487: val loss: 0.36658719182014465\n",
      "Step 2488: train loss: 0.3793308734893799\n",
      "Step 2488: val loss: 0.3663563132286072\n",
      "Step 2489: train loss: 0.3790808320045471\n",
      "Step 2489: val loss: 0.366126149892807\n",
      "Step 2490: train loss: 0.3788311779499054\n",
      "Step 2490: val loss: 0.3658963143825531\n",
      "Step 2491: train loss: 0.3785821199417114\n",
      "Step 2491: val loss: 0.3656669557094574\n",
      "Step 2492: train loss: 0.3783333897590637\n",
      "Step 2492: val loss: 0.36543798446655273\n",
      "Step 2493: train loss: 0.3780851364135742\n",
      "Step 2493: val loss: 0.36520975828170776\n",
      "Step 2494: train loss: 0.3778374493122101\n",
      "Step 2494: val loss: 0.36498159170150757\n",
      "Step 2495: train loss: 0.377590149641037\n",
      "Step 2495: val loss: 0.3647540807723999\n",
      "Step 2496: train loss: 0.37734344601631165\n",
      "Step 2496: val loss: 0.36452704668045044\n",
      "Step 2497: train loss: 0.3770970404148102\n",
      "Step 2497: val loss: 0.3643004894256592\n",
      "Step 2498: train loss: 0.37685129046440125\n",
      "Step 2498: val loss: 0.36407414078712463\n",
      "Step 2499: train loss: 0.37660568952560425\n",
      "Step 2499: val loss: 0.36384817957878113\n",
      "Step 2500: train loss: 0.37636053562164307\n",
      "Step 2500: val loss: 0.363622784614563\n",
      "Step 2501: train loss: 0.3761158585548401\n",
      "Step 2501: val loss: 0.3633977770805359\n",
      "Step 2502: train loss: 0.3758717179298401\n",
      "Step 2502: val loss: 0.3631732165813446\n",
      "Step 2503: train loss: 0.3756278455257416\n",
      "Step 2503: val loss: 0.36294910311698914\n",
      "Step 2504: train loss: 0.3753846287727356\n",
      "Step 2504: val loss: 0.36272555589675903\n",
      "Step 2505: train loss: 0.37514179944992065\n",
      "Step 2505: val loss: 0.3625023365020752\n",
      "Step 2506: train loss: 0.3748995065689087\n",
      "Step 2506: val loss: 0.3622797131538391\n",
      "Step 2507: train loss: 0.37465769052505493\n",
      "Step 2507: val loss: 0.3620574176311493\n",
      "Step 2508: train loss: 0.3744162917137146\n",
      "Step 2508: val loss: 0.3618354797363281\n",
      "Step 2509: train loss: 0.37417513132095337\n",
      "Step 2509: val loss: 0.3616139590740204\n",
      "Step 2510: train loss: 0.37393444776535034\n",
      "Step 2510: val loss: 0.3613928258419037\n",
      "Step 2511: train loss: 0.37369412183761597\n",
      "Step 2511: val loss: 0.3611721098423004\n",
      "Step 2512: train loss: 0.37345439195632935\n",
      "Step 2512: val loss: 0.3609519600868225\n",
      "Step 2513: train loss: 0.37321504950523376\n",
      "Step 2513: val loss: 0.3607320785522461\n",
      "Step 2514: train loss: 0.3729761838912964\n",
      "Step 2514: val loss: 0.3605128228664398\n",
      "Step 2515: train loss: 0.3727377653121948\n",
      "Step 2515: val loss: 0.3602939546108246\n",
      "Step 2516: train loss: 0.37249988317489624\n",
      "Step 2516: val loss: 0.36007556319236755\n",
      "Step 2517: train loss: 0.37226229906082153\n",
      "Step 2517: val loss: 0.35985732078552246\n",
      "Step 2518: train loss: 0.37202510237693787\n",
      "Step 2518: val loss: 0.35963964462280273\n",
      "Step 2519: train loss: 0.37178835272789\n",
      "Step 2519: val loss: 0.3594224154949188\n",
      "Step 2520: train loss: 0.3715519607067108\n",
      "Step 2520: val loss: 0.3592054843902588\n",
      "Step 2521: train loss: 0.3713161051273346\n",
      "Step 2521: val loss: 0.3589891493320465\n",
      "Step 2522: train loss: 0.3710807263851166\n",
      "Step 2522: val loss: 0.3587730824947357\n",
      "Step 2523: train loss: 0.3708456754684448\n",
      "Step 2523: val loss: 0.35855746269226074\n",
      "Step 2524: train loss: 0.37061113119125366\n",
      "Step 2524: val loss: 0.3583424687385559\n",
      "Step 2525: train loss: 0.37037715315818787\n",
      "Step 2525: val loss: 0.3581276834011078\n",
      "Step 2526: train loss: 0.37014323472976685\n",
      "Step 2526: val loss: 0.3579132556915283\n",
      "Step 2527: train loss: 0.3699099123477936\n",
      "Step 2527: val loss: 0.35769933462142944\n",
      "Step 2528: train loss: 0.36967694759368896\n",
      "Step 2528: val loss: 0.3574858605861664\n",
      "Step 2529: train loss: 0.36944448947906494\n",
      "Step 2529: val loss: 0.3572726845741272\n",
      "Step 2530: train loss: 0.36921244859695435\n",
      "Step 2530: val loss: 0.357060045003891\n",
      "Step 2531: train loss: 0.3689810037612915\n",
      "Step 2531: val loss: 0.3568477928638458\n",
      "Step 2532: train loss: 0.36874979734420776\n",
      "Step 2532: val loss: 0.35663604736328125\n",
      "Step 2533: train loss: 0.3685190975666046\n",
      "Step 2533: val loss: 0.3564246594905853\n",
      "Step 2534: train loss: 0.36828887462615967\n",
      "Step 2534: val loss: 0.3562135398387909\n",
      "Step 2535: train loss: 0.36805880069732666\n",
      "Step 2535: val loss: 0.3560028672218323\n",
      "Step 2536: train loss: 0.36782923340797424\n",
      "Step 2536: val loss: 0.35579267144203186\n",
      "Step 2537: train loss: 0.3676001727581024\n",
      "Step 2537: val loss: 0.3555830121040344\n",
      "Step 2538: train loss: 0.367371529340744\n",
      "Step 2538: val loss: 0.3553735315799713\n",
      "Step 2539: train loss: 0.36714330315589905\n",
      "Step 2539: val loss: 0.3551645874977112\n",
      "Step 2540: train loss: 0.3669155240058899\n",
      "Step 2540: val loss: 0.35495615005493164\n",
      "Step 2541: train loss: 0.3666881024837494\n",
      "Step 2541: val loss: 0.35474786162376404\n",
      "Step 2542: train loss: 0.3664611279964447\n",
      "Step 2542: val loss: 0.35454005002975464\n",
      "Step 2543: train loss: 0.36623436212539673\n",
      "Step 2543: val loss: 0.35433250665664673\n",
      "Step 2544: train loss: 0.36600810289382935\n",
      "Step 2544: val loss: 0.3541256785392761\n",
      "Step 2545: train loss: 0.36578235030174255\n",
      "Step 2545: val loss: 0.3539190888404846\n",
      "Step 2546: train loss: 0.3655570447444916\n",
      "Step 2546: val loss: 0.35371294617652893\n",
      "Step 2547: train loss: 0.3653320372104645\n",
      "Step 2547: val loss: 0.3535071611404419\n",
      "Step 2548: train loss: 0.3651076555252075\n",
      "Step 2548: val loss: 0.35330185294151306\n",
      "Step 2549: train loss: 0.3648834824562073\n",
      "Step 2549: val loss: 0.3530968725681305\n",
      "Step 2550: train loss: 0.36465972661972046\n",
      "Step 2550: val loss: 0.3528922498226166\n",
      "Step 2551: train loss: 0.36443641781806946\n",
      "Step 2551: val loss: 0.3526880145072937\n",
      "Step 2552: train loss: 0.3642134368419647\n",
      "Step 2552: val loss: 0.3524843156337738\n",
      "Step 2553: train loss: 0.36399102210998535\n",
      "Step 2553: val loss: 0.3522808253765106\n",
      "Step 2554: train loss: 0.36376896500587463\n",
      "Step 2554: val loss: 0.3520779013633728\n",
      "Step 2555: train loss: 0.36354726552963257\n",
      "Step 2555: val loss: 0.351875364780426\n",
      "Step 2556: train loss: 0.3633260428905487\n",
      "Step 2556: val loss: 0.3516731858253479\n",
      "Step 2557: train loss: 0.36310499906539917\n",
      "Step 2557: val loss: 0.3514712452888489\n",
      "Step 2558: train loss: 0.3628845512866974\n",
      "Step 2558: val loss: 0.3512699007987976\n",
      "Step 2559: train loss: 0.3626644015312195\n",
      "Step 2559: val loss: 0.351068913936615\n",
      "Step 2560: train loss: 0.36244481801986694\n",
      "Step 2560: val loss: 0.35086825489997864\n",
      "Step 2561: train loss: 0.36222556233406067\n",
      "Step 2561: val loss: 0.35066816210746765\n",
      "Step 2562: train loss: 0.36200666427612305\n",
      "Step 2562: val loss: 0.35046809911727905\n",
      "Step 2563: train loss: 0.3617880344390869\n",
      "Step 2563: val loss: 0.35026851296424866\n",
      "Step 2564: train loss: 0.36156991124153137\n",
      "Step 2564: val loss: 0.35006940364837646\n",
      "Step 2565: train loss: 0.3613521456718445\n",
      "Step 2565: val loss: 0.3498707413673401\n",
      "Step 2566: train loss: 0.3611348569393158\n",
      "Step 2566: val loss: 0.34967240691185\n",
      "Step 2567: train loss: 0.3609178960323334\n",
      "Step 2567: val loss: 0.3494745194911957\n",
      "Step 2568: train loss: 0.36070141196250916\n",
      "Step 2568: val loss: 0.3492770493030548\n",
      "Step 2569: train loss: 0.36048540472984314\n",
      "Step 2569: val loss: 0.34907978773117065\n",
      "Step 2570: train loss: 0.3602695167064667\n",
      "Step 2570: val loss: 0.3488829433917999\n",
      "Step 2571: train loss: 0.36005425453186035\n",
      "Step 2571: val loss: 0.3486865162849426\n",
      "Step 2572: train loss: 0.35983923077583313\n",
      "Step 2572: val loss: 0.3484904170036316\n",
      "Step 2573: train loss: 0.3596246838569641\n",
      "Step 2573: val loss: 0.34829479455947876\n",
      "Step 2574: train loss: 0.35941049456596375\n",
      "Step 2574: val loss: 0.34809961915016174\n",
      "Step 2575: train loss: 0.35919687151908875\n",
      "Step 2575: val loss: 0.347904771566391\n",
      "Step 2576: train loss: 0.35898342728614807\n",
      "Step 2576: val loss: 0.34771010279655457\n",
      "Step 2577: train loss: 0.3587702512741089\n",
      "Step 2577: val loss: 0.3475160002708435\n",
      "Step 2578: train loss: 0.35855767130851746\n",
      "Step 2578: val loss: 0.3473222851753235\n",
      "Step 2579: train loss: 0.35834547877311707\n",
      "Step 2579: val loss: 0.3471289277076721\n",
      "Step 2580: train loss: 0.3581337034702301\n",
      "Step 2580: val loss: 0.3469359874725342\n",
      "Step 2581: train loss: 0.3579223155975342\n",
      "Step 2581: val loss: 0.34674328565597534\n",
      "Step 2582: train loss: 0.35771119594573975\n",
      "Step 2582: val loss: 0.3465510308742523\n",
      "Step 2583: train loss: 0.3575003445148468\n",
      "Step 2583: val loss: 0.3463590443134308\n",
      "Step 2584: train loss: 0.35728999972343445\n",
      "Step 2584: val loss: 0.34616759419441223\n",
      "Step 2585: train loss: 0.3570800721645355\n",
      "Step 2585: val loss: 0.3459763526916504\n",
      "Step 2586: train loss: 0.35687053203582764\n",
      "Step 2586: val loss: 0.3457857370376587\n",
      "Step 2587: train loss: 0.35666152834892273\n",
      "Step 2587: val loss: 0.34559524059295654\n",
      "Step 2588: train loss: 0.3564526438713074\n",
      "Step 2588: val loss: 0.3454052209854126\n",
      "Step 2589: train loss: 0.35624417662620544\n",
      "Step 2589: val loss: 0.34521549940109253\n",
      "Step 2590: train loss: 0.35603606700897217\n",
      "Step 2590: val loss: 0.3450262248516083\n",
      "Step 2591: train loss: 0.3558284342288971\n",
      "Step 2591: val loss: 0.3448373079299927\n",
      "Step 2592: train loss: 0.35562121868133545\n",
      "Step 2592: val loss: 0.34464889764785767\n",
      "Step 2593: train loss: 0.35541439056396484\n",
      "Step 2593: val loss: 0.3444605767726898\n",
      "Step 2594: train loss: 0.35520780086517334\n",
      "Step 2594: val loss: 0.34427276253700256\n",
      "Step 2595: train loss: 0.35500165820121765\n",
      "Step 2595: val loss: 0.34408533573150635\n",
      "Step 2596: train loss: 0.35479578375816345\n",
      "Step 2596: val loss: 0.343898206949234\n",
      "Step 2597: train loss: 0.35459044575691223\n",
      "Step 2597: val loss: 0.3437114953994751\n",
      "Step 2598: train loss: 0.35438549518585205\n",
      "Step 2598: val loss: 0.34352514147758484\n",
      "Step 2599: train loss: 0.3541809618473053\n",
      "Step 2599: val loss: 0.34333911538124084\n",
      "Step 2600: train loss: 0.35397660732269287\n",
      "Step 2600: val loss: 0.3431534469127655\n",
      "Step 2601: train loss: 0.3537726104259491\n",
      "Step 2601: val loss: 0.3429681658744812\n",
      "Step 2602: train loss: 0.3535691499710083\n",
      "Step 2602: val loss: 0.3427833318710327\n",
      "Step 2603: train loss: 0.35336601734161377\n",
      "Step 2603: val loss: 0.34259873628616333\n",
      "Step 2604: train loss: 0.35316330194473267\n",
      "Step 2604: val loss: 0.3424144983291626\n",
      "Step 2605: train loss: 0.3529607653617859\n",
      "Step 2605: val loss: 0.34223058819770813\n",
      "Step 2606: train loss: 0.35275861620903015\n",
      "Step 2606: val loss: 0.3420470952987671\n",
      "Step 2607: train loss: 0.35255685448646545\n",
      "Step 2607: val loss: 0.3418640196323395\n",
      "Step 2608: train loss: 0.3523556590080261\n",
      "Step 2608: val loss: 0.3416811525821686\n",
      "Step 2609: train loss: 0.3521546721458435\n",
      "Step 2609: val loss: 0.34149888157844543\n",
      "Step 2610: train loss: 0.3519541621208191\n",
      "Step 2610: val loss: 0.34131667017936707\n",
      "Step 2611: train loss: 0.351753830909729\n",
      "Step 2611: val loss: 0.34113505482673645\n",
      "Step 2612: train loss: 0.35155394673347473\n",
      "Step 2612: val loss: 0.34095361828804016\n",
      "Step 2613: train loss: 0.3513544499874115\n",
      "Step 2613: val loss: 0.3407726585865021\n",
      "Step 2614: train loss: 0.3511553406715393\n",
      "Step 2614: val loss: 0.3405921161174774\n",
      "Step 2615: train loss: 0.35095667839050293\n",
      "Step 2615: val loss: 0.3404116928577423\n",
      "Step 2616: train loss: 0.3507581651210785\n",
      "Step 2616: val loss: 0.34023183584213257\n",
      "Step 2617: train loss: 0.3505600690841675\n",
      "Step 2617: val loss: 0.34005215764045715\n",
      "Step 2618: train loss: 0.3503623604774475\n",
      "Step 2618: val loss: 0.33987295627593994\n",
      "Step 2619: train loss: 0.3501649796962738\n",
      "Step 2619: val loss: 0.33969399333000183\n",
      "Step 2620: train loss: 0.3499680757522583\n",
      "Step 2620: val loss: 0.3395153880119324\n",
      "Step 2621: train loss: 0.3497714698314667\n",
      "Step 2621: val loss: 0.33933717012405396\n",
      "Step 2622: train loss: 0.34957510232925415\n",
      "Step 2622: val loss: 0.33915942907333374\n",
      "Step 2623: train loss: 0.34937921166419983\n",
      "Step 2623: val loss: 0.3389819264411926\n",
      "Step 2624: train loss: 0.3491836488246918\n",
      "Step 2624: val loss: 0.3388047516345978\n",
      "Step 2625: train loss: 0.34898853302001953\n",
      "Step 2625: val loss: 0.3386278450489044\n",
      "Step 2626: train loss: 0.34879356622695923\n",
      "Step 2626: val loss: 0.3384513556957245\n",
      "Step 2627: train loss: 0.34859907627105713\n",
      "Step 2627: val loss: 0.3382751941680908\n",
      "Step 2628: train loss: 0.3484049141407013\n",
      "Step 2628: val loss: 0.3380993604660034\n",
      "Step 2629: train loss: 0.3482111692428589\n",
      "Step 2629: val loss: 0.3379240334033966\n",
      "Step 2630: train loss: 0.34801772236824036\n",
      "Step 2630: val loss: 0.33774882555007935\n",
      "Step 2631: train loss: 0.3478246331214905\n",
      "Step 2631: val loss: 0.3375740349292755\n",
      "Step 2632: train loss: 0.34763193130493164\n",
      "Step 2632: val loss: 0.3373997211456299\n",
      "Step 2633: train loss: 0.34743955731391907\n",
      "Step 2633: val loss: 0.3372255563735962\n",
      "Step 2634: train loss: 0.34724748134613037\n",
      "Step 2634: val loss: 0.3370519280433655\n",
      "Step 2635: train loss: 0.34705591201782227\n",
      "Step 2635: val loss: 0.3368785083293915\n",
      "Step 2636: train loss: 0.3468644320964813\n",
      "Step 2636: val loss: 0.3367053270339966\n",
      "Step 2637: train loss: 0.3466733992099762\n",
      "Step 2637: val loss: 0.3365326523780823\n",
      "Step 2638: train loss: 0.3464828133583069\n",
      "Step 2638: val loss: 0.33636021614074707\n",
      "Step 2639: train loss: 0.34629255533218384\n",
      "Step 2639: val loss: 0.33618825674057007\n",
      "Step 2640: train loss: 0.34610268473625183\n",
      "Step 2640: val loss: 0.3360165059566498\n",
      "Step 2641: train loss: 0.34591302275657654\n",
      "Step 2641: val loss: 0.3358452022075653\n",
      "Step 2642: train loss: 0.3457237780094147\n",
      "Step 2642: val loss: 0.33567410707473755\n",
      "Step 2643: train loss: 0.3455348014831543\n",
      "Step 2643: val loss: 0.3355034291744232\n",
      "Step 2644: train loss: 0.34534627199172974\n",
      "Step 2644: val loss: 0.3353331983089447\n",
      "Step 2645: train loss: 0.3451581299304962\n",
      "Step 2645: val loss: 0.33516302704811096\n",
      "Step 2646: train loss: 0.3449702560901642\n",
      "Step 2646: val loss: 0.3349933326244354\n",
      "Step 2647: train loss: 0.34478268027305603\n",
      "Step 2647: val loss: 0.334823876619339\n",
      "Step 2648: train loss: 0.3445954918861389\n",
      "Step 2648: val loss: 0.33465495705604553\n",
      "Step 2649: train loss: 0.34440866112709045\n",
      "Step 2649: val loss: 0.33448609709739685\n",
      "Step 2650: train loss: 0.34422197937965393\n",
      "Step 2650: val loss: 0.3343176245689392\n",
      "Step 2651: train loss: 0.3440358638763428\n",
      "Step 2651: val loss: 0.3341495394706726\n",
      "Step 2652: train loss: 0.34384995698928833\n",
      "Step 2652: val loss: 0.3339819312095642\n",
      "Step 2653: train loss: 0.3436644375324249\n",
      "Step 2653: val loss: 0.33381447196006775\n",
      "Step 2654: train loss: 0.3434793949127197\n",
      "Step 2654: val loss: 0.33364740014076233\n",
      "Step 2655: train loss: 0.34329450130462646\n",
      "Step 2655: val loss: 0.3334805369377136\n",
      "Step 2656: train loss: 0.34310993552207947\n",
      "Step 2656: val loss: 0.3333141505718231\n",
      "Step 2657: train loss: 0.3429257869720459\n",
      "Step 2657: val loss: 0.3331480622291565\n",
      "Step 2658: train loss: 0.342741996049881\n",
      "Step 2658: val loss: 0.3329821825027466\n",
      "Step 2659: train loss: 0.3425584137439728\n",
      "Step 2659: val loss: 0.3328167200088501\n",
      "Step 2660: train loss: 0.3423752188682556\n",
      "Step 2660: val loss: 0.3326515555381775\n",
      "Step 2661: train loss: 0.3421923518180847\n",
      "Step 2661: val loss: 0.33248674869537354\n",
      "Step 2662: train loss: 0.34200990200042725\n",
      "Step 2662: val loss: 0.332322359085083\n",
      "Step 2663: train loss: 0.34182772040367126\n",
      "Step 2663: val loss: 0.33215805888175964\n",
      "Step 2664: train loss: 0.34164589643478394\n",
      "Step 2664: val loss: 0.3319942355155945\n",
      "Step 2665: train loss: 0.3414643406867981\n",
      "Step 2665: val loss: 0.3318306803703308\n",
      "Step 2666: train loss: 0.34128326177597046\n",
      "Step 2666: val loss: 0.3316675126552582\n",
      "Step 2667: train loss: 0.34110239148139954\n",
      "Step 2667: val loss: 0.33150458335876465\n",
      "Step 2668: train loss: 0.3409218192100525\n",
      "Step 2668: val loss: 0.33134204149246216\n",
      "Step 2669: train loss: 0.3407415747642517\n",
      "Step 2669: val loss: 0.33117973804473877\n",
      "Step 2670: train loss: 0.3405616879463196\n",
      "Step 2670: val loss: 0.33101794123649597\n",
      "Step 2671: train loss: 0.3403821885585785\n",
      "Step 2671: val loss: 0.3308563530445099\n",
      "Step 2672: train loss: 0.3402031362056732\n",
      "Step 2672: val loss: 0.3306950330734253\n",
      "Step 2673: train loss: 0.3400242030620575\n",
      "Step 2673: val loss: 0.330533891916275\n",
      "Step 2674: train loss: 0.33984559774398804\n",
      "Step 2674: val loss: 0.33037325739860535\n",
      "Step 2675: train loss: 0.33966729044914246\n",
      "Step 2675: val loss: 0.3302129805088043\n",
      "Step 2676: train loss: 0.33948957920074463\n",
      "Step 2676: val loss: 0.33005282282829285\n",
      "Step 2677: train loss: 0.3393118381500244\n",
      "Step 2677: val loss: 0.32989302277565\n",
      "Step 2678: train loss: 0.3391346037387848\n",
      "Step 2678: val loss: 0.3297337591648102\n",
      "Step 2679: train loss: 0.33895769715309143\n",
      "Step 2679: val loss: 0.3295746445655823\n",
      "Step 2680: train loss: 0.3387811481952667\n",
      "Step 2680: val loss: 0.32941576838493347\n",
      "Step 2681: train loss: 0.33860471844673157\n",
      "Step 2681: val loss: 0.32925722002983093\n",
      "Step 2682: train loss: 0.33842870593070984\n",
      "Step 2682: val loss: 0.32909905910491943\n",
      "Step 2683: train loss: 0.33825308084487915\n",
      "Step 2683: val loss: 0.3289412260055542\n",
      "Step 2684: train loss: 0.33807772397994995\n",
      "Step 2684: val loss: 0.32878369092941284\n",
      "Step 2685: train loss: 0.33790263533592224\n",
      "Step 2685: val loss: 0.328626424074173\n",
      "Step 2686: train loss: 0.33772793412208557\n",
      "Step 2686: val loss: 0.32846948504447937\n",
      "Step 2687: train loss: 0.33755356073379517\n",
      "Step 2687: val loss: 0.3283129632472992\n",
      "Step 2688: train loss: 0.337379515171051\n",
      "Step 2688: val loss: 0.32815638184547424\n",
      "Step 2689: train loss: 0.3372056186199188\n",
      "Step 2689: val loss: 0.32800036668777466\n",
      "Step 2690: train loss: 0.3370322287082672\n",
      "Step 2690: val loss: 0.32784467935562134\n",
      "Step 2691: train loss: 0.3368590176105499\n",
      "Step 2691: val loss: 0.32768920063972473\n",
      "Step 2692: train loss: 0.33668631315231323\n",
      "Step 2692: val loss: 0.3275340795516968\n",
      "Step 2693: train loss: 0.3365136981010437\n",
      "Step 2693: val loss: 0.3273793160915375\n",
      "Step 2694: train loss: 0.3363414406776428\n",
      "Step 2694: val loss: 0.3272246718406677\n",
      "Step 2695: train loss: 0.3361695110797882\n",
      "Step 2695: val loss: 0.3270705044269562\n",
      "Step 2696: train loss: 0.3359980583190918\n",
      "Step 2696: val loss: 0.3269166350364685\n",
      "Step 2697: train loss: 0.33582669496536255\n",
      "Step 2697: val loss: 0.32676294445991516\n",
      "Step 2698: train loss: 0.33565574884414673\n",
      "Step 2698: val loss: 0.32660961151123047\n",
      "Step 2699: train loss: 0.33548504114151\n",
      "Step 2699: val loss: 0.326456755399704\n",
      "Step 2700: train loss: 0.33531486988067627\n",
      "Step 2700: val loss: 0.32630395889282227\n",
      "Step 2701: train loss: 0.3351447582244873\n",
      "Step 2701: val loss: 0.32615143060684204\n",
      "Step 2702: train loss: 0.3349750339984894\n",
      "Step 2702: val loss: 0.32599931955337524\n",
      "Step 2703: train loss: 0.33480557799339294\n",
      "Step 2703: val loss: 0.3258475959300995\n",
      "Step 2704: train loss: 0.33463653922080994\n",
      "Step 2704: val loss: 0.32569602131843567\n",
      "Step 2705: train loss: 0.3344676196575165\n",
      "Step 2705: val loss: 0.3255448043346405\n",
      "Step 2706: train loss: 0.3342991769313812\n",
      "Step 2706: val loss: 0.3253938555717468\n",
      "Step 2707: train loss: 0.33413103222846985\n",
      "Step 2707: val loss: 0.3252433240413666\n",
      "Step 2708: train loss: 0.3339631259441376\n",
      "Step 2708: val loss: 0.3250928223133087\n",
      "Step 2709: train loss: 0.3337954878807068\n",
      "Step 2709: val loss: 0.3249428868293762\n",
      "Step 2710: train loss: 0.3336282968521118\n",
      "Step 2710: val loss: 0.3247932195663452\n",
      "Step 2711: train loss: 0.33346128463745117\n",
      "Step 2711: val loss: 0.32464373111724854\n",
      "Step 2712: train loss: 0.33329471945762634\n",
      "Step 2712: val loss: 0.3244946002960205\n",
      "Step 2713: train loss: 0.3331283628940582\n",
      "Step 2713: val loss: 0.32434579730033875\n",
      "Step 2714: train loss: 0.3329622149467468\n",
      "Step 2714: val loss: 0.32419708371162415\n",
      "Step 2715: train loss: 0.332796573638916\n",
      "Step 2715: val loss: 0.3240490257740021\n",
      "Step 2716: train loss: 0.3326312005519867\n",
      "Step 2716: val loss: 0.32390087842941284\n",
      "Step 2717: train loss: 0.3324660062789917\n",
      "Step 2717: val loss: 0.3237532675266266\n",
      "Step 2718: train loss: 0.3323011100292206\n",
      "Step 2718: val loss: 0.32360580563545227\n",
      "Step 2719: train loss: 0.3321365416049957\n",
      "Step 2719: val loss: 0.32345855236053467\n",
      "Step 2720: train loss: 0.33197224140167236\n",
      "Step 2720: val loss: 0.32331180572509766\n",
      "Step 2721: train loss: 0.3318082392215729\n",
      "Step 2721: val loss: 0.32316523790359497\n",
      "Step 2722: train loss: 0.33164459466934204\n",
      "Step 2722: val loss: 0.3230190575122833\n",
      "Step 2723: train loss: 0.3314812481403351\n",
      "Step 2723: val loss: 0.3228730857372284\n",
      "Step 2724: train loss: 0.3313181400299072\n",
      "Step 2724: val loss: 0.3227272629737854\n",
      "Step 2725: train loss: 0.33115527033805847\n",
      "Step 2725: val loss: 0.32258185744285583\n",
      "Step 2726: train loss: 0.33099278807640076\n",
      "Step 2726: val loss: 0.3224368691444397\n",
      "Step 2727: train loss: 0.3308306634426117\n",
      "Step 2727: val loss: 0.3222919702529907\n",
      "Step 2728: train loss: 0.3306688070297241\n",
      "Step 2728: val loss: 0.3221473693847656\n",
      "Step 2729: train loss: 0.33050715923309326\n",
      "Step 2729: val loss: 0.3220031261444092\n",
      "Step 2730: train loss: 0.33034592866897583\n",
      "Step 2730: val loss: 0.3218591809272766\n",
      "Step 2731: train loss: 0.3301849663257599\n",
      "Step 2731: val loss: 0.32171547412872314\n",
      "Step 2732: train loss: 0.33002427220344543\n",
      "Step 2732: val loss: 0.3215721547603607\n",
      "Step 2733: train loss: 0.3298637270927429\n",
      "Step 2733: val loss: 0.3214290142059326\n",
      "Step 2734: train loss: 0.3297036290168762\n",
      "Step 2734: val loss: 0.3212861716747284\n",
      "Step 2735: train loss: 0.3295437693595886\n",
      "Step 2735: val loss: 0.32114356756210327\n",
      "Step 2736: train loss: 0.32938408851623535\n",
      "Step 2736: val loss: 0.3210013806819916\n",
      "Step 2737: train loss: 0.32922497391700745\n",
      "Step 2737: val loss: 0.3208593428134918\n",
      "Step 2738: train loss: 0.32906603813171387\n",
      "Step 2738: val loss: 0.32071754336357117\n",
      "Step 2739: train loss: 0.3289072811603546\n",
      "Step 2739: val loss: 0.32057610154151917\n",
      "Step 2740: train loss: 0.3287489712238312\n",
      "Step 2740: val loss: 0.3204349875450134\n",
      "Step 2741: train loss: 0.3285908102989197\n",
      "Step 2741: val loss: 0.32029399275779724\n",
      "Step 2742: train loss: 0.3284328877925873\n",
      "Step 2742: val loss: 0.32015344500541687\n",
      "Step 2743: train loss: 0.3282753527164459\n",
      "Step 2743: val loss: 0.3200131356716156\n",
      "Step 2744: train loss: 0.3281182050704956\n",
      "Step 2744: val loss: 0.31987303495407104\n",
      "Step 2745: train loss: 0.32796114683151245\n",
      "Step 2745: val loss: 0.3197331428527832\n",
      "Step 2746: train loss: 0.32780447602272034\n",
      "Step 2746: val loss: 0.3195936977863312\n",
      "Step 2747: train loss: 0.3276481330394745\n",
      "Step 2747: val loss: 0.31945446133613586\n",
      "Step 2748: train loss: 0.32749199867248535\n",
      "Step 2748: val loss: 0.3193154036998749\n",
      "Step 2749: train loss: 0.32733607292175293\n",
      "Step 2749: val loss: 0.3191767632961273\n",
      "Step 2750: train loss: 0.3271804749965668\n",
      "Step 2750: val loss: 0.3190383017063141\n",
      "Step 2751: train loss: 0.32702523469924927\n",
      "Step 2751: val loss: 0.3189001977443695\n",
      "Step 2752: train loss: 0.32687026262283325\n",
      "Step 2752: val loss: 0.31876230239868164\n",
      "Step 2753: train loss: 0.32671552896499634\n",
      "Step 2753: val loss: 0.31862473487854004\n",
      "Step 2754: train loss: 0.3265611231327057\n",
      "Step 2754: val loss: 0.318487286567688\n",
      "Step 2755: train loss: 0.32640698552131653\n",
      "Step 2755: val loss: 0.3183501362800598\n",
      "Step 2756: train loss: 0.32625308632850647\n",
      "Step 2756: val loss: 0.31821340322494507\n",
      "Step 2757: train loss: 0.3260994255542755\n",
      "Step 2757: val loss: 0.31807687878608704\n",
      "Step 2758: train loss: 0.3259460926055908\n",
      "Step 2758: val loss: 0.31794071197509766\n",
      "Step 2759: train loss: 0.32579314708709717\n",
      "Step 2759: val loss: 0.31780460476875305\n",
      "Step 2760: train loss: 0.32564032077789307\n",
      "Step 2760: val loss: 0.3176689147949219\n",
      "Step 2761: train loss: 0.3254878520965576\n",
      "Step 2761: val loss: 0.3175334632396698\n",
      "Step 2762: train loss: 0.3253357708454132\n",
      "Step 2762: val loss: 0.3173982501029968\n",
      "Step 2763: train loss: 0.3251837491989136\n",
      "Step 2763: val loss: 0.31726324558258057\n",
      "Step 2764: train loss: 0.3250320553779602\n",
      "Step 2764: val loss: 0.3171286880970001\n",
      "Step 2765: train loss: 0.3248807489871979\n",
      "Step 2765: val loss: 0.31699416041374207\n",
      "Step 2766: train loss: 0.3247295320034027\n",
      "Step 2766: val loss: 0.3168600797653198\n",
      "Step 2767: train loss: 0.32457873225212097\n",
      "Step 2767: val loss: 0.3167262077331543\n",
      "Step 2768: train loss: 0.3244282007217407\n",
      "Step 2768: val loss: 0.31659266352653503\n",
      "Step 2769: train loss: 0.3242780268192291\n",
      "Step 2769: val loss: 0.3164592683315277\n",
      "Step 2770: train loss: 0.3241279423236847\n",
      "Step 2770: val loss: 0.31632623076438904\n",
      "Step 2771: train loss: 0.32397812604904175\n",
      "Step 2771: val loss: 0.3161934018135071\n",
      "Step 2772: train loss: 0.32382863759994507\n",
      "Step 2772: val loss: 0.3160608112812042\n",
      "Step 2773: train loss: 0.32367947697639465\n",
      "Step 2773: val loss: 0.31592851877212524\n",
      "Step 2774: train loss: 0.3235306143760681\n",
      "Step 2774: val loss: 0.31579655408859253\n",
      "Step 2775: train loss: 0.32338187098503113\n",
      "Step 2775: val loss: 0.3156645894050598\n",
      "Step 2776: train loss: 0.32323330640792847\n",
      "Step 2776: val loss: 0.3155331611633301\n",
      "Step 2777: train loss: 0.3230852484703064\n",
      "Step 2777: val loss: 0.31540194153785706\n",
      "Step 2778: train loss: 0.3229374289512634\n",
      "Step 2778: val loss: 0.31527093052864075\n",
      "Step 2779: train loss: 0.32278987765312195\n",
      "Step 2779: val loss: 0.3151401877403259\n",
      "Step 2780: train loss: 0.3226424753665924\n",
      "Step 2780: val loss: 0.3150096535682678\n",
      "Step 2781: train loss: 0.3224954903125763\n",
      "Step 2781: val loss: 0.31487950682640076\n",
      "Step 2782: train loss: 0.32234877347946167\n",
      "Step 2782: val loss: 0.31474950909614563\n",
      "Step 2783: train loss: 0.32220223546028137\n",
      "Step 2783: val loss: 0.314619779586792\n",
      "Step 2784: train loss: 0.32205596566200256\n",
      "Step 2784: val loss: 0.31449031829833984\n",
      "Step 2785: train loss: 0.32191002368927\n",
      "Step 2785: val loss: 0.3143610954284668\n",
      "Step 2786: train loss: 0.321764200925827\n",
      "Step 2786: val loss: 0.31423211097717285\n",
      "Step 2787: train loss: 0.3216187655925751\n",
      "Step 2787: val loss: 0.31410351395606995\n",
      "Step 2788: train loss: 0.3214735984802246\n",
      "Step 2788: val loss: 0.3139750361442566\n",
      "Step 2789: train loss: 0.3213285505771637\n",
      "Step 2789: val loss: 0.3138468265533447\n",
      "Step 2790: train loss: 0.3211838901042938\n",
      "Step 2790: val loss: 0.3137189745903015\n",
      "Step 2791: train loss: 0.3210395276546478\n",
      "Step 2791: val loss: 0.3135911822319031\n",
      "Step 2792: train loss: 0.3208951950073242\n",
      "Step 2792: val loss: 0.3134637475013733\n",
      "Step 2793: train loss: 0.3207513689994812\n",
      "Step 2793: val loss: 0.3133365511894226\n",
      "Step 2794: train loss: 0.3206076920032501\n",
      "Step 2794: val loss: 0.3132096529006958\n",
      "Step 2795: train loss: 0.32046452164649963\n",
      "Step 2795: val loss: 0.31308287382125854\n",
      "Step 2796: train loss: 0.320321261882782\n",
      "Step 2796: val loss: 0.3129565119743347\n",
      "Step 2797: train loss: 0.32017841935157776\n",
      "Step 2797: val loss: 0.31283038854599\n",
      "Step 2798: train loss: 0.3200359642505646\n",
      "Step 2798: val loss: 0.3127044141292572\n",
      "Step 2799: train loss: 0.31989356875419617\n",
      "Step 2799: val loss: 0.3125787079334259\n",
      "Step 2800: train loss: 0.319751501083374\n",
      "Step 2800: val loss: 0.31245335936546326\n",
      "Step 2801: train loss: 0.31960970163345337\n",
      "Step 2801: val loss: 0.31232815980911255\n",
      "Step 2802: train loss: 0.31946811079978943\n",
      "Step 2802: val loss: 0.3122032582759857\n",
      "Step 2803: train loss: 0.319326788187027\n",
      "Step 2803: val loss: 0.3120785355567932\n",
      "Step 2804: train loss: 0.3191857933998108\n",
      "Step 2804: val loss: 0.311954140663147\n",
      "Step 2805: train loss: 0.3190450370311737\n",
      "Step 2805: val loss: 0.31182998418807983\n",
      "Step 2806: train loss: 0.31890442967414856\n",
      "Step 2806: val loss: 0.311706006526947\n",
      "Step 2807: train loss: 0.31876417994499207\n",
      "Step 2807: val loss: 0.31158214807510376\n",
      "Step 2808: train loss: 0.31862398982048035\n",
      "Step 2808: val loss: 0.3114587664604187\n",
      "Step 2809: train loss: 0.31848424673080444\n",
      "Step 2809: val loss: 0.3113355338573456\n",
      "Step 2810: train loss: 0.3183448314666748\n",
      "Step 2810: val loss: 0.31121259927749634\n",
      "Step 2811: train loss: 0.31820544600486755\n",
      "Step 2811: val loss: 0.31108972430229187\n",
      "Step 2812: train loss: 0.31806644797325134\n",
      "Step 2812: val loss: 0.310967355966568\n",
      "Step 2813: train loss: 0.31792765855789185\n",
      "Step 2813: val loss: 0.3108450174331665\n",
      "Step 2814: train loss: 0.31778913736343384\n",
      "Step 2814: val loss: 0.3107230067253113\n",
      "Step 2815: train loss: 0.3176508843898773\n",
      "Step 2815: val loss: 0.31060126423835754\n",
      "Step 2816: train loss: 0.3175128698348999\n",
      "Step 2816: val loss: 0.31047967076301575\n",
      "Step 2817: train loss: 0.3173750340938568\n",
      "Step 2817: val loss: 0.3103584051132202\n",
      "Step 2818: train loss: 0.31723752617836\n",
      "Step 2818: val loss: 0.3102373480796814\n",
      "Step 2819: train loss: 0.31710028648376465\n",
      "Step 2819: val loss: 0.3101164996623993\n",
      "Step 2820: train loss: 0.31696319580078125\n",
      "Step 2820: val loss: 0.30999594926834106\n",
      "Step 2821: train loss: 0.3168264329433441\n",
      "Step 2821: val loss: 0.30987560749053955\n",
      "Step 2822: train loss: 0.31668996810913086\n",
      "Step 2822: val loss: 0.3097555339336395\n",
      "Step 2823: train loss: 0.31655362248420715\n",
      "Step 2823: val loss: 0.30963563919067383\n",
      "Step 2824: train loss: 0.31641754508018494\n",
      "Step 2824: val loss: 0.30951595306396484\n",
      "Step 2825: train loss: 0.3162817060947418\n",
      "Step 2825: val loss: 0.30939653515815735\n",
      "Step 2826: train loss: 0.3161461651325226\n",
      "Step 2826: val loss: 0.30927738547325134\n",
      "Step 2827: train loss: 0.31601089239120483\n",
      "Step 2827: val loss: 0.3091585338115692\n",
      "Step 2828: train loss: 0.3158758580684662\n",
      "Step 2828: val loss: 0.3090399205684662\n",
      "Step 2829: train loss: 0.31574100255966187\n",
      "Step 2829: val loss: 0.30892136693000793\n",
      "Step 2830: train loss: 0.31560638546943665\n",
      "Step 2830: val loss: 0.3088032305240631\n",
      "Step 2831: train loss: 0.31547215580940247\n",
      "Step 2831: val loss: 0.30868515372276306\n",
      "Step 2832: train loss: 0.31533804535865784\n",
      "Step 2832: val loss: 0.3085673749446869\n",
      "Step 2833: train loss: 0.3152042329311371\n",
      "Step 2833: val loss: 0.30845001339912415\n",
      "Step 2834: train loss: 0.31507056951522827\n",
      "Step 2834: val loss: 0.3083326518535614\n",
      "Step 2835: train loss: 0.3149372637271881\n",
      "Step 2835: val loss: 0.30821558833122253\n",
      "Step 2836: train loss: 0.31480416655540466\n",
      "Step 2836: val loss: 0.30809876322746277\n",
      "Step 2837: train loss: 0.3146713674068451\n",
      "Step 2837: val loss: 0.3079821765422821\n",
      "Step 2838: train loss: 0.3145386576652527\n",
      "Step 2838: val loss: 0.30786585807800293\n",
      "Step 2839: train loss: 0.31440621614456177\n",
      "Step 2839: val loss: 0.30774983763694763\n",
      "Step 2840: train loss: 0.3142741322517395\n",
      "Step 2840: val loss: 0.30763381719589233\n",
      "Step 2841: train loss: 0.3141421377658844\n",
      "Step 2841: val loss: 0.3075181841850281\n",
      "Step 2842: train loss: 0.31401050090789795\n",
      "Step 2842: val loss: 0.3074028491973877\n",
      "Step 2843: train loss: 0.31387919187545776\n",
      "Step 2843: val loss: 0.30728763341903687\n",
      "Step 2844: train loss: 0.31374791264533997\n",
      "Step 2844: val loss: 0.30717262625694275\n",
      "Step 2845: train loss: 0.31361690163612366\n",
      "Step 2845: val loss: 0.3070579171180725\n",
      "Step 2846: train loss: 0.3134863078594208\n",
      "Step 2846: val loss: 0.30694344639778137\n",
      "Step 2847: train loss: 0.31335586309432983\n",
      "Step 2847: val loss: 0.3068290948867798\n",
      "Step 2848: train loss: 0.313225656747818\n",
      "Step 2848: val loss: 0.3067150413990021\n",
      "Step 2849: train loss: 0.3130955398082733\n",
      "Step 2849: val loss: 0.30660122632980347\n",
      "Step 2850: train loss: 0.3129656910896301\n",
      "Step 2850: val loss: 0.3064876198768616\n",
      "Step 2851: train loss: 0.3128361999988556\n",
      "Step 2851: val loss: 0.30637413263320923\n",
      "Step 2852: train loss: 0.3127068281173706\n",
      "Step 2852: val loss: 0.30626100301742554\n",
      "Step 2853: train loss: 0.3125777244567871\n",
      "Step 2853: val loss: 0.30614808201789856\n",
      "Step 2854: train loss: 0.31244897842407227\n",
      "Step 2854: val loss: 0.3060353696346283\n",
      "Step 2855: train loss: 0.3123202919960022\n",
      "Step 2855: val loss: 0.30592289566993713\n",
      "Step 2856: train loss: 0.3121919333934784\n",
      "Step 2856: val loss: 0.3058106005191803\n",
      "Step 2857: train loss: 0.3120637834072113\n",
      "Step 2857: val loss: 0.30569854378700256\n",
      "Step 2858: train loss: 0.3119358420372009\n",
      "Step 2858: val loss: 0.3055867552757263\n",
      "Step 2859: train loss: 0.31180819869041443\n",
      "Step 2859: val loss: 0.3054752051830292\n",
      "Step 2860: train loss: 0.3116808533668518\n",
      "Step 2860: val loss: 0.30536380410194397\n",
      "Step 2861: train loss: 0.31155359745025635\n",
      "Step 2861: val loss: 0.3052525520324707\n",
      "Step 2862: train loss: 0.3114266097545624\n",
      "Step 2862: val loss: 0.30514171719551086\n",
      "Step 2863: train loss: 0.3112998604774475\n",
      "Step 2863: val loss: 0.30503103137016296\n",
      "Step 2864: train loss: 0.31117334961891174\n",
      "Step 2864: val loss: 0.30492058396339417\n",
      "Step 2865: train loss: 0.3110470771789551\n",
      "Step 2865: val loss: 0.30481022596359253\n",
      "Step 2866: train loss: 0.31092092394828796\n",
      "Step 2866: val loss: 0.3047001361846924\n",
      "Step 2867: train loss: 0.31079503893852234\n",
      "Step 2867: val loss: 0.3045903742313385\n",
      "Step 2868: train loss: 0.3106694221496582\n",
      "Step 2868: val loss: 0.3044807016849518\n",
      "Step 2869: train loss: 0.3105440139770508\n",
      "Step 2869: val loss: 0.30437126755714417\n",
      "Step 2870: train loss: 0.3104189336299896\n",
      "Step 2870: val loss: 0.30426207184791565\n",
      "Step 2871: train loss: 0.3102940618991852\n",
      "Step 2871: val loss: 0.3041531443595886\n",
      "Step 2872: train loss: 0.3101693093776703\n",
      "Step 2872: val loss: 0.30404430627822876\n",
      "Step 2873: train loss: 0.3100447952747345\n",
      "Step 2873: val loss: 0.3039359152317047\n",
      "Step 2874: train loss: 0.30992060899734497\n",
      "Step 2874: val loss: 0.3038276135921478\n",
      "Step 2875: train loss: 0.3097965717315674\n",
      "Step 2875: val loss: 0.3037196099758148\n",
      "Step 2876: train loss: 0.30967286229133606\n",
      "Step 2876: val loss: 0.3036116063594818\n",
      "Step 2877: train loss: 0.30954912304878235\n",
      "Step 2877: val loss: 0.30350393056869507\n",
      "Step 2878: train loss: 0.3094257712364197\n",
      "Step 2878: val loss: 0.3033965826034546\n",
      "Step 2879: train loss: 0.3093026280403137\n",
      "Step 2879: val loss: 0.3032892644405365\n",
      "Step 2880: train loss: 0.30917972326278687\n",
      "Step 2880: val loss: 0.3031822443008423\n",
      "Step 2881: train loss: 0.30905699729919434\n",
      "Step 2881: val loss: 0.3030754029750824\n",
      "Step 2882: train loss: 0.30893462896347046\n",
      "Step 2882: val loss: 0.302968829870224\n",
      "Step 2883: train loss: 0.30881237983703613\n",
      "Step 2883: val loss: 0.30286240577697754\n",
      "Step 2884: train loss: 0.3086903691291809\n",
      "Step 2884: val loss: 0.3027562201023102\n",
      "Step 2885: train loss: 0.30856847763061523\n",
      "Step 2885: val loss: 0.30265021324157715\n",
      "Step 2886: train loss: 0.30844682455062866\n",
      "Step 2886: val loss: 0.3025445342063904\n",
      "Step 2887: train loss: 0.30832549929618835\n",
      "Step 2887: val loss: 0.3024388551712036\n",
      "Step 2888: train loss: 0.3082042932510376\n",
      "Step 2888: val loss: 0.3023335635662079\n",
      "Step 2889: train loss: 0.3080834448337555\n",
      "Step 2889: val loss: 0.3022284507751465\n",
      "Step 2890: train loss: 0.3079627752304077\n",
      "Step 2890: val loss: 0.302123486995697\n",
      "Step 2891: train loss: 0.3078421950340271\n",
      "Step 2891: val loss: 0.30201879143714905\n",
      "Step 2892: train loss: 0.30772194266319275\n",
      "Step 2892: val loss: 0.301914244890213\n",
      "Step 2893: train loss: 0.3076018691062927\n",
      "Step 2893: val loss: 0.3018099069595337\n",
      "Step 2894: train loss: 0.30748194456100464\n",
      "Step 2894: val loss: 0.301705926656723\n",
      "Step 2895: train loss: 0.3073623776435852\n",
      "Step 2895: val loss: 0.30160197615623474\n",
      "Step 2896: train loss: 0.3072429895401001\n",
      "Step 2896: val loss: 0.30149829387664795\n",
      "Step 2897: train loss: 0.3071237802505493\n",
      "Step 2897: val loss: 0.3013947606086731\n",
      "Step 2898: train loss: 0.30700480937957764\n",
      "Step 2898: val loss: 0.3012915551662445\n",
      "Step 2899: train loss: 0.3068859577178955\n",
      "Step 2899: val loss: 0.30118852853775024\n",
      "Step 2900: train loss: 0.3067674934864044\n",
      "Step 2900: val loss: 0.3010856807231903\n",
      "Step 2901: train loss: 0.3066491186618805\n",
      "Step 2901: val loss: 0.30098292231559753\n",
      "Step 2902: train loss: 0.30653101205825806\n",
      "Step 2902: val loss: 0.3008805513381958\n",
      "Step 2903: train loss: 0.30641308426856995\n",
      "Step 2903: val loss: 0.3007782995700836\n",
      "Step 2904: train loss: 0.30629533529281616\n",
      "Step 2904: val loss: 0.300676167011261\n",
      "Step 2905: train loss: 0.30617785453796387\n",
      "Step 2905: val loss: 0.300574392080307\n",
      "Step 2906: train loss: 0.3060606122016907\n",
      "Step 2906: val loss: 0.3004726469516754\n",
      "Step 2907: train loss: 0.30594345927238464\n",
      "Step 2907: val loss: 0.30037128925323486\n",
      "Step 2908: train loss: 0.3058266341686249\n",
      "Step 2908: val loss: 0.3002699613571167\n",
      "Step 2909: train loss: 0.30570995807647705\n",
      "Step 2909: val loss: 0.3001689612865448\n",
      "Step 2910: train loss: 0.3055935800075531\n",
      "Step 2910: val loss: 0.300068199634552\n",
      "Step 2911: train loss: 0.30547744035720825\n",
      "Step 2911: val loss: 0.29996758699417114\n",
      "Step 2912: train loss: 0.30536124110221863\n",
      "Step 2912: val loss: 0.2998672127723694\n",
      "Step 2913: train loss: 0.305245578289032\n",
      "Step 2913: val loss: 0.29976686835289\n",
      "Step 2914: train loss: 0.3051299452781677\n",
      "Step 2914: val loss: 0.2996669113636017\n",
      "Step 2915: train loss: 0.30501461029052734\n",
      "Step 2915: val loss: 0.2995670735836029\n",
      "Step 2916: train loss: 0.3048994243144989\n",
      "Step 2916: val loss: 0.2994675040245056\n",
      "Step 2917: train loss: 0.3047843873500824\n",
      "Step 2917: val loss: 0.29936808347702026\n",
      "Step 2918: train loss: 0.3046697676181793\n",
      "Step 2918: val loss: 0.29926881194114685\n",
      "Step 2919: train loss: 0.30455514788627625\n",
      "Step 2919: val loss: 0.2991698384284973\n",
      "Step 2920: train loss: 0.30444076657295227\n",
      "Step 2920: val loss: 0.29907095432281494\n",
      "Step 2921: train loss: 0.3043266236782074\n",
      "Step 2921: val loss: 0.2989722490310669\n",
      "Step 2922: train loss: 0.3042127788066864\n",
      "Step 2922: val loss: 0.2988739311695099\n",
      "Step 2923: train loss: 0.30409905314445496\n",
      "Step 2923: val loss: 0.29877567291259766\n",
      "Step 2924: train loss: 0.3039853870868683\n",
      "Step 2924: val loss: 0.298677533864975\n",
      "Step 2925: train loss: 0.30387216806411743\n",
      "Step 2925: val loss: 0.29857972264289856\n",
      "Step 2926: train loss: 0.3037590980529785\n",
      "Step 2926: val loss: 0.29848212003707886\n",
      "Step 2927: train loss: 0.3036460876464844\n",
      "Step 2927: val loss: 0.2983846068382263\n",
      "Step 2928: train loss: 0.3035334050655365\n",
      "Step 2928: val loss: 0.29828736186027527\n",
      "Step 2929: train loss: 0.30342090129852295\n",
      "Step 2929: val loss: 0.29819023609161377\n",
      "Step 2930: train loss: 0.30330854654312134\n",
      "Step 2930: val loss: 0.29809340834617615\n",
      "Step 2931: train loss: 0.3031964600086212\n",
      "Step 2931: val loss: 0.2979966998100281\n",
      "Step 2932: train loss: 0.3030845522880554\n",
      "Step 2932: val loss: 0.29790017008781433\n",
      "Step 2933: train loss: 0.3029729127883911\n",
      "Step 2933: val loss: 0.29780375957489014\n",
      "Step 2934: train loss: 0.3028613328933716\n",
      "Step 2934: val loss: 0.297707736492157\n",
      "Step 2935: train loss: 0.3027500510215759\n",
      "Step 2935: val loss: 0.29761192202568054\n",
      "Step 2936: train loss: 0.3026390075683594\n",
      "Step 2936: val loss: 0.2975160777568817\n",
      "Step 2937: train loss: 0.30252814292907715\n",
      "Step 2937: val loss: 0.29742056131362915\n",
      "Step 2938: train loss: 0.30241742730140686\n",
      "Step 2938: val loss: 0.2973252534866333\n",
      "Step 2939: train loss: 0.3023068606853485\n",
      "Step 2939: val loss: 0.2972300350666046\n",
      "Step 2940: train loss: 0.3021966218948364\n",
      "Step 2940: val loss: 0.2971350848674774\n",
      "Step 2941: train loss: 0.3020865321159363\n",
      "Step 2941: val loss: 0.29704028367996216\n",
      "Step 2942: train loss: 0.30197662115097046\n",
      "Step 2942: val loss: 0.2969456613063812\n",
      "Step 2943: train loss: 0.30186688899993896\n",
      "Step 2943: val loss: 0.29685133695602417\n",
      "Step 2944: train loss: 0.30175742506980896\n",
      "Step 2944: val loss: 0.2967570424079895\n",
      "Step 2945: train loss: 0.3016480505466461\n",
      "Step 2945: val loss: 0.2966631352901459\n",
      "Step 2946: train loss: 0.30153900384902954\n",
      "Step 2946: val loss: 0.296569287776947\n",
      "Step 2947: train loss: 0.3014300465583801\n",
      "Step 2947: val loss: 0.2964756488800049\n",
      "Step 2948: train loss: 0.30132150650024414\n",
      "Step 2948: val loss: 0.2963821291923523\n",
      "Step 2949: train loss: 0.3012128472328186\n",
      "Step 2949: val loss: 0.2962889075279236\n",
      "Step 2950: train loss: 0.3011046350002289\n",
      "Step 2950: val loss: 0.2961958944797516\n",
      "Step 2951: train loss: 0.30099645256996155\n",
      "Step 2951: val loss: 0.2961030602455139\n",
      "Step 2952: train loss: 0.3008885979652405\n",
      "Step 2952: val loss: 0.296010285615921\n",
      "Step 2953: train loss: 0.30078092217445374\n",
      "Step 2953: val loss: 0.29591771960258484\n",
      "Step 2954: train loss: 0.30067330598831177\n",
      "Step 2954: val loss: 0.29582542181015015\n",
      "Step 2955: train loss: 0.3005659878253937\n",
      "Step 2955: val loss: 0.29573339223861694\n",
      "Step 2956: train loss: 0.3004589378833771\n",
      "Step 2956: val loss: 0.29564139246940613\n",
      "Step 2957: train loss: 0.30035194754600525\n",
      "Step 2957: val loss: 0.2955496609210968\n",
      "Step 2958: train loss: 0.30024516582489014\n",
      "Step 2958: val loss: 0.295458048582077\n",
      "Step 2959: train loss: 0.3001386821269989\n",
      "Step 2959: val loss: 0.29536664485931396\n",
      "Step 2960: train loss: 0.30003225803375244\n",
      "Step 2960: val loss: 0.29527541995048523\n",
      "Step 2961: train loss: 0.2999259829521179\n",
      "Step 2961: val loss: 0.2951844334602356\n",
      "Step 2962: train loss: 0.29982009530067444\n",
      "Step 2962: val loss: 0.29509350657463074\n",
      "Step 2963: train loss: 0.2997142970561981\n",
      "Step 2963: val loss: 0.29500284790992737\n",
      "Step 2964: train loss: 0.2996087074279785\n",
      "Step 2964: val loss: 0.2949123680591583\n",
      "Step 2965: train loss: 0.29950329661369324\n",
      "Step 2965: val loss: 0.29482197761535645\n",
      "Step 2966: train loss: 0.2993980348110199\n",
      "Step 2966: val loss: 0.29473188519477844\n",
      "Step 2967: train loss: 0.29929304122924805\n",
      "Step 2967: val loss: 0.29464203119277954\n",
      "Step 2968: train loss: 0.2991882264614105\n",
      "Step 2968: val loss: 0.294552206993103\n",
      "Step 2969: train loss: 0.29908356070518494\n",
      "Step 2969: val loss: 0.2944625914096832\n",
      "Step 2970: train loss: 0.29897913336753845\n",
      "Step 2970: val loss: 0.29437315464019775\n",
      "Step 2971: train loss: 0.2988748550415039\n",
      "Step 2971: val loss: 0.29428398609161377\n",
      "Step 2972: train loss: 0.2987707853317261\n",
      "Step 2972: val loss: 0.2941949963569641\n",
      "Step 2973: train loss: 0.2986668050289154\n",
      "Step 2973: val loss: 0.29410606622695923\n",
      "Step 2974: train loss: 0.2985631227493286\n",
      "Step 2974: val loss: 0.29401740431785583\n",
      "Step 2975: train loss: 0.29845964908599854\n",
      "Step 2975: val loss: 0.2939288318157196\n",
      "Step 2976: train loss: 0.29835620522499084\n",
      "Step 2976: val loss: 0.29384055733680725\n",
      "Step 2977: train loss: 0.29825323820114136\n",
      "Step 2977: val loss: 0.29375237226486206\n",
      "Step 2978: train loss: 0.2981501519680023\n",
      "Step 2978: val loss: 0.2936643362045288\n",
      "Step 2979: train loss: 0.29804742336273193\n",
      "Step 2979: val loss: 0.29357659816741943\n",
      "Step 2980: train loss: 0.2979447841644287\n",
      "Step 2980: val loss: 0.293489009141922\n",
      "Step 2981: train loss: 0.2978423535823822\n",
      "Step 2981: val loss: 0.2934015393257141\n",
      "Step 2982: train loss: 0.2977401316165924\n",
      "Step 2982: val loss: 0.29331424832344055\n",
      "Step 2983: train loss: 0.29763808846473694\n",
      "Step 2983: val loss: 0.2932271361351013\n",
      "Step 2984: train loss: 0.2975362539291382\n",
      "Step 2984: val loss: 0.2931402325630188\n",
      "Step 2985: train loss: 0.297434538602829\n",
      "Step 2985: val loss: 0.29305344820022583\n",
      "Step 2986: train loss: 0.2973330616950989\n",
      "Step 2986: val loss: 0.2929668724536896\n",
      "Step 2987: train loss: 0.2972317636013031\n",
      "Step 2987: val loss: 0.29288044571876526\n",
      "Step 2988: train loss: 0.29713061451911926\n",
      "Step 2988: val loss: 0.29279422760009766\n",
      "Step 2989: train loss: 0.2970297038555145\n",
      "Step 2989: val loss: 0.29270821809768677\n",
      "Step 2990: train loss: 0.29692888259887695\n",
      "Step 2990: val loss: 0.29262229800224304\n",
      "Step 2991: train loss: 0.29682838916778564\n",
      "Step 2991: val loss: 0.2925364673137665\n",
      "Step 2992: train loss: 0.2967279553413391\n",
      "Step 2992: val loss: 0.2924511134624481\n",
      "Step 2993: train loss: 0.29662778973579407\n",
      "Step 2993: val loss: 0.292365700006485\n",
      "Step 2994: train loss: 0.2965276837348938\n",
      "Step 2994: val loss: 0.29228052496910095\n",
      "Step 2995: train loss: 0.296427845954895\n",
      "Step 2995: val loss: 0.29219546914100647\n",
      "Step 2996: train loss: 0.29632821679115295\n",
      "Step 2996: val loss: 0.2921106219291687\n",
      "Step 2997: train loss: 0.29622864723205566\n",
      "Step 2997: val loss: 0.29202601313591003\n",
      "Step 2998: train loss: 0.2961294651031494\n",
      "Step 2998: val loss: 0.29194143414497375\n",
      "Step 2999: train loss: 0.2960302233695984\n",
      "Step 2999: val loss: 0.29185718297958374\n"
     ]
    }
   ],
   "source": [
    "# Plot the training and validation losses as a function of the gradient descent iterations.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "else \"cpu\")\n",
    "\n",
    "model = nn.Linear(5, 1)\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "\n",
    "loss_validation = []  # create an emply list for the validation loss\n",
    "loss_training = [] # create an empty list for training loss\n",
    "\n",
    "num_steps = 3000\n",
    "for step in range(num_steps):\n",
    "  model.train() \n",
    "  optimizer.zero_grad() \n",
    "  y_ = model(x_train) \n",
    "  loss = loss_fn(y_, y_train) \n",
    "  loss_training.append(loss) # append the loss of the training on each step of the iteration at the end of the list \n",
    "  print(f\"Step {step}: train loss: {loss}\")\n",
    "\n",
    "  loss.backward()  \n",
    "  optimizer.step() \n",
    "  \n",
    "  model.eval() \n",
    "  y_ = model(x_validate)\n",
    "  val_loss = loss_fn(y_, y_validate)\n",
    "  loss_validation.append(val_loss) # append the loss of the validation on each step of the iteration at the end of the list \n",
    "  print(f\"Step {step}: val loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b64716a-885c-44a6-9439-9235b48f65f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "with torch.no_grad():\n",
    "  plt.plot(loss_validation, '.') #plot the validation loss list\n",
    "  ax.set_xlabel(\"Steps\", fontsize=16)\n",
    "  ax.set_ylabel(\"Error\", fontsize=16)\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(loss_training, \".\") #plot the training loss list\n",
    "  ax.set_xlabel(\"Steps\", fontsize=16)\n",
    "  ax.set_ylabel(\"Error\", fontsize=16)\n",
    "\n",
    "# Visualize the polynomial defined by the estimate of w you obtained above, and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f53fb8d-58bb-4df5-af03-4d198c936bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "else \"cpu\")\n",
    "\n",
    "model = nn.Linear(5, 1, bias = False)\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_steps = 3000\n",
    "for step in range(num_steps):\n",
    "  model.train() \n",
    "  optimizer.zero_grad() \n",
    "  y_ = model(x_train) \n",
    "  loss = loss_fn(y_, y_train) \n",
    "  \n",
    "  loss.backward()  \n",
    "  optimizer.step() \n",
    "  \n",
    "  model.eval() \n",
    "  y_ = model(x_validate)\n",
    "  val_loss = loss_fn(y_, y_validate)\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    def f(x):\n",
    "      return 1 * model.weight[0,0].detach().numpy() + x * model.weight[0,1].detach().numpy() + x**2 * model.weight[0,2].detach().numpy() + x**3 * model.weight[0,3].detach().numpy() + x**4 * model.weight[0,4].detach().numpy()\n",
    "\n",
    "    if(step == 2999): # we plot on the last step of the gradient iteration so we can see the final result of the w estimations\n",
    "      x = np.linspace(-3, 3, 500)\n",
    "      plt.plot(x, f(x), color='purple')\n",
    "      plt.show()\n",
    "\n",
    "# Report and explain what happens when the training dataset is reduced to 10 observations while keeping the number of validation data points unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff906ec-2a2f-469f-b7aa-5e67a4daea66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanha\\AppData\\Local\\Temp\\ipykernel_18436\\3730382899.py:33: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y = x.dot(w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight values: Parameter containing:\n",
      "tensor([[ 0.1202, -0.0659, -0.3946,  0.3384,  0.4115]], requires_grad=True)\n",
      "Step 0: train loss: 5.0115838050842285\n",
      "Step 0: val loss: 19.32400131225586\n",
      "Step 1: train loss: 4.200450420379639\n",
      "Step 1: val loss: 19.295101165771484\n",
      "Step 2: train loss: 4.193614482879639\n",
      "Step 2: val loss: 19.272890090942383\n",
      "Step 3: train loss: 4.189078330993652\n",
      "Step 3: val loss: 19.25110626220703\n",
      "Step 4: train loss: 4.184559345245361\n",
      "Step 4: val loss: 19.22983169555664\n",
      "Step 5: train loss: 4.180050849914551\n",
      "Step 5: val loss: 19.20906639099121\n",
      "Step 6: train loss: 4.175552845001221\n",
      "Step 6: val loss: 19.188800811767578\n",
      "Step 7: train loss: 4.171065807342529\n",
      "Step 7: val loss: 19.169034957885742\n",
      "Step 8: train loss: 4.16658878326416\n",
      "Step 8: val loss: 19.149770736694336\n",
      "Step 9: train loss: 4.162121772766113\n",
      "Step 9: val loss: 19.130996704101562\n",
      "Step 10: train loss: 4.157666206359863\n",
      "Step 10: val loss: 19.112714767456055\n",
      "Step 11: train loss: 4.153220176696777\n",
      "Step 11: val loss: 19.09492301940918\n",
      "Step 12: train loss: 4.148785591125488\n",
      "Step 12: val loss: 19.077617645263672\n",
      "Step 13: train loss: 4.144360542297363\n",
      "Step 13: val loss: 19.060791015625\n",
      "Step 14: train loss: 4.1399455070495605\n",
      "Step 14: val loss: 19.044448852539062\n",
      "Step 15: train loss: 4.135540962219238\n",
      "Step 15: val loss: 19.028581619262695\n",
      "Step 16: train loss: 4.13114595413208\n",
      "Step 16: val loss: 19.0131893157959\n",
      "Step 17: train loss: 4.1267619132995605\n",
      "Step 17: val loss: 18.99827003479004\n",
      "Step 18: train loss: 4.1223883628845215\n",
      "Step 18: val loss: 18.98381805419922\n",
      "Step 19: train loss: 4.118023872375488\n",
      "Step 19: val loss: 18.969833374023438\n",
      "Step 20: train loss: 4.113670349121094\n",
      "Step 20: val loss: 18.95631217956543\n",
      "Step 21: train loss: 4.109326362609863\n",
      "Step 21: val loss: 18.943254470825195\n",
      "Step 22: train loss: 4.104992389678955\n",
      "Step 22: val loss: 18.930652618408203\n",
      "Step 23: train loss: 4.100667953491211\n",
      "Step 23: val loss: 18.918506622314453\n",
      "Step 24: train loss: 4.096354007720947\n",
      "Step 24: val loss: 18.90681266784668\n",
      "Step 25: train loss: 4.092049598693848\n",
      "Step 25: val loss: 18.895570755004883\n",
      "Step 26: train loss: 4.08775520324707\n",
      "Step 26: val loss: 18.884775161743164\n",
      "Step 27: train loss: 4.083470344543457\n",
      "Step 27: val loss: 18.874425888061523\n",
      "Step 28: train loss: 4.079195976257324\n",
      "Step 28: val loss: 18.864519119262695\n",
      "Step 29: train loss: 4.074930191040039\n",
      "Step 29: val loss: 18.855051040649414\n",
      "Step 30: train loss: 4.070674419403076\n",
      "Step 30: val loss: 18.846017837524414\n",
      "Step 31: train loss: 4.066429138183594\n",
      "Step 31: val loss: 18.83742332458496\n",
      "Step 32: train loss: 4.062192916870117\n",
      "Step 32: val loss: 18.82925796508789\n",
      "Step 33: train loss: 4.0579657554626465\n",
      "Step 33: val loss: 18.821523666381836\n",
      "Step 34: train loss: 4.053748607635498\n",
      "Step 34: val loss: 18.81421661376953\n",
      "Step 35: train loss: 4.049540996551514\n",
      "Step 35: val loss: 18.807334899902344\n",
      "Step 36: train loss: 4.045342445373535\n",
      "Step 36: val loss: 18.800870895385742\n",
      "Step 37: train loss: 4.041153907775879\n",
      "Step 37: val loss: 18.794828414916992\n",
      "Step 38: train loss: 4.036974906921387\n",
      "Step 38: val loss: 18.78920555114746\n",
      "Step 39: train loss: 4.0328049659729\n",
      "Step 39: val loss: 18.783994674682617\n",
      "Step 40: train loss: 4.028644561767578\n",
      "Step 40: val loss: 18.77919578552246\n",
      "Step 41: train loss: 4.024493217468262\n",
      "Step 41: val loss: 18.774805068969727\n",
      "Step 42: train loss: 4.020350933074951\n",
      "Step 42: val loss: 18.770822525024414\n",
      "Step 43: train loss: 4.016218185424805\n",
      "Step 43: val loss: 18.767244338989258\n",
      "Step 44: train loss: 4.012094497680664\n",
      "Step 44: val loss: 18.764066696166992\n",
      "Step 45: train loss: 4.0079803466796875\n",
      "Step 45: val loss: 18.761293411254883\n",
      "Step 46: train loss: 4.003874778747559\n",
      "Step 46: val loss: 18.758914947509766\n",
      "Step 47: train loss: 3.9997787475585938\n",
      "Step 47: val loss: 18.75693130493164\n",
      "Step 48: train loss: 3.995692491531372\n",
      "Step 48: val loss: 18.755340576171875\n",
      "Step 49: train loss: 3.9916141033172607\n",
      "Step 49: val loss: 18.75413703918457\n",
      "Step 50: train loss: 3.9875450134277344\n",
      "Step 50: val loss: 18.753326416015625\n",
      "Step 51: train loss: 3.983484983444214\n",
      "Step 51: val loss: 18.752897262573242\n",
      "Step 52: train loss: 3.9794342517852783\n",
      "Step 52: val loss: 18.752853393554688\n",
      "Step 53: train loss: 3.9753925800323486\n",
      "Step 53: val loss: 18.75318717956543\n",
      "Step 54: train loss: 3.9713592529296875\n",
      "Step 54: val loss: 18.753904342651367\n",
      "Step 55: train loss: 3.967334747314453\n",
      "Step 55: val loss: 18.754995346069336\n",
      "Step 56: train loss: 3.9633190631866455\n",
      "Step 56: val loss: 18.75646209716797\n",
      "Step 57: train loss: 3.9593124389648438\n",
      "Step 57: val loss: 18.758298873901367\n",
      "Step 58: train loss: 3.9553146362304688\n",
      "Step 58: val loss: 18.76050567626953\n",
      "Step 59: train loss: 3.9513251781463623\n",
      "Step 59: val loss: 18.763078689575195\n",
      "Step 60: train loss: 3.947345018386841\n",
      "Step 60: val loss: 18.766016006469727\n",
      "Step 61: train loss: 3.943373203277588\n",
      "Step 61: val loss: 18.769317626953125\n",
      "Step 62: train loss: 3.9394099712371826\n",
      "Step 62: val loss: 18.772979736328125\n",
      "Step 63: train loss: 3.935455322265625\n",
      "Step 63: val loss: 18.777002334594727\n",
      "Step 64: train loss: 3.931509494781494\n",
      "Step 64: val loss: 18.7813777923584\n",
      "Step 65: train loss: 3.927572250366211\n",
      "Step 65: val loss: 18.786108016967773\n",
      "Step 66: train loss: 3.923643112182617\n",
      "Step 66: val loss: 18.791187286376953\n",
      "Step 67: train loss: 3.919722318649292\n",
      "Step 67: val loss: 18.796619415283203\n",
      "Step 68: train loss: 3.915811061859131\n",
      "Step 68: val loss: 18.802396774291992\n",
      "Step 69: train loss: 3.91190767288208\n",
      "Step 69: val loss: 18.80851936340332\n",
      "Step 70: train loss: 3.9080123901367188\n",
      "Step 70: val loss: 18.814985275268555\n",
      "Step 71: train loss: 3.904125690460205\n",
      "Step 71: val loss: 18.821794509887695\n",
      "Step 72: train loss: 3.900247573852539\n",
      "Step 72: val loss: 18.82893943786621\n",
      "Step 73: train loss: 3.8963780403137207\n",
      "Step 73: val loss: 18.836423873901367\n",
      "Step 74: train loss: 3.8925158977508545\n",
      "Step 74: val loss: 18.844240188598633\n",
      "Step 75: train loss: 3.888662338256836\n",
      "Step 75: val loss: 18.852388381958008\n",
      "Step 76: train loss: 3.884817600250244\n",
      "Step 76: val loss: 18.860868453979492\n",
      "Step 77: train loss: 3.8809802532196045\n",
      "Step 77: val loss: 18.869680404663086\n",
      "Step 78: train loss: 3.8771519660949707\n",
      "Step 78: val loss: 18.87881088256836\n",
      "Step 79: train loss: 3.873331069946289\n",
      "Step 79: val loss: 18.88827133178711\n",
      "Step 80: train loss: 3.8695178031921387\n",
      "Step 80: val loss: 18.898052215576172\n",
      "Step 81: train loss: 3.8657143115997314\n",
      "Step 81: val loss: 18.908151626586914\n",
      "Step 82: train loss: 3.8619179725646973\n",
      "Step 82: val loss: 18.9185733795166\n",
      "Step 83: train loss: 3.8581295013427734\n",
      "Step 83: val loss: 18.929309844970703\n",
      "Step 84: train loss: 3.8543498516082764\n",
      "Step 84: val loss: 18.940359115600586\n",
      "Step 85: train loss: 3.8505771160125732\n",
      "Step 85: val loss: 18.951723098754883\n",
      "Step 86: train loss: 3.8468127250671387\n",
      "Step 86: val loss: 18.963396072387695\n",
      "Step 87: train loss: 3.8430564403533936\n",
      "Step 87: val loss: 18.975374221801758\n",
      "Step 88: train loss: 3.8393077850341797\n",
      "Step 88: val loss: 18.987659454345703\n",
      "Step 89: train loss: 3.8355674743652344\n",
      "Step 89: val loss: 19.00025177001953\n",
      "Step 90: train loss: 3.8318347930908203\n",
      "Step 90: val loss: 19.013145446777344\n",
      "Step 91: train loss: 3.8281092643737793\n",
      "Step 91: val loss: 19.026336669921875\n",
      "Step 92: train loss: 3.824392795562744\n",
      "Step 92: val loss: 19.03982925415039\n",
      "Step 93: train loss: 3.820683240890503\n",
      "Step 93: val loss: 19.053621292114258\n",
      "Step 94: train loss: 3.816981554031372\n",
      "Step 94: val loss: 19.067703247070312\n",
      "Step 95: train loss: 3.8132877349853516\n",
      "Step 95: val loss: 19.08207893371582\n",
      "Step 96: train loss: 3.8096015453338623\n",
      "Step 96: val loss: 19.096744537353516\n",
      "Step 97: train loss: 3.8059234619140625\n",
      "Step 97: val loss: 19.11170196533203\n",
      "Step 98: train loss: 3.8022525310516357\n",
      "Step 98: val loss: 19.126943588256836\n",
      "Step 99: train loss: 3.798588991165161\n",
      "Step 99: val loss: 19.142473220825195\n",
      "Step 100: train loss: 3.794933319091797\n",
      "Step 100: val loss: 19.15828514099121\n",
      "Step 101: train loss: 3.791285753250122\n",
      "Step 101: val loss: 19.17437744140625\n",
      "Step 102: train loss: 3.7876453399658203\n",
      "Step 102: val loss: 19.19074821472168\n",
      "Step 103: train loss: 3.7840118408203125\n",
      "Step 103: val loss: 19.207401275634766\n",
      "Step 104: train loss: 3.7803866863250732\n",
      "Step 104: val loss: 19.224328994750977\n",
      "Step 105: train loss: 3.776768445968628\n",
      "Step 105: val loss: 19.241527557373047\n",
      "Step 106: train loss: 3.773157835006714\n",
      "Step 106: val loss: 19.25900650024414\n",
      "Step 107: train loss: 3.769554853439331\n",
      "Step 107: val loss: 19.27674674987793\n",
      "Step 108: train loss: 3.7659592628479004\n",
      "Step 108: val loss: 19.294761657714844\n",
      "Step 109: train loss: 3.7623703479766846\n",
      "Step 109: val loss: 19.31304168701172\n",
      "Step 110: train loss: 3.758789539337158\n",
      "Step 110: val loss: 19.331584930419922\n",
      "Step 111: train loss: 3.7552154064178467\n",
      "Step 111: val loss: 19.35039710998535\n",
      "Step 112: train loss: 3.7516491413116455\n",
      "Step 112: val loss: 19.369468688964844\n",
      "Step 113: train loss: 3.7480900287628174\n",
      "Step 113: val loss: 19.38880157470703\n",
      "Step 114: train loss: 3.7445380687713623\n",
      "Step 114: val loss: 19.40839195251465\n",
      "Step 115: train loss: 3.7409934997558594\n",
      "Step 115: val loss: 19.42823600769043\n",
      "Step 116: train loss: 3.7374560832977295\n",
      "Step 116: val loss: 19.44833755493164\n",
      "Step 117: train loss: 3.7339248657226562\n",
      "Step 117: val loss: 19.46869468688965\n",
      "Step 118: train loss: 3.7304024696350098\n",
      "Step 118: val loss: 19.489299774169922\n",
      "Step 119: train loss: 3.726886034011841\n",
      "Step 119: val loss: 19.510156631469727\n",
      "Step 120: train loss: 3.723377227783203\n",
      "Step 120: val loss: 19.531261444091797\n",
      "Step 121: train loss: 3.7198758125305176\n",
      "Step 121: val loss: 19.552610397338867\n",
      "Step 122: train loss: 3.7163805961608887\n",
      "Step 122: val loss: 19.57421112060547\n",
      "Step 123: train loss: 3.712893009185791\n",
      "Step 123: val loss: 19.596046447753906\n",
      "Step 124: train loss: 3.709412097930908\n",
      "Step 124: val loss: 19.61812973022461\n",
      "Step 125: train loss: 3.7059378623962402\n",
      "Step 125: val loss: 19.64044761657715\n",
      "Step 126: train loss: 3.7024714946746826\n",
      "Step 126: val loss: 19.663005828857422\n",
      "Step 127: train loss: 3.6990115642547607\n",
      "Step 127: val loss: 19.68580436706543\n",
      "Step 128: train loss: 3.695559024810791\n",
      "Step 128: val loss: 19.708833694458008\n",
      "Step 129: train loss: 3.6921119689941406\n",
      "Step 129: val loss: 19.732101440429688\n",
      "Step 130: train loss: 3.688673496246338\n",
      "Step 130: val loss: 19.755596160888672\n",
      "Step 131: train loss: 3.6852409839630127\n",
      "Step 131: val loss: 19.77931785583496\n",
      "Step 132: train loss: 3.6818156242370605\n",
      "Step 132: val loss: 19.803274154663086\n",
      "Step 133: train loss: 3.6783969402313232\n",
      "Step 133: val loss: 19.827457427978516\n",
      "Step 134: train loss: 3.6749846935272217\n",
      "Step 134: val loss: 19.851869583129883\n",
      "Step 135: train loss: 3.6715800762176514\n",
      "Step 135: val loss: 19.876497268676758\n",
      "Step 136: train loss: 3.6681816577911377\n",
      "Step 136: val loss: 19.901348114013672\n",
      "Step 137: train loss: 3.6647896766662598\n",
      "Step 137: val loss: 19.926424026489258\n",
      "Step 138: train loss: 3.6614043712615967\n",
      "Step 138: val loss: 19.951719284057617\n",
      "Step 139: train loss: 3.6580264568328857\n",
      "Step 139: val loss: 19.977231979370117\n",
      "Step 140: train loss: 3.6546549797058105\n",
      "Step 140: val loss: 20.002960205078125\n",
      "Step 141: train loss: 3.651289463043213\n",
      "Step 141: val loss: 20.028905868530273\n",
      "Step 142: train loss: 3.6479313373565674\n",
      "Step 142: val loss: 20.0550594329834\n",
      "Step 143: train loss: 3.6445794105529785\n",
      "Step 143: val loss: 20.08142852783203\n",
      "Step 144: train loss: 3.6412339210510254\n",
      "Step 144: val loss: 20.108003616333008\n",
      "Step 145: train loss: 3.6378955841064453\n",
      "Step 145: val loss: 20.134788513183594\n",
      "Step 146: train loss: 3.634563446044922\n",
      "Step 146: val loss: 20.161788940429688\n",
      "Step 147: train loss: 3.631237030029297\n",
      "Step 147: val loss: 20.188983917236328\n",
      "Step 148: train loss: 3.627918243408203\n",
      "Step 148: val loss: 20.21639060974121\n",
      "Step 149: train loss: 3.624605655670166\n",
      "Step 149: val loss: 20.243993759155273\n",
      "Step 150: train loss: 3.6212992668151855\n",
      "Step 150: val loss: 20.271804809570312\n",
      "Step 151: train loss: 3.618000030517578\n",
      "Step 151: val loss: 20.29981231689453\n",
      "Step 152: train loss: 3.614706516265869\n",
      "Step 152: val loss: 20.328020095825195\n",
      "Step 153: train loss: 3.6114189624786377\n",
      "Step 153: val loss: 20.356422424316406\n",
      "Step 154: train loss: 3.6081383228302\n",
      "Step 154: val loss: 20.38502311706543\n",
      "Step 155: train loss: 3.6048645973205566\n",
      "Step 155: val loss: 20.413816452026367\n",
      "Step 156: train loss: 3.6015963554382324\n",
      "Step 156: val loss: 20.44280242919922\n",
      "Step 157: train loss: 3.598334550857544\n",
      "Step 157: val loss: 20.471982955932617\n",
      "Step 158: train loss: 3.5950794219970703\n",
      "Step 158: val loss: 20.50135040283203\n",
      "Step 159: train loss: 3.591829776763916\n",
      "Step 159: val loss: 20.53090476989746\n",
      "Step 160: train loss: 3.5885868072509766\n",
      "Step 160: val loss: 20.560649871826172\n",
      "Step 161: train loss: 3.585350513458252\n",
      "Step 161: val loss: 20.590578079223633\n",
      "Step 162: train loss: 3.5821194648742676\n",
      "Step 162: val loss: 20.620695114135742\n",
      "Step 163: train loss: 3.578895092010498\n",
      "Step 163: val loss: 20.6509952545166\n",
      "Step 164: train loss: 3.5756771564483643\n",
      "Step 164: val loss: 20.681474685668945\n",
      "Step 165: train loss: 3.57246470451355\n",
      "Step 165: val loss: 20.71213150024414\n",
      "Step 166: train loss: 3.5692591667175293\n",
      "Step 166: val loss: 20.74297523498535\n",
      "Step 167: train loss: 3.5660598278045654\n",
      "Step 167: val loss: 20.773988723754883\n",
      "Step 168: train loss: 3.562865734100342\n",
      "Step 168: val loss: 20.80518341064453\n",
      "Step 169: train loss: 3.559678554534912\n",
      "Step 169: val loss: 20.8365535736084\n",
      "Step 170: train loss: 3.5564968585968018\n",
      "Step 170: val loss: 20.868093490600586\n",
      "Step 171: train loss: 3.553321123123169\n",
      "Step 171: val loss: 20.89980697631836\n",
      "Step 172: train loss: 3.5501513481140137\n",
      "Step 172: val loss: 20.931697845458984\n",
      "Step 173: train loss: 3.546988010406494\n",
      "Step 173: val loss: 20.9637508392334\n",
      "Step 174: train loss: 3.543830156326294\n",
      "Step 174: val loss: 20.9959774017334\n",
      "Step 175: train loss: 3.5406787395477295\n",
      "Step 175: val loss: 21.02836799621582\n",
      "Step 176: train loss: 3.5375335216522217\n",
      "Step 176: val loss: 21.06092643737793\n",
      "Step 177: train loss: 3.534393310546875\n",
      "Step 177: val loss: 21.093650817871094\n",
      "Step 178: train loss: 3.5312600135803223\n",
      "Step 178: val loss: 21.126537322998047\n",
      "Step 179: train loss: 3.5281319618225098\n",
      "Step 179: val loss: 21.15958595275879\n",
      "Step 180: train loss: 3.5250096321105957\n",
      "Step 180: val loss: 21.192792892456055\n",
      "Step 181: train loss: 3.52189302444458\n",
      "Step 181: val loss: 21.226163864135742\n",
      "Step 182: train loss: 3.518782377243042\n",
      "Step 182: val loss: 21.259693145751953\n",
      "Step 183: train loss: 3.5156776905059814\n",
      "Step 183: val loss: 21.293378829956055\n",
      "Step 184: train loss: 3.5125789642333984\n",
      "Step 184: val loss: 21.32721519470215\n",
      "Step 185: train loss: 3.5094857215881348\n",
      "Step 185: val loss: 21.361217498779297\n",
      "Step 186: train loss: 3.5063986778259277\n",
      "Step 186: val loss: 21.395362854003906\n",
      "Step 187: train loss: 3.503316879272461\n",
      "Step 187: val loss: 21.429668426513672\n",
      "Step 188: train loss: 3.50024151802063\n",
      "Step 188: val loss: 21.46411895751953\n",
      "Step 189: train loss: 3.497171401977539\n",
      "Step 189: val loss: 21.49872398376465\n",
      "Step 190: train loss: 3.4941067695617676\n",
      "Step 190: val loss: 21.533475875854492\n",
      "Step 191: train loss: 3.4910476207733154\n",
      "Step 191: val loss: 21.568376541137695\n",
      "Step 192: train loss: 3.487995147705078\n",
      "Step 192: val loss: 21.60342788696289\n",
      "Step 193: train loss: 3.4849467277526855\n",
      "Step 193: val loss: 21.63861656188965\n",
      "Step 194: train loss: 3.481904983520508\n",
      "Step 194: val loss: 21.673954010009766\n",
      "Step 195: train loss: 3.4788691997528076\n",
      "Step 195: val loss: 21.70943832397461\n",
      "Step 196: train loss: 3.4758377075195312\n",
      "Step 196: val loss: 21.745052337646484\n",
      "Step 197: train loss: 3.472813129425049\n",
      "Step 197: val loss: 21.78081512451172\n",
      "Step 198: train loss: 3.4697933197021484\n",
      "Step 198: val loss: 21.816713333129883\n",
      "Step 199: train loss: 3.4667789936065674\n",
      "Step 199: val loss: 21.85275650024414\n",
      "Step 200: train loss: 3.4637703895568848\n",
      "Step 200: val loss: 21.888931274414062\n",
      "Step 201: train loss: 3.4607677459716797\n",
      "Step 201: val loss: 21.92524528503418\n",
      "Step 202: train loss: 3.4577701091766357\n",
      "Step 202: val loss: 21.961688995361328\n",
      "Step 203: train loss: 3.454777956008911\n",
      "Step 203: val loss: 21.998273849487305\n",
      "Step 204: train loss: 3.4517910480499268\n",
      "Step 204: val loss: 22.03498649597168\n",
      "Step 205: train loss: 3.4488093852996826\n",
      "Step 205: val loss: 22.07183265686035\n",
      "Step 206: train loss: 3.445833683013916\n",
      "Step 206: val loss: 22.108808517456055\n",
      "Step 207: train loss: 3.4428634643554688\n",
      "Step 207: val loss: 22.145910263061523\n",
      "Step 208: train loss: 3.4398982524871826\n",
      "Step 208: val loss: 22.183149337768555\n",
      "Step 209: train loss: 3.436938524246216\n",
      "Step 209: val loss: 22.220508575439453\n",
      "Step 210: train loss: 3.4339840412139893\n",
      "Step 210: val loss: 22.257993698120117\n",
      "Step 211: train loss: 3.431035280227661\n",
      "Step 211: val loss: 22.295608520507812\n",
      "Step 212: train loss: 3.428091526031494\n",
      "Step 212: val loss: 22.333341598510742\n",
      "Step 213: train loss: 3.4251537322998047\n",
      "Step 213: val loss: 22.371206283569336\n",
      "Step 214: train loss: 3.422220230102539\n",
      "Step 214: val loss: 22.40918731689453\n",
      "Step 215: train loss: 3.419292449951172\n",
      "Step 215: val loss: 22.44728660583496\n",
      "Step 216: train loss: 3.416369676589966\n",
      "Step 216: val loss: 22.485509872436523\n",
      "Step 217: train loss: 3.413452625274658\n",
      "Step 217: val loss: 22.523847579956055\n",
      "Step 218: train loss: 3.4105401039123535\n",
      "Step 218: val loss: 22.56230926513672\n",
      "Step 219: train loss: 3.4076333045959473\n",
      "Step 219: val loss: 22.600887298583984\n",
      "Step 220: train loss: 3.404731035232544\n",
      "Step 220: val loss: 22.639577865600586\n",
      "Step 221: train loss: 3.4018349647521973\n",
      "Step 221: val loss: 22.678380966186523\n",
      "Step 222: train loss: 3.3989434242248535\n",
      "Step 222: val loss: 22.717302322387695\n",
      "Step 223: train loss: 3.39605712890625\n",
      "Step 223: val loss: 22.756332397460938\n",
      "Step 224: train loss: 3.3931758403778076\n",
      "Step 224: val loss: 22.79547882080078\n",
      "Step 225: train loss: 3.3903000354766846\n",
      "Step 225: val loss: 22.83473014831543\n",
      "Step 226: train loss: 3.3874289989471436\n",
      "Step 226: val loss: 22.874094009399414\n",
      "Step 227: train loss: 3.3845629692077637\n",
      "Step 227: val loss: 22.9135684967041\n",
      "Step 228: train loss: 3.381701946258545\n",
      "Step 228: val loss: 22.95314598083496\n",
      "Step 229: train loss: 3.378847122192383\n",
      "Step 229: val loss: 22.992834091186523\n",
      "Step 230: train loss: 3.3759961128234863\n",
      "Step 230: val loss: 23.03262710571289\n",
      "Step 231: train loss: 3.373150587081909\n",
      "Step 231: val loss: 23.07252311706543\n",
      "Step 232: train loss: 3.370309829711914\n",
      "Step 232: val loss: 23.112525939941406\n",
      "Step 233: train loss: 3.36747407913208\n",
      "Step 233: val loss: 23.152626037597656\n",
      "Step 234: train loss: 3.3646435737609863\n",
      "Step 234: val loss: 23.19283676147461\n",
      "Step 235: train loss: 3.3618175983428955\n",
      "Step 235: val loss: 23.233137130737305\n",
      "Step 236: train loss: 3.358996868133545\n",
      "Step 236: val loss: 23.273548126220703\n",
      "Step 237: train loss: 3.3561809062957764\n",
      "Step 237: val loss: 23.314054489135742\n",
      "Step 238: train loss: 3.353369951248169\n",
      "Step 238: val loss: 23.354652404785156\n",
      "Step 239: train loss: 3.3505642414093018\n",
      "Step 239: val loss: 23.395355224609375\n",
      "Step 240: train loss: 3.3477625846862793\n",
      "Step 240: val loss: 23.4361515045166\n",
      "Step 241: train loss: 3.344966411590576\n",
      "Step 241: val loss: 23.4770450592041\n",
      "Step 242: train loss: 3.3421757221221924\n",
      "Step 242: val loss: 23.51803207397461\n",
      "Step 243: train loss: 3.339388370513916\n",
      "Step 243: val loss: 23.55910873413086\n",
      "Step 244: train loss: 3.336606979370117\n",
      "Step 244: val loss: 23.60028076171875\n",
      "Step 245: train loss: 3.3338303565979004\n",
      "Step 245: val loss: 23.641544342041016\n",
      "Step 246: train loss: 3.3310580253601074\n",
      "Step 246: val loss: 23.682903289794922\n",
      "Step 247: train loss: 3.328291416168213\n",
      "Step 247: val loss: 23.724340438842773\n",
      "Step 248: train loss: 3.325528621673584\n",
      "Step 248: val loss: 23.765880584716797\n",
      "Step 249: train loss: 3.3227710723876953\n",
      "Step 249: val loss: 23.807493209838867\n",
      "Step 250: train loss: 3.3200180530548096\n",
      "Step 250: val loss: 23.849206924438477\n",
      "Step 251: train loss: 3.317269802093506\n",
      "Step 251: val loss: 23.891002655029297\n",
      "Step 252: train loss: 3.314526319503784\n",
      "Step 252: val loss: 23.932880401611328\n",
      "Step 253: train loss: 3.3117871284484863\n",
      "Step 253: val loss: 23.9748477935791\n",
      "Step 254: train loss: 3.3090529441833496\n",
      "Step 254: val loss: 24.016895294189453\n",
      "Step 255: train loss: 3.306324005126953\n",
      "Step 255: val loss: 24.059024810791016\n",
      "Step 256: train loss: 3.3035995960235596\n",
      "Step 256: val loss: 24.101234436035156\n",
      "Step 257: train loss: 3.300879716873169\n",
      "Step 257: val loss: 24.143529891967773\n",
      "Step 258: train loss: 3.2981631755828857\n",
      "Step 258: val loss: 24.18590545654297\n",
      "Step 259: train loss: 3.295452833175659\n",
      "Step 259: val loss: 24.228357315063477\n",
      "Step 260: train loss: 3.2927470207214355\n",
      "Step 260: val loss: 24.27089500427246\n",
      "Step 261: train loss: 3.2900452613830566\n",
      "Step 261: val loss: 24.313499450683594\n",
      "Step 262: train loss: 3.2873477935791016\n",
      "Step 262: val loss: 24.356191635131836\n",
      "Step 263: train loss: 3.284655809402466\n",
      "Step 263: val loss: 24.39895248413086\n",
      "Step 264: train loss: 3.2819676399230957\n",
      "Step 264: val loss: 24.44179344177246\n",
      "Step 265: train loss: 3.2792842388153076\n",
      "Step 265: val loss: 24.484704971313477\n",
      "Step 266: train loss: 3.2766056060791016\n",
      "Step 266: val loss: 24.527692794799805\n",
      "Step 267: train loss: 3.2739319801330566\n",
      "Step 267: val loss: 24.570749282836914\n",
      "Step 268: train loss: 3.2712619304656982\n",
      "Step 268: val loss: 24.613882064819336\n",
      "Step 269: train loss: 3.26859712600708\n",
      "Step 269: val loss: 24.657087326049805\n",
      "Step 270: train loss: 3.2659363746643066\n",
      "Step 270: val loss: 24.700353622436523\n",
      "Step 271: train loss: 3.263279438018799\n",
      "Step 271: val loss: 24.743703842163086\n",
      "Step 272: train loss: 3.260627269744873\n",
      "Step 272: val loss: 24.787111282348633\n",
      "Step 273: train loss: 3.2579808235168457\n",
      "Step 273: val loss: 24.830591201782227\n",
      "Step 274: train loss: 3.2553372383117676\n",
      "Step 274: val loss: 24.874135971069336\n",
      "Step 275: train loss: 3.2526988983154297\n",
      "Step 275: val loss: 24.917753219604492\n",
      "Step 276: train loss: 3.2500648498535156\n",
      "Step 276: val loss: 24.9614315032959\n",
      "Step 277: train loss: 3.2474350929260254\n",
      "Step 277: val loss: 25.005176544189453\n",
      "Step 278: train loss: 3.244810104370117\n",
      "Step 278: val loss: 25.048986434936523\n",
      "Step 279: train loss: 3.2421889305114746\n",
      "Step 279: val loss: 25.092857360839844\n",
      "Step 280: train loss: 3.239572525024414\n",
      "Step 280: val loss: 25.13679313659668\n",
      "Step 281: train loss: 3.2369606494903564\n",
      "Step 281: val loss: 25.180788040161133\n",
      "Step 282: train loss: 3.2343525886535645\n",
      "Step 282: val loss: 25.224842071533203\n",
      "Step 283: train loss: 3.231748580932617\n",
      "Step 283: val loss: 25.268964767456055\n",
      "Step 284: train loss: 3.229149580001831\n",
      "Step 284: val loss: 25.31313705444336\n",
      "Step 285: train loss: 3.2265548706054688\n",
      "Step 285: val loss: 25.35737419128418\n",
      "Step 286: train loss: 3.223963499069214\n",
      "Step 286: val loss: 25.40167236328125\n",
      "Step 287: train loss: 3.2213778495788574\n",
      "Step 287: val loss: 25.44601821899414\n",
      "Step 288: train loss: 3.2187952995300293\n",
      "Step 288: val loss: 25.490428924560547\n",
      "Step 289: train loss: 3.2162177562713623\n",
      "Step 289: val loss: 25.534893035888672\n",
      "Step 290: train loss: 3.213644504547119\n",
      "Step 290: val loss: 25.57941436767578\n",
      "Step 291: train loss: 3.2110748291015625\n",
      "Step 291: val loss: 25.623994827270508\n",
      "Step 292: train loss: 3.2085094451904297\n",
      "Step 292: val loss: 25.668617248535156\n",
      "Step 293: train loss: 3.2059483528137207\n",
      "Step 293: val loss: 25.713298797607422\n",
      "Step 294: train loss: 3.2033915519714355\n",
      "Step 294: val loss: 25.758041381835938\n",
      "Step 295: train loss: 3.2008392810821533\n",
      "Step 295: val loss: 25.802820205688477\n",
      "Step 296: train loss: 3.198291301727295\n",
      "Step 296: val loss: 25.847658157348633\n",
      "Step 297: train loss: 3.195746421813965\n",
      "Step 297: val loss: 25.892549514770508\n",
      "Step 298: train loss: 3.193206548690796\n",
      "Step 298: val loss: 25.937484741210938\n",
      "Step 299: train loss: 3.1906704902648926\n",
      "Step 299: val loss: 25.982473373413086\n",
      "Step 300: train loss: 3.188138961791992\n",
      "Step 300: val loss: 26.027507781982422\n",
      "Step 301: train loss: 3.1856112480163574\n",
      "Step 301: val loss: 26.07259178161621\n",
      "Step 302: train loss: 3.1830873489379883\n",
      "Step 302: val loss: 26.117721557617188\n",
      "Step 303: train loss: 3.180567741394043\n",
      "Step 303: val loss: 26.16289520263672\n",
      "Step 304: train loss: 3.1780524253845215\n",
      "Step 304: val loss: 26.20811653137207\n",
      "Step 305: train loss: 3.175541639328003\n",
      "Step 305: val loss: 26.253381729125977\n",
      "Step 306: train loss: 3.1730339527130127\n",
      "Step 306: val loss: 26.29869270324707\n",
      "Step 307: train loss: 3.1705307960510254\n",
      "Step 307: val loss: 26.34404945373535\n",
      "Step 308: train loss: 3.1680312156677246\n",
      "Step 308: val loss: 26.389442443847656\n",
      "Step 309: train loss: 3.1655359268188477\n",
      "Step 309: val loss: 26.434886932373047\n",
      "Step 310: train loss: 3.1630454063415527\n",
      "Step 310: val loss: 26.48036766052246\n",
      "Step 311: train loss: 3.160557746887207\n",
      "Step 311: val loss: 26.5258846282959\n",
      "Step 312: train loss: 3.1580746173858643\n",
      "Step 312: val loss: 26.571449279785156\n",
      "Step 313: train loss: 3.155595302581787\n",
      "Step 313: val loss: 26.61705207824707\n",
      "Step 314: train loss: 3.153120517730713\n",
      "Step 314: val loss: 26.662689208984375\n",
      "Step 315: train loss: 3.150648593902588\n",
      "Step 315: val loss: 26.708375930786133\n",
      "Step 316: train loss: 3.148181438446045\n",
      "Step 316: val loss: 26.754087448120117\n",
      "Step 317: train loss: 3.1457180976867676\n",
      "Step 317: val loss: 26.799842834472656\n",
      "Step 318: train loss: 3.143259048461914\n",
      "Step 318: val loss: 26.84563636779785\n",
      "Step 319: train loss: 3.140803337097168\n",
      "Step 319: val loss: 26.891462326049805\n",
      "Step 320: train loss: 3.1383516788482666\n",
      "Step 320: val loss: 26.937326431274414\n",
      "Step 321: train loss: 3.135903835296631\n",
      "Step 321: val loss: 26.98322868347168\n",
      "Step 322: train loss: 3.1334598064422607\n",
      "Step 322: val loss: 27.02915382385254\n",
      "Step 323: train loss: 3.1310203075408936\n",
      "Step 323: val loss: 27.07512092590332\n",
      "Step 324: train loss: 3.1285839080810547\n",
      "Step 324: val loss: 27.121118545532227\n",
      "Step 325: train loss: 3.1261520385742188\n",
      "Step 325: val loss: 27.167152404785156\n",
      "Step 326: train loss: 3.1237235069274902\n",
      "Step 326: val loss: 27.213214874267578\n",
      "Step 327: train loss: 3.1212990283966064\n",
      "Step 327: val loss: 27.259302139282227\n",
      "Step 328: train loss: 3.11887788772583\n",
      "Step 328: val loss: 27.30542755126953\n",
      "Step 329: train loss: 3.1164615154266357\n",
      "Step 329: val loss: 27.351585388183594\n",
      "Step 330: train loss: 3.1140480041503906\n",
      "Step 330: val loss: 27.397768020629883\n",
      "Step 331: train loss: 3.1116385459899902\n",
      "Step 331: val loss: 27.443981170654297\n",
      "Step 332: train loss: 3.1092329025268555\n",
      "Step 332: val loss: 27.490222930908203\n",
      "Step 333: train loss: 3.1068310737609863\n",
      "Step 333: val loss: 27.53649139404297\n",
      "Step 334: train loss: 3.104433298110962\n",
      "Step 334: val loss: 27.582782745361328\n",
      "Step 335: train loss: 3.102039098739624\n",
      "Step 335: val loss: 27.629112243652344\n",
      "Step 336: train loss: 3.0996487140655518\n",
      "Step 336: val loss: 27.67545509338379\n",
      "Step 337: train loss: 3.097261905670166\n",
      "Step 337: val loss: 27.721837997436523\n",
      "Step 338: train loss: 3.0948784351348877\n",
      "Step 338: val loss: 27.768232345581055\n",
      "Step 339: train loss: 3.092499017715454\n",
      "Step 339: val loss: 27.814661026000977\n",
      "Step 340: train loss: 3.0901236534118652\n",
      "Step 340: val loss: 27.861095428466797\n",
      "Step 341: train loss: 3.087751626968384\n",
      "Step 341: val loss: 27.907569885253906\n",
      "Step 342: train loss: 3.085383176803589\n",
      "Step 342: val loss: 27.954063415527344\n",
      "Step 343: train loss: 3.0830185413360596\n",
      "Step 343: val loss: 28.000581741333008\n",
      "Step 344: train loss: 3.080658435821533\n",
      "Step 344: val loss: 28.0471134185791\n",
      "Step 345: train loss: 3.078300714492798\n",
      "Step 345: val loss: 28.093669891357422\n",
      "Step 346: train loss: 3.075946807861328\n",
      "Step 346: val loss: 28.140249252319336\n",
      "Step 347: train loss: 3.073596954345703\n",
      "Step 347: val loss: 28.186845779418945\n",
      "Step 348: train loss: 3.0712506771087646\n",
      "Step 348: val loss: 28.233463287353516\n",
      "Step 349: train loss: 3.0689074993133545\n",
      "Step 349: val loss: 28.280099868774414\n",
      "Step 350: train loss: 3.066568613052368\n",
      "Step 350: val loss: 28.326751708984375\n",
      "Step 351: train loss: 3.0642333030700684\n",
      "Step 351: val loss: 28.373422622680664\n",
      "Step 352: train loss: 3.0619008541107178\n",
      "Step 352: val loss: 28.420120239257812\n",
      "Step 353: train loss: 3.05957293510437\n",
      "Step 353: val loss: 28.466815948486328\n",
      "Step 354: train loss: 3.057248115539551\n",
      "Step 354: val loss: 28.513545989990234\n",
      "Step 355: train loss: 3.054926872253418\n",
      "Step 355: val loss: 28.56028175354004\n",
      "Step 356: train loss: 3.052609443664551\n",
      "Step 356: val loss: 28.60703468322754\n",
      "Step 357: train loss: 3.0502946376800537\n",
      "Step 357: val loss: 28.653810501098633\n",
      "Step 358: train loss: 3.0479841232299805\n",
      "Step 358: val loss: 28.700592041015625\n",
      "Step 359: train loss: 3.0456767082214355\n",
      "Step 359: val loss: 28.74738311767578\n",
      "Step 360: train loss: 3.0433735847473145\n",
      "Step 360: val loss: 28.794193267822266\n",
      "Step 361: train loss: 3.0410733222961426\n",
      "Step 361: val loss: 28.841014862060547\n",
      "Step 362: train loss: 3.0387768745422363\n",
      "Step 362: val loss: 28.88785743713379\n",
      "Step 363: train loss: 3.0364842414855957\n",
      "Step 363: val loss: 28.9346981048584\n",
      "Step 364: train loss: 3.0341944694519043\n",
      "Step 364: val loss: 28.98155975341797\n",
      "Step 365: train loss: 3.0319082736968994\n",
      "Step 365: val loss: 29.028432846069336\n",
      "Step 366: train loss: 3.029625415802002\n",
      "Step 366: val loss: 29.07530403137207\n",
      "Step 367: train loss: 3.02734637260437\n",
      "Step 367: val loss: 29.122196197509766\n",
      "Step 368: train loss: 3.0250706672668457\n",
      "Step 368: val loss: 29.169092178344727\n",
      "Step 369: train loss: 3.0227980613708496\n",
      "Step 369: val loss: 29.216007232666016\n",
      "Step 370: train loss: 3.02052903175354\n",
      "Step 370: val loss: 29.262922286987305\n",
      "Step 371: train loss: 3.018263339996338\n",
      "Step 371: val loss: 29.309844970703125\n",
      "Step 372: train loss: 3.0160019397735596\n",
      "Step 372: val loss: 29.35677146911621\n",
      "Step 373: train loss: 3.0137429237365723\n",
      "Step 373: val loss: 29.403717041015625\n",
      "Step 374: train loss: 3.0114877223968506\n",
      "Step 374: val loss: 29.450658798217773\n",
      "Step 375: train loss: 3.0092356204986572\n",
      "Step 375: val loss: 29.49761199951172\n",
      "Step 376: train loss: 3.0069870948791504\n",
      "Step 376: val loss: 29.544570922851562\n",
      "Step 377: train loss: 3.00474214553833\n",
      "Step 377: val loss: 29.59153175354004\n",
      "Step 378: train loss: 3.002500057220459\n",
      "Step 378: val loss: 29.638504028320312\n",
      "Step 379: train loss: 3.0002615451812744\n",
      "Step 379: val loss: 29.685468673706055\n",
      "Step 380: train loss: 2.9980263710021973\n",
      "Step 380: val loss: 29.73244857788086\n",
      "Step 381: train loss: 2.9957945346832275\n",
      "Step 381: val loss: 29.779428482055664\n",
      "Step 382: train loss: 2.9935660362243652\n",
      "Step 382: val loss: 29.82640838623047\n",
      "Step 383: train loss: 2.991340398788452\n",
      "Step 383: val loss: 29.873395919799805\n",
      "Step 384: train loss: 2.989118814468384\n",
      "Step 384: val loss: 29.920385360717773\n",
      "Step 385: train loss: 2.9869003295898438\n",
      "Step 385: val loss: 29.967363357543945\n",
      "Step 386: train loss: 2.984684467315674\n",
      "Step 386: val loss: 30.014358520507812\n",
      "Step 387: train loss: 2.9824728965759277\n",
      "Step 387: val loss: 30.061349868774414\n",
      "Step 388: train loss: 2.980264186859131\n",
      "Step 388: val loss: 30.108339309692383\n",
      "Step 389: train loss: 2.978058338165283\n",
      "Step 389: val loss: 30.155338287353516\n",
      "Step 390: train loss: 2.975856304168701\n",
      "Step 390: val loss: 30.202320098876953\n",
      "Step 391: train loss: 2.9736571311950684\n",
      "Step 391: val loss: 30.249317169189453\n",
      "Step 392: train loss: 2.971461772918701\n",
      "Step 392: val loss: 30.296300888061523\n",
      "Step 393: train loss: 2.969269037246704\n",
      "Step 393: val loss: 30.343294143676758\n",
      "Step 394: train loss: 2.9670798778533936\n",
      "Step 394: val loss: 30.390274047851562\n",
      "Step 395: train loss: 2.9648938179016113\n",
      "Step 395: val loss: 30.4372615814209\n",
      "Step 396: train loss: 2.962710380554199\n",
      "Step 396: val loss: 30.484241485595703\n",
      "Step 397: train loss: 2.96053147315979\n",
      "Step 397: val loss: 30.53121566772461\n",
      "Step 398: train loss: 2.958354949951172\n",
      "Step 398: val loss: 30.57819175720215\n",
      "Step 399: train loss: 2.956181764602661\n",
      "Step 399: val loss: 30.625158309936523\n",
      "Step 400: train loss: 2.9540114402770996\n",
      "Step 400: val loss: 30.6721248626709\n",
      "Step 401: train loss: 2.9518444538116455\n",
      "Step 401: val loss: 30.719079971313477\n",
      "Step 402: train loss: 2.949680805206299\n",
      "Step 402: val loss: 30.766027450561523\n",
      "Step 403: train loss: 2.9475197792053223\n",
      "Step 403: val loss: 30.8129825592041\n",
      "Step 404: train loss: 2.9453625679016113\n",
      "Step 404: val loss: 30.85992431640625\n",
      "Step 405: train loss: 2.9432082176208496\n",
      "Step 405: val loss: 30.906862258911133\n",
      "Step 406: train loss: 2.941056728363037\n",
      "Step 406: val loss: 30.953794479370117\n",
      "Step 407: train loss: 2.938908815383911\n",
      "Step 407: val loss: 31.000709533691406\n",
      "Step 408: train loss: 2.9367635250091553\n",
      "Step 408: val loss: 31.04762840270996\n",
      "Step 409: train loss: 2.934621572494507\n",
      "Step 409: val loss: 31.094532012939453\n",
      "Step 410: train loss: 2.9324822425842285\n",
      "Step 410: val loss: 31.14143180847168\n",
      "Step 411: train loss: 2.930346965789795\n",
      "Step 411: val loss: 31.188322067260742\n",
      "Step 412: train loss: 2.9282145500183105\n",
      "Step 412: val loss: 31.235200881958008\n",
      "Step 413: train loss: 2.9260849952697754\n",
      "Step 413: val loss: 31.282066345214844\n",
      "Step 414: train loss: 2.9239583015441895\n",
      "Step 414: val loss: 31.328927993774414\n",
      "Step 415: train loss: 2.9218344688415527\n",
      "Step 415: val loss: 31.375782012939453\n",
      "Step 416: train loss: 2.9197144508361816\n",
      "Step 416: val loss: 31.422624588012695\n",
      "Step 417: train loss: 2.9175970554351807\n",
      "Step 417: val loss: 31.469453811645508\n",
      "Step 418: train loss: 2.915482759475708\n",
      "Step 418: val loss: 31.51627540588379\n",
      "Step 419: train loss: 2.9133710861206055\n",
      "Step 419: val loss: 31.563081741333008\n",
      "Step 420: train loss: 2.9112629890441895\n",
      "Step 420: val loss: 31.60987091064453\n",
      "Step 421: train loss: 2.9091579914093018\n",
      "Step 421: val loss: 31.656652450561523\n",
      "Step 422: train loss: 2.907055377960205\n",
      "Step 422: val loss: 31.703428268432617\n",
      "Step 423: train loss: 2.904956340789795\n",
      "Step 423: val loss: 31.75018310546875\n",
      "Step 424: train loss: 2.902859926223755\n",
      "Step 424: val loss: 31.796926498413086\n",
      "Step 425: train loss: 2.900766611099243\n",
      "Step 425: val loss: 31.843658447265625\n",
      "Step 426: train loss: 2.8986763954162598\n",
      "Step 426: val loss: 31.890365600585938\n",
      "Step 427: train loss: 2.8965890407562256\n",
      "Step 427: val loss: 31.937074661254883\n",
      "Step 428: train loss: 2.8945047855377197\n",
      "Step 428: val loss: 31.9837589263916\n",
      "Step 429: train loss: 2.892423152923584\n",
      "Step 429: val loss: 32.030426025390625\n",
      "Step 430: train loss: 2.890345335006714\n",
      "Step 430: val loss: 32.07707977294922\n",
      "Step 431: train loss: 2.8882694244384766\n",
      "Step 431: val loss: 32.123722076416016\n",
      "Step 432: train loss: 2.8861966133117676\n",
      "Step 432: val loss: 32.170345306396484\n",
      "Step 433: train loss: 2.884127140045166\n",
      "Step 433: val loss: 32.21695327758789\n",
      "Step 434: train loss: 2.8820605278015137\n",
      "Step 434: val loss: 32.263545989990234\n",
      "Step 435: train loss: 2.8799965381622314\n",
      "Step 435: val loss: 32.31011962890625\n",
      "Step 436: train loss: 2.8779356479644775\n",
      "Step 436: val loss: 32.35667037963867\n",
      "Step 437: train loss: 2.875877618789673\n",
      "Step 437: val loss: 32.40321350097656\n",
      "Step 438: train loss: 2.8738226890563965\n",
      "Step 438: val loss: 32.449737548828125\n",
      "Step 439: train loss: 2.8717703819274902\n",
      "Step 439: val loss: 32.49623489379883\n",
      "Step 440: train loss: 2.8697211742401123\n",
      "Step 440: val loss: 32.542720794677734\n",
      "Step 441: train loss: 2.8676743507385254\n",
      "Step 441: val loss: 32.58918380737305\n",
      "Step 442: train loss: 2.865631341934204\n",
      "Step 442: val loss: 32.635623931884766\n",
      "Step 443: train loss: 2.8635900020599365\n",
      "Step 443: val loss: 32.682044982910156\n",
      "Step 444: train loss: 2.8615522384643555\n",
      "Step 444: val loss: 32.72845458984375\n",
      "Step 445: train loss: 2.8595170974731445\n",
      "Step 445: val loss: 32.77484893798828\n",
      "Step 446: train loss: 2.857485055923462\n",
      "Step 446: val loss: 32.821205139160156\n",
      "Step 447: train loss: 2.8554553985595703\n",
      "Step 447: val loss: 32.86754608154297\n",
      "Step 448: train loss: 2.853428363800049\n",
      "Step 448: val loss: 32.913875579833984\n",
      "Step 449: train loss: 2.851404905319214\n",
      "Step 449: val loss: 32.960174560546875\n",
      "Step 450: train loss: 2.849384307861328\n",
      "Step 450: val loss: 33.0064582824707\n",
      "Step 451: train loss: 2.847365617752075\n",
      "Step 451: val loss: 33.05271530151367\n",
      "Step 452: train loss: 2.8453502655029297\n",
      "Step 452: val loss: 33.09894943237305\n",
      "Step 453: train loss: 2.8433377742767334\n",
      "Step 453: val loss: 33.14516067504883\n",
      "Step 454: train loss: 2.8413281440734863\n",
      "Step 454: val loss: 33.19135665893555\n",
      "Step 455: train loss: 2.8393213748931885\n",
      "Step 455: val loss: 33.237518310546875\n",
      "Step 456: train loss: 2.8373167514801025\n",
      "Step 456: val loss: 33.283668518066406\n",
      "Step 457: train loss: 2.835315465927124\n",
      "Step 457: val loss: 33.329776763916016\n",
      "Step 458: train loss: 2.8333163261413574\n",
      "Step 458: val loss: 33.375877380371094\n",
      "Step 459: train loss: 2.8313205242156982\n",
      "Step 459: val loss: 33.421939849853516\n",
      "Step 460: train loss: 2.829327344894409\n",
      "Step 460: val loss: 33.46799087524414\n",
      "Step 461: train loss: 2.827336311340332\n",
      "Step 461: val loss: 33.514015197753906\n",
      "Step 462: train loss: 2.8253490924835205\n",
      "Step 462: val loss: 33.56000900268555\n",
      "Step 463: train loss: 2.8233642578125\n",
      "Step 463: val loss: 33.605979919433594\n",
      "Step 464: train loss: 2.8213815689086914\n",
      "Step 464: val loss: 33.65192413330078\n",
      "Step 465: train loss: 2.819401979446411\n",
      "Step 465: val loss: 33.69784164428711\n",
      "Step 466: train loss: 2.817424774169922\n",
      "Step 466: val loss: 33.743736267089844\n",
      "Step 467: train loss: 2.815450429916382\n",
      "Step 467: val loss: 33.78960418701172\n",
      "Step 468: train loss: 2.8134796619415283\n",
      "Step 468: val loss: 33.83544158935547\n",
      "Step 469: train loss: 2.8115105628967285\n",
      "Step 469: val loss: 33.881248474121094\n",
      "Step 470: train loss: 2.809544086456299\n",
      "Step 470: val loss: 33.927032470703125\n",
      "Step 471: train loss: 2.8075811862945557\n",
      "Step 471: val loss: 33.97279739379883\n",
      "Step 472: train loss: 2.805619955062866\n",
      "Step 472: val loss: 34.018524169921875\n",
      "Step 473: train loss: 2.803661823272705\n",
      "Step 473: val loss: 34.064231872558594\n",
      "Step 474: train loss: 2.801706075668335\n",
      "Step 474: val loss: 34.10989761352539\n",
      "Step 475: train loss: 2.799753189086914\n",
      "Step 475: val loss: 34.155548095703125\n",
      "Step 476: train loss: 2.7978031635284424\n",
      "Step 476: val loss: 34.2011604309082\n",
      "Step 477: train loss: 2.795855760574341\n",
      "Step 477: val loss: 34.246742248535156\n",
      "Step 478: train loss: 2.7939107418060303\n",
      "Step 478: val loss: 34.29230880737305\n",
      "Step 479: train loss: 2.791968822479248\n",
      "Step 479: val loss: 34.337833404541016\n",
      "Step 480: train loss: 2.7900290489196777\n",
      "Step 480: val loss: 34.38331985473633\n",
      "Step 481: train loss: 2.7880918979644775\n",
      "Step 481: val loss: 34.42878723144531\n",
      "Step 482: train loss: 2.7861571311950684\n",
      "Step 482: val loss: 34.4742317199707\n",
      "Step 483: train loss: 2.7842254638671875\n",
      "Step 483: val loss: 34.51963806152344\n",
      "Step 484: train loss: 2.7822964191436768\n",
      "Step 484: val loss: 34.56501007080078\n",
      "Step 485: train loss: 2.780369281768799\n",
      "Step 485: val loss: 34.61035919189453\n",
      "Step 486: train loss: 2.778445243835449\n",
      "Step 486: val loss: 34.65566635131836\n",
      "Step 487: train loss: 2.776524066925049\n",
      "Step 487: val loss: 34.700950622558594\n",
      "Step 488: train loss: 2.7746052742004395\n",
      "Step 488: val loss: 34.74620056152344\n",
      "Step 489: train loss: 2.772688627243042\n",
      "Step 489: val loss: 34.791412353515625\n",
      "Step 490: train loss: 2.7707748413085938\n",
      "Step 490: val loss: 34.83659744262695\n",
      "Step 491: train loss: 2.7688632011413574\n",
      "Step 491: val loss: 34.881752014160156\n",
      "Step 492: train loss: 2.7669551372528076\n",
      "Step 492: val loss: 34.9268684387207\n",
      "Step 493: train loss: 2.7650485038757324\n",
      "Step 493: val loss: 34.971961975097656\n",
      "Step 494: train loss: 2.7631449699401855\n",
      "Step 494: val loss: 35.017005920410156\n",
      "Step 495: train loss: 2.761244297027588\n",
      "Step 495: val loss: 35.06202697753906\n",
      "Step 496: train loss: 2.759345531463623\n",
      "Step 496: val loss: 35.10700988769531\n",
      "Step 497: train loss: 2.7574493885040283\n",
      "Step 497: val loss: 35.151973724365234\n",
      "Step 498: train loss: 2.7555558681488037\n",
      "Step 498: val loss: 35.1968879699707\n",
      "Step 499: train loss: 2.753664493560791\n",
      "Step 499: val loss: 35.24176788330078\n",
      "Step 500: train loss: 2.7517762184143066\n",
      "Step 500: val loss: 35.28661346435547\n",
      "Step 501: train loss: 2.749890089035034\n",
      "Step 501: val loss: 35.33142852783203\n",
      "Step 502: train loss: 2.748006582260132\n",
      "Step 502: val loss: 35.37621307373047\n",
      "Step 503: train loss: 2.7461254596710205\n",
      "Step 503: val loss: 35.42095184326172\n",
      "Step 504: train loss: 2.7442467212677\n",
      "Step 504: val loss: 35.465667724609375\n",
      "Step 505: train loss: 2.74237060546875\n",
      "Step 505: val loss: 35.51034164428711\n",
      "Step 506: train loss: 2.74049711227417\n",
      "Step 506: val loss: 35.55497360229492\n",
      "Step 507: train loss: 2.738626003265381\n",
      "Step 507: val loss: 35.59957504272461\n",
      "Step 508: train loss: 2.7367568016052246\n",
      "Step 508: val loss: 35.64414596557617\n",
      "Step 509: train loss: 2.7348906993865967\n",
      "Step 509: val loss: 35.68866729736328\n",
      "Step 510: train loss: 2.7330267429351807\n",
      "Step 510: val loss: 35.73316955566406\n",
      "Step 511: train loss: 2.731165885925293\n",
      "Step 511: val loss: 35.777618408203125\n",
      "Step 512: train loss: 2.729306221008301\n",
      "Step 512: val loss: 35.82204055786133\n",
      "Step 513: train loss: 2.727449893951416\n",
      "Step 513: val loss: 35.866416931152344\n",
      "Step 514: train loss: 2.725595712661743\n",
      "Step 514: val loss: 35.9107666015625\n",
      "Step 515: train loss: 2.7237439155578613\n",
      "Step 515: val loss: 35.95507049560547\n",
      "Step 516: train loss: 2.7218949794769287\n",
      "Step 516: val loss: 35.99933624267578\n",
      "Step 517: train loss: 2.72004771232605\n",
      "Step 517: val loss: 36.04356384277344\n",
      "Step 518: train loss: 2.71820330619812\n",
      "Step 518: val loss: 36.0877571105957\n",
      "Step 519: train loss: 2.7163615226745605\n",
      "Step 519: val loss: 36.13190460205078\n",
      "Step 520: train loss: 2.7145211696624756\n",
      "Step 520: val loss: 36.176029205322266\n",
      "Step 521: train loss: 2.712684154510498\n",
      "Step 521: val loss: 36.2200927734375\n",
      "Step 522: train loss: 2.7108495235443115\n",
      "Step 522: val loss: 36.26414489746094\n",
      "Step 523: train loss: 2.709016799926758\n",
      "Step 523: val loss: 36.308135986328125\n",
      "Step 524: train loss: 2.707186222076416\n",
      "Step 524: val loss: 36.35209655761719\n",
      "Step 525: train loss: 2.7053589820861816\n",
      "Step 525: val loss: 36.3960075378418\n",
      "Step 526: train loss: 2.703533172607422\n",
      "Step 526: val loss: 36.43989181518555\n",
      "Step 527: train loss: 2.7017102241516113\n",
      "Step 527: val loss: 36.48372268676758\n",
      "Step 528: train loss: 2.6998891830444336\n",
      "Step 528: val loss: 36.52752685546875\n",
      "Step 529: train loss: 2.698070526123047\n",
      "Step 529: val loss: 36.5712890625\n",
      "Step 530: train loss: 2.6962552070617676\n",
      "Step 530: val loss: 36.6150016784668\n",
      "Step 531: train loss: 2.694441795349121\n",
      "Step 531: val loss: 36.65867233276367\n",
      "Step 532: train loss: 2.6926300525665283\n",
      "Step 532: val loss: 36.70231628417969\n",
      "Step 533: train loss: 2.6908209323883057\n",
      "Step 533: val loss: 36.745906829833984\n",
      "Step 534: train loss: 2.689013719558716\n",
      "Step 534: val loss: 36.789466857910156\n",
      "Step 535: train loss: 2.6872096061706543\n",
      "Step 535: val loss: 36.832984924316406\n",
      "Step 536: train loss: 2.6854076385498047\n",
      "Step 536: val loss: 36.87644577026367\n",
      "Step 537: train loss: 2.683607339859009\n",
      "Step 537: val loss: 36.91987228393555\n",
      "Step 538: train loss: 2.681809902191162\n",
      "Step 538: val loss: 36.9632568359375\n",
      "Step 539: train loss: 2.6800143718719482\n",
      "Step 539: val loss: 37.00661087036133\n",
      "Step 540: train loss: 2.6782212257385254\n",
      "Step 540: val loss: 37.04991912841797\n",
      "Step 541: train loss: 2.6764309406280518\n",
      "Step 541: val loss: 37.093170166015625\n",
      "Step 542: train loss: 2.674642324447632\n",
      "Step 542: val loss: 37.13638687133789\n",
      "Step 543: train loss: 2.672856569290161\n",
      "Step 543: val loss: 37.179561614990234\n",
      "Step 544: train loss: 2.671072483062744\n",
      "Step 544: val loss: 37.222694396972656\n",
      "Step 545: train loss: 2.6692912578582764\n",
      "Step 545: val loss: 37.26578903198242\n",
      "Step 546: train loss: 2.667511463165283\n",
      "Step 546: val loss: 37.308834075927734\n",
      "Step 547: train loss: 2.66573429107666\n",
      "Step 547: val loss: 37.35184097290039\n",
      "Step 548: train loss: 2.663959503173828\n",
      "Step 548: val loss: 37.39480209350586\n",
      "Step 549: train loss: 2.662187099456787\n",
      "Step 549: val loss: 37.437713623046875\n",
      "Step 550: train loss: 2.660416603088379\n",
      "Step 550: val loss: 37.48058319091797\n",
      "Step 551: train loss: 2.6586484909057617\n",
      "Step 551: val loss: 37.52341079711914\n",
      "Step 552: train loss: 2.6568830013275146\n",
      "Step 552: val loss: 37.566184997558594\n",
      "Step 553: train loss: 2.655118703842163\n",
      "Step 553: val loss: 37.60892868041992\n",
      "Step 554: train loss: 2.65335750579834\n",
      "Step 554: val loss: 37.65163040161133\n",
      "Step 555: train loss: 2.651597738265991\n",
      "Step 555: val loss: 37.694271087646484\n",
      "Step 556: train loss: 2.649840831756592\n",
      "Step 556: val loss: 37.736873626708984\n",
      "Step 557: train loss: 2.6480860710144043\n",
      "Step 557: val loss: 37.77943801879883\n",
      "Step 558: train loss: 2.646333694458008\n",
      "Step 558: val loss: 37.82194900512695\n",
      "Step 559: train loss: 2.644582986831665\n",
      "Step 559: val loss: 37.86442184448242\n",
      "Step 560: train loss: 2.6428351402282715\n",
      "Step 560: val loss: 37.9068489074707\n",
      "Step 561: train loss: 2.6410887241363525\n",
      "Step 561: val loss: 37.94922637939453\n",
      "Step 562: train loss: 2.6393449306488037\n",
      "Step 562: val loss: 37.991554260253906\n",
      "Step 563: train loss: 2.6376030445098877\n",
      "Step 563: val loss: 38.033843994140625\n",
      "Step 564: train loss: 2.6358635425567627\n",
      "Step 564: val loss: 38.07608413696289\n",
      "Step 565: train loss: 2.6341257095336914\n",
      "Step 565: val loss: 38.118289947509766\n",
      "Step 566: train loss: 2.6323909759521484\n",
      "Step 566: val loss: 38.16043472290039\n",
      "Step 567: train loss: 2.6306581497192383\n",
      "Step 567: val loss: 38.20252990722656\n",
      "Step 568: train loss: 2.628927230834961\n",
      "Step 568: val loss: 38.244590759277344\n",
      "Step 569: train loss: 2.6271984577178955\n",
      "Step 569: val loss: 38.286598205566406\n",
      "Step 570: train loss: 2.625471830368042\n",
      "Step 570: val loss: 38.32856369018555\n",
      "Step 571: train loss: 2.623746871948242\n",
      "Step 571: val loss: 38.370479583740234\n",
      "Step 572: train loss: 2.6220250129699707\n",
      "Step 572: val loss: 38.41234588623047\n",
      "Step 573: train loss: 2.620304584503174\n",
      "Step 573: val loss: 38.454166412353516\n",
      "Step 574: train loss: 2.618586301803589\n",
      "Step 574: val loss: 38.49595260620117\n",
      "Step 575: train loss: 2.616870880126953\n",
      "Step 575: val loss: 38.53767013549805\n",
      "Step 576: train loss: 2.615156650543213\n",
      "Step 576: val loss: 38.57935333251953\n",
      "Step 577: train loss: 2.6134450435638428\n",
      "Step 577: val loss: 38.62097930908203\n",
      "Step 578: train loss: 2.6117353439331055\n",
      "Step 578: val loss: 38.66256332397461\n",
      "Step 579: train loss: 2.6100285053253174\n",
      "Step 579: val loss: 38.7041015625\n",
      "Step 580: train loss: 2.6083226203918457\n",
      "Step 580: val loss: 38.745582580566406\n",
      "Step 581: train loss: 2.606618642807007\n",
      "Step 581: val loss: 38.787025451660156\n",
      "Step 582: train loss: 2.6049184799194336\n",
      "Step 582: val loss: 38.82841491699219\n",
      "Step 583: train loss: 2.6032185554504395\n",
      "Step 583: val loss: 38.86975860595703\n",
      "Step 584: train loss: 2.6015219688415527\n",
      "Step 584: val loss: 38.91106033325195\n",
      "Step 585: train loss: 2.5998268127441406\n",
      "Step 585: val loss: 38.95230484008789\n",
      "Step 586: train loss: 2.5981338024139404\n",
      "Step 586: val loss: 38.99349594116211\n",
      "Step 587: train loss: 2.596442937850952\n",
      "Step 587: val loss: 39.03464889526367\n",
      "Step 588: train loss: 2.594754457473755\n",
      "Step 588: val loss: 39.075740814208984\n",
      "Step 589: train loss: 2.5930674076080322\n",
      "Step 589: val loss: 39.11680221557617\n",
      "Step 590: train loss: 2.5913827419281006\n",
      "Step 590: val loss: 39.15779113769531\n",
      "Step 591: train loss: 2.589700222015381\n",
      "Step 591: val loss: 39.19874954223633\n",
      "Step 592: train loss: 2.588019371032715\n",
      "Step 592: val loss: 39.239654541015625\n",
      "Step 593: train loss: 2.5863406658172607\n",
      "Step 593: val loss: 39.28049850463867\n",
      "Step 594: train loss: 2.5846641063690186\n",
      "Step 594: val loss: 39.3213005065918\n",
      "Step 595: train loss: 2.5829901695251465\n",
      "Step 595: val loss: 39.36205291748047\n",
      "Step 596: train loss: 2.58131742477417\n",
      "Step 596: val loss: 39.402774810791016\n",
      "Step 597: train loss: 2.5796473026275635\n",
      "Step 597: val loss: 39.44340515136719\n",
      "Step 598: train loss: 2.5779788494110107\n",
      "Step 598: val loss: 39.48401641845703\n",
      "Step 599: train loss: 2.576312303543091\n",
      "Step 599: val loss: 39.52457046508789\n",
      "Step 600: train loss: 2.574648141860962\n",
      "Step 600: val loss: 39.565067291259766\n",
      "Step 601: train loss: 2.5729854106903076\n",
      "Step 601: val loss: 39.60552978515625\n",
      "Step 602: train loss: 2.5713253021240234\n",
      "Step 602: val loss: 39.64592742919922\n",
      "Step 603: train loss: 2.569667100906372\n",
      "Step 603: val loss: 39.68626022338867\n",
      "Step 604: train loss: 2.5680105686187744\n",
      "Step 604: val loss: 39.726566314697266\n",
      "Step 605: train loss: 2.5663561820983887\n",
      "Step 605: val loss: 39.76681137084961\n",
      "Step 606: train loss: 2.564704179763794\n",
      "Step 606: val loss: 39.8070068359375\n",
      "Step 607: train loss: 2.5630533695220947\n",
      "Step 607: val loss: 39.84716033935547\n",
      "Step 608: train loss: 2.5614049434661865\n",
      "Step 608: val loss: 39.88725662231445\n",
      "Step 609: train loss: 2.5597586631774902\n",
      "Step 609: val loss: 39.92729568481445\n",
      "Step 610: train loss: 2.5581140518188477\n",
      "Step 610: val loss: 39.967288970947266\n",
      "Step 611: train loss: 2.556471586227417\n",
      "Step 611: val loss: 40.00722122192383\n",
      "Step 612: train loss: 2.5548312664031982\n",
      "Step 612: val loss: 40.0471076965332\n",
      "Step 613: train loss: 2.553192615509033\n",
      "Step 613: val loss: 40.086952209472656\n",
      "Step 614: train loss: 2.551555871963501\n",
      "Step 614: val loss: 40.126739501953125\n",
      "Step 615: train loss: 2.5499212741851807\n",
      "Step 615: val loss: 40.16646957397461\n",
      "Step 616: train loss: 2.548288345336914\n",
      "Step 616: val loss: 40.20615768432617\n",
      "Step 617: train loss: 2.546657085418701\n",
      "Step 617: val loss: 40.245784759521484\n",
      "Step 618: train loss: 2.5450286865234375\n",
      "Step 618: val loss: 40.28535842895508\n",
      "Step 619: train loss: 2.5434014797210693\n",
      "Step 619: val loss: 40.32487869262695\n",
      "Step 620: train loss: 2.541776657104492\n",
      "Step 620: val loss: 40.36436080932617\n",
      "Step 621: train loss: 2.540153980255127\n",
      "Step 621: val loss: 40.40377426147461\n",
      "Step 622: train loss: 2.5385324954986572\n",
      "Step 622: val loss: 40.44314193725586\n",
      "Step 623: train loss: 2.5369133949279785\n",
      "Step 623: val loss: 40.482460021972656\n",
      "Step 624: train loss: 2.5352959632873535\n",
      "Step 624: val loss: 40.521724700927734\n",
      "Step 625: train loss: 2.5336804389953613\n",
      "Step 625: val loss: 40.560935974121094\n",
      "Step 626: train loss: 2.532067060470581\n",
      "Step 626: val loss: 40.6000862121582\n",
      "Step 627: train loss: 2.5304551124572754\n",
      "Step 627: val loss: 40.639190673828125\n",
      "Step 628: train loss: 2.52884578704834\n",
      "Step 628: val loss: 40.678245544433594\n",
      "Step 629: train loss: 2.5272376537323\n",
      "Step 629: val loss: 40.717247009277344\n",
      "Step 630: train loss: 2.525631904602051\n",
      "Step 630: val loss: 40.75618362426758\n",
      "Step 631: train loss: 2.5240280628204346\n",
      "Step 631: val loss: 40.795074462890625\n",
      "Step 632: train loss: 2.522425651550293\n",
      "Step 632: val loss: 40.83391189575195\n",
      "Step 633: train loss: 2.5208258628845215\n",
      "Step 633: val loss: 40.87269592285156\n",
      "Step 634: train loss: 2.5192272663116455\n",
      "Step 634: val loss: 40.91142272949219\n",
      "Step 635: train loss: 2.5176308155059814\n",
      "Step 635: val loss: 40.95009994506836\n",
      "Step 636: train loss: 2.516035795211792\n",
      "Step 636: val loss: 40.988731384277344\n",
      "Step 637: train loss: 2.5144429206848145\n",
      "Step 637: val loss: 41.02730178833008\n",
      "Step 638: train loss: 2.512852191925049\n",
      "Step 638: val loss: 41.06581497192383\n",
      "Step 639: train loss: 2.511263132095337\n",
      "Step 639: val loss: 41.10427474975586\n",
      "Step 640: train loss: 2.5096757411956787\n",
      "Step 640: val loss: 41.14268112182617\n",
      "Step 641: train loss: 2.5080904960632324\n",
      "Step 641: val loss: 41.181034088134766\n",
      "Step 642: train loss: 2.5065066814422607\n",
      "Step 642: val loss: 41.21934127807617\n",
      "Step 643: train loss: 2.50492525100708\n",
      "Step 643: val loss: 41.25758743286133\n",
      "Step 644: train loss: 2.503345489501953\n",
      "Step 644: val loss: 41.29576873779297\n",
      "Step 645: train loss: 2.501767635345459\n",
      "Step 645: val loss: 41.333900451660156\n",
      "Step 646: train loss: 2.5001914501190186\n",
      "Step 646: val loss: 41.37198257446289\n",
      "Step 647: train loss: 2.4986166954040527\n",
      "Step 647: val loss: 41.41001510620117\n",
      "Step 648: train loss: 2.497044801712036\n",
      "Step 648: val loss: 41.44799041748047\n",
      "Step 649: train loss: 2.495473861694336\n",
      "Step 649: val loss: 41.485904693603516\n",
      "Step 650: train loss: 2.4939050674438477\n",
      "Step 650: val loss: 41.52377700805664\n",
      "Step 651: train loss: 2.492337703704834\n",
      "Step 651: val loss: 41.561580657958984\n",
      "Step 652: train loss: 2.4907727241516113\n",
      "Step 652: val loss: 41.599334716796875\n",
      "Step 653: train loss: 2.489208698272705\n",
      "Step 653: val loss: 41.63703155517578\n",
      "Step 654: train loss: 2.487647533416748\n",
      "Step 654: val loss: 41.674678802490234\n",
      "Step 655: train loss: 2.4860877990722656\n",
      "Step 655: val loss: 41.71226501464844\n",
      "Step 656: train loss: 2.484529733657837\n",
      "Step 656: val loss: 41.74980163574219\n",
      "Step 657: train loss: 2.482973575592041\n",
      "Step 657: val loss: 41.78727340698242\n",
      "Step 658: train loss: 2.4814186096191406\n",
      "Step 658: val loss: 41.8246955871582\n",
      "Step 659: train loss: 2.4798665046691895\n",
      "Step 659: val loss: 41.86206817626953\n",
      "Step 660: train loss: 2.478315591812134\n",
      "Step 660: val loss: 41.89937973022461\n",
      "Step 661: train loss: 2.476766347885132\n",
      "Step 661: val loss: 41.936641693115234\n",
      "Step 662: train loss: 2.4752187728881836\n",
      "Step 662: val loss: 41.973838806152344\n",
      "Step 663: train loss: 2.4736733436584473\n",
      "Step 663: val loss: 42.010990142822266\n",
      "Step 664: train loss: 2.4721293449401855\n",
      "Step 664: val loss: 42.04807662963867\n",
      "Step 665: train loss: 2.4705872535705566\n",
      "Step 665: val loss: 42.08510208129883\n",
      "Step 666: train loss: 2.4690475463867188\n",
      "Step 666: val loss: 42.12208557128906\n",
      "Step 667: train loss: 2.467508554458618\n",
      "Step 667: val loss: 42.15901565551758\n",
      "Step 668: train loss: 2.4659714698791504\n",
      "Step 668: val loss: 42.195884704589844\n",
      "Step 669: train loss: 2.4644370079040527\n",
      "Step 669: val loss: 42.232688903808594\n",
      "Step 670: train loss: 2.4629034996032715\n",
      "Step 670: val loss: 42.269447326660156\n",
      "Step 671: train loss: 2.461371898651123\n",
      "Step 671: val loss: 42.3061408996582\n",
      "Step 672: train loss: 2.4598419666290283\n",
      "Step 672: val loss: 42.3427848815918\n",
      "Step 673: train loss: 2.4583139419555664\n",
      "Step 673: val loss: 42.37937545776367\n",
      "Step 674: train loss: 2.456787586212158\n",
      "Step 674: val loss: 42.41590881347656\n",
      "Step 675: train loss: 2.455263137817383\n",
      "Step 675: val loss: 42.45237350463867\n",
      "Step 676: train loss: 2.453740358352661\n",
      "Step 676: val loss: 42.48880386352539\n",
      "Step 677: train loss: 2.452218770980835\n",
      "Step 677: val loss: 42.52516555786133\n",
      "Step 678: train loss: 2.4506995677948\n",
      "Step 678: val loss: 42.56146240234375\n",
      "Step 679: train loss: 2.4491820335388184\n",
      "Step 679: val loss: 42.59770965576172\n",
      "Step 680: train loss: 2.4476661682128906\n",
      "Step 680: val loss: 42.633914947509766\n",
      "Step 681: train loss: 2.4461519718170166\n",
      "Step 681: val loss: 42.670040130615234\n",
      "Step 682: train loss: 2.444639205932617\n",
      "Step 682: val loss: 42.70612335205078\n",
      "Step 683: train loss: 2.4431283473968506\n",
      "Step 683: val loss: 42.74215316772461\n",
      "Step 684: train loss: 2.4416186809539795\n",
      "Step 684: val loss: 42.778114318847656\n",
      "Step 685: train loss: 2.440110921859741\n",
      "Step 685: val loss: 42.81402587890625\n",
      "Step 686: train loss: 2.438605308532715\n",
      "Step 686: val loss: 42.849876403808594\n",
      "Step 687: train loss: 2.437101364135742\n",
      "Step 687: val loss: 42.88567352294922\n",
      "Step 688: train loss: 2.4355990886688232\n",
      "Step 688: val loss: 42.92141342163086\n",
      "Step 689: train loss: 2.434098720550537\n",
      "Step 689: val loss: 42.957096099853516\n",
      "Step 690: train loss: 2.4325993061065674\n",
      "Step 690: val loss: 42.99272537231445\n",
      "Step 691: train loss: 2.4311020374298096\n",
      "Step 691: val loss: 43.028297424316406\n",
      "Step 692: train loss: 2.4296061992645264\n",
      "Step 692: val loss: 43.06379318237305\n",
      "Step 693: train loss: 2.4281115531921387\n",
      "Step 693: val loss: 43.09925079345703\n",
      "Step 694: train loss: 2.4266197681427\n",
      "Step 694: val loss: 43.134647369384766\n",
      "Step 695: train loss: 2.425128698348999\n",
      "Step 695: val loss: 43.16999053955078\n",
      "Step 696: train loss: 2.423640012741089\n",
      "Step 696: val loss: 43.205265045166016\n",
      "Step 697: train loss: 2.422152280807495\n",
      "Step 697: val loss: 43.24049377441406\n",
      "Step 698: train loss: 2.4206666946411133\n",
      "Step 698: val loss: 43.275665283203125\n",
      "Step 699: train loss: 2.419182538986206\n",
      "Step 699: val loss: 43.3107795715332\n",
      "Step 700: train loss: 2.4176998138427734\n",
      "Step 700: val loss: 43.345829010009766\n",
      "Step 701: train loss: 2.4162189960479736\n",
      "Step 701: val loss: 43.380828857421875\n",
      "Step 702: train loss: 2.4147400856018066\n",
      "Step 702: val loss: 43.41576385498047\n",
      "Step 703: train loss: 2.4132628440856934\n",
      "Step 703: val loss: 43.45064163208008\n",
      "Step 704: train loss: 2.4117863178253174\n",
      "Step 704: val loss: 43.485469818115234\n",
      "Step 705: train loss: 2.4103124141693115\n",
      "Step 705: val loss: 43.520233154296875\n",
      "Step 706: train loss: 2.408839702606201\n",
      "Step 706: val loss: 43.55495071411133\n",
      "Step 707: train loss: 2.4073688983917236\n",
      "Step 707: val loss: 43.58958435058594\n",
      "Step 708: train loss: 2.4058988094329834\n",
      "Step 708: val loss: 43.62418746948242\n",
      "Step 709: train loss: 2.4044315814971924\n",
      "Step 709: val loss: 43.658721923828125\n",
      "Step 710: train loss: 2.4029653072357178\n",
      "Step 710: val loss: 43.69319152832031\n",
      "Step 711: train loss: 2.401500940322876\n",
      "Step 711: val loss: 43.727622985839844\n",
      "Step 712: train loss: 2.4000377655029297\n",
      "Step 712: val loss: 43.76197814941406\n",
      "Step 713: train loss: 2.398576259613037\n",
      "Step 713: val loss: 43.79628372192383\n",
      "Step 714: train loss: 2.3971168994903564\n",
      "Step 714: val loss: 43.83053207397461\n",
      "Step 715: train loss: 2.395658493041992\n",
      "Step 715: val loss: 43.864715576171875\n",
      "Step 716: train loss: 2.3942019939422607\n",
      "Step 716: val loss: 43.898841857910156\n",
      "Step 717: train loss: 2.392747163772583\n",
      "Step 717: val loss: 43.932918548583984\n",
      "Step 718: train loss: 2.391293525695801\n",
      "Step 718: val loss: 43.96694564819336\n",
      "Step 719: train loss: 2.3898420333862305\n",
      "Step 719: val loss: 44.00090408325195\n",
      "Step 720: train loss: 2.3883917331695557\n",
      "Step 720: val loss: 44.03479766845703\n",
      "Step 721: train loss: 2.3869433403015137\n",
      "Step 721: val loss: 44.068634033203125\n",
      "Step 722: train loss: 2.385496139526367\n",
      "Step 722: val loss: 44.102413177490234\n",
      "Step 723: train loss: 2.3840506076812744\n",
      "Step 723: val loss: 44.13613510131836\n",
      "Step 724: train loss: 2.3826069831848145\n",
      "Step 724: val loss: 44.16980743408203\n",
      "Step 725: train loss: 2.381164073944092\n",
      "Step 725: val loss: 44.20341491699219\n",
      "Step 726: train loss: 2.379723310470581\n",
      "Step 726: val loss: 44.236968994140625\n",
      "Step 727: train loss: 2.378284215927124\n",
      "Step 727: val loss: 44.27046585083008\n",
      "Step 728: train loss: 2.3768470287323\n",
      "Step 728: val loss: 44.303890228271484\n",
      "Step 729: train loss: 2.375410556793213\n",
      "Step 729: val loss: 44.3372688293457\n",
      "Step 730: train loss: 2.373976230621338\n",
      "Step 730: val loss: 44.370582580566406\n",
      "Step 731: train loss: 2.3725433349609375\n",
      "Step 731: val loss: 44.40385055541992\n",
      "Step 732: train loss: 2.3711116313934326\n",
      "Step 732: val loss: 44.437042236328125\n",
      "Step 733: train loss: 2.3696820735931396\n",
      "Step 733: val loss: 44.47018814086914\n",
      "Step 734: train loss: 2.368253707885742\n",
      "Step 734: val loss: 44.50326919555664\n",
      "Step 735: train loss: 2.3668274879455566\n",
      "Step 735: val loss: 44.536293029785156\n",
      "Step 736: train loss: 2.3654017448425293\n",
      "Step 736: val loss: 44.56926727294922\n",
      "Step 737: train loss: 2.3639779090881348\n",
      "Step 737: val loss: 44.60216522216797\n",
      "Step 738: train loss: 2.362555980682373\n",
      "Step 738: val loss: 44.63501739501953\n",
      "Step 739: train loss: 2.361135721206665\n",
      "Step 739: val loss: 44.66780090332031\n",
      "Step 740: train loss: 2.3597164154052734\n",
      "Step 740: val loss: 44.700538635253906\n",
      "Step 741: train loss: 2.3582987785339355\n",
      "Step 741: val loss: 44.73321533203125\n",
      "Step 742: train loss: 2.3568825721740723\n",
      "Step 742: val loss: 44.765830993652344\n",
      "Step 743: train loss: 2.3554680347442627\n",
      "Step 743: val loss: 44.798397064208984\n",
      "Step 744: train loss: 2.3540549278259277\n",
      "Step 744: val loss: 44.83088684082031\n",
      "Step 745: train loss: 2.3526434898376465\n",
      "Step 745: val loss: 44.86333465576172\n",
      "Step 746: train loss: 2.3512332439422607\n",
      "Step 746: val loss: 44.89570999145508\n",
      "Step 747: train loss: 2.349825143814087\n",
      "Step 747: val loss: 44.928035736083984\n",
      "Step 748: train loss: 2.3484179973602295\n",
      "Step 748: val loss: 44.960304260253906\n",
      "Step 749: train loss: 2.347012996673584\n",
      "Step 749: val loss: 44.99250030517578\n",
      "Step 750: train loss: 2.3456084728240967\n",
      "Step 750: val loss: 45.0246467590332\n",
      "Step 751: train loss: 2.3442060947418213\n",
      "Step 751: val loss: 45.05674362182617\n",
      "Step 752: train loss: 2.3428051471710205\n",
      "Step 752: val loss: 45.088768005371094\n",
      "Step 753: train loss: 2.3414058685302734\n",
      "Step 753: val loss: 45.12073516845703\n",
      "Step 754: train loss: 2.3400075435638428\n",
      "Step 754: val loss: 45.15264892578125\n",
      "Step 755: train loss: 2.338611364364624\n",
      "Step 755: val loss: 45.18450164794922\n",
      "Step 756: train loss: 2.3372159004211426\n",
      "Step 756: val loss: 45.2162971496582\n",
      "Step 757: train loss: 2.335822343826294\n",
      "Step 757: val loss: 45.24803924560547\n",
      "Step 758: train loss: 2.33443021774292\n",
      "Step 758: val loss: 45.27972412109375\n",
      "Step 759: train loss: 2.3330397605895996\n",
      "Step 759: val loss: 45.311336517333984\n",
      "Step 760: train loss: 2.3316500186920166\n",
      "Step 760: val loss: 45.342899322509766\n",
      "Step 761: train loss: 2.3302626609802246\n",
      "Step 761: val loss: 45.37440490722656\n",
      "Step 762: train loss: 2.328876256942749\n",
      "Step 762: val loss: 45.40584945678711\n",
      "Step 763: train loss: 2.3274917602539062\n",
      "Step 763: val loss: 45.43722152709961\n",
      "Step 764: train loss: 2.326108455657959\n",
      "Step 764: val loss: 45.468544006347656\n",
      "Step 765: train loss: 2.3247265815734863\n",
      "Step 765: val loss: 45.499820709228516\n",
      "Step 766: train loss: 2.3233463764190674\n",
      "Step 766: val loss: 45.53102493286133\n",
      "Step 767: train loss: 2.3219668865203857\n",
      "Step 767: val loss: 45.56216812133789\n",
      "Step 768: train loss: 2.320589542388916\n",
      "Step 768: val loss: 45.593265533447266\n",
      "Step 769: train loss: 2.319213390350342\n",
      "Step 769: val loss: 45.62429428100586\n",
      "Step 770: train loss: 2.3178393840789795\n",
      "Step 770: val loss: 45.6552619934082\n",
      "Step 771: train loss: 2.3164658546447754\n",
      "Step 771: val loss: 45.686180114746094\n",
      "Step 772: train loss: 2.315094232559204\n",
      "Step 772: val loss: 45.7170295715332\n",
      "Step 773: train loss: 2.3137238025665283\n",
      "Step 773: val loss: 45.74782180786133\n",
      "Step 774: train loss: 2.312354564666748\n",
      "Step 774: val loss: 45.77857208251953\n",
      "Step 775: train loss: 2.3109874725341797\n",
      "Step 775: val loss: 45.80924606323242\n",
      "Step 776: train loss: 2.3096208572387695\n",
      "Step 776: val loss: 45.839866638183594\n",
      "Step 777: train loss: 2.3082566261291504\n",
      "Step 777: val loss: 45.87042236328125\n",
      "Step 778: train loss: 2.3068935871124268\n",
      "Step 778: val loss: 45.90092468261719\n",
      "Step 779: train loss: 2.3055315017700195\n",
      "Step 779: val loss: 45.931365966796875\n",
      "Step 780: train loss: 2.304171085357666\n",
      "Step 780: val loss: 45.961753845214844\n",
      "Step 781: train loss: 2.3028125762939453\n",
      "Step 781: val loss: 45.9920654296875\n",
      "Step 782: train loss: 2.301455020904541\n",
      "Step 782: val loss: 46.022342681884766\n",
      "Step 783: train loss: 2.300098419189453\n",
      "Step 783: val loss: 46.052547454833984\n",
      "Step 784: train loss: 2.298743963241577\n",
      "Step 784: val loss: 46.08269500732422\n",
      "Step 785: train loss: 2.2973904609680176\n",
      "Step 785: val loss: 46.11278533935547\n",
      "Step 786: train loss: 2.29603910446167\n",
      "Step 786: val loss: 46.14280700683594\n",
      "Step 787: train loss: 2.2946882247924805\n",
      "Step 787: val loss: 46.172786712646484\n",
      "Step 788: train loss: 2.293339252471924\n",
      "Step 788: val loss: 46.202701568603516\n",
      "Step 789: train loss: 2.291991710662842\n",
      "Step 789: val loss: 46.23255157470703\n",
      "Step 790: train loss: 2.290645122528076\n",
      "Step 790: val loss: 46.26234817504883\n",
      "Step 791: train loss: 2.2893009185791016\n",
      "Step 791: val loss: 46.29206848144531\n",
      "Step 792: train loss: 2.287957191467285\n",
      "Step 792: val loss: 46.321739196777344\n",
      "Step 793: train loss: 2.2866146564483643\n",
      "Step 793: val loss: 46.351375579833984\n",
      "Step 794: train loss: 2.2852742671966553\n",
      "Step 794: val loss: 46.380916595458984\n",
      "Step 795: train loss: 2.283935070037842\n",
      "Step 795: val loss: 46.410430908203125\n",
      "Step 796: train loss: 2.282597064971924\n",
      "Step 796: val loss: 46.43986511230469\n",
      "Step 797: train loss: 2.2812602519989014\n",
      "Step 797: val loss: 46.469242095947266\n",
      "Step 798: train loss: 2.2799253463745117\n",
      "Step 798: val loss: 46.49856185913086\n",
      "Step 799: train loss: 2.2785913944244385\n",
      "Step 799: val loss: 46.5278205871582\n",
      "Step 800: train loss: 2.2772586345672607\n",
      "Step 800: val loss: 46.55703353881836\n",
      "Step 801: train loss: 2.275927782058716\n",
      "Step 801: val loss: 46.5861701965332\n",
      "Step 802: train loss: 2.2745981216430664\n",
      "Step 802: val loss: 46.615264892578125\n",
      "Step 803: train loss: 2.2732698917388916\n",
      "Step 803: val loss: 46.64429473876953\n",
      "Step 804: train loss: 2.2719428539276123\n",
      "Step 804: val loss: 46.67325210571289\n",
      "Step 805: train loss: 2.2706170082092285\n",
      "Step 805: val loss: 46.702171325683594\n",
      "Step 806: train loss: 2.2692923545837402\n",
      "Step 806: val loss: 46.73102951049805\n",
      "Step 807: train loss: 2.267969846725464\n",
      "Step 807: val loss: 46.75980758666992\n",
      "Step 808: train loss: 2.266648530960083\n",
      "Step 808: val loss: 46.78855514526367\n",
      "Step 809: train loss: 2.2653279304504395\n",
      "Step 809: val loss: 46.81721878051758\n",
      "Step 810: train loss: 2.2640089988708496\n",
      "Step 810: val loss: 46.84583282470703\n",
      "Step 811: train loss: 2.2626914978027344\n",
      "Step 811: val loss: 46.8743896484375\n",
      "Step 812: train loss: 2.2613754272460938\n",
      "Step 812: val loss: 46.902896881103516\n",
      "Step 813: train loss: 2.2600600719451904\n",
      "Step 813: val loss: 46.931331634521484\n",
      "Step 814: train loss: 2.25874662399292\n",
      "Step 814: val loss: 46.95970916748047\n",
      "Step 815: train loss: 2.257434606552124\n",
      "Step 815: val loss: 46.9880256652832\n",
      "Step 816: train loss: 2.2561233043670654\n",
      "Step 816: val loss: 47.016292572021484\n",
      "Step 817: train loss: 2.2548141479492188\n",
      "Step 817: val loss: 47.04450225830078\n",
      "Step 818: train loss: 2.253505229949951\n",
      "Step 818: val loss: 47.07263946533203\n",
      "Step 819: train loss: 2.2521984577178955\n",
      "Step 819: val loss: 47.10073471069336\n",
      "Step 820: train loss: 2.2508931159973145\n",
      "Step 820: val loss: 47.12876510620117\n",
      "Step 821: train loss: 2.2495884895324707\n",
      "Step 821: val loss: 47.15673065185547\n",
      "Step 822: train loss: 2.2482857704162598\n",
      "Step 822: val loss: 47.18463897705078\n",
      "Step 823: train loss: 2.2469840049743652\n",
      "Step 823: val loss: 47.21249008178711\n",
      "Step 824: train loss: 2.2456836700439453\n",
      "Step 824: val loss: 47.24027633666992\n",
      "Step 825: train loss: 2.244385004043579\n",
      "Step 825: val loss: 47.26801300048828\n",
      "Step 826: train loss: 2.24308705329895\n",
      "Step 826: val loss: 47.295692443847656\n",
      "Step 827: train loss: 2.241790771484375\n",
      "Step 827: val loss: 47.323299407958984\n",
      "Step 828: train loss: 2.240495443344116\n",
      "Step 828: val loss: 47.350868225097656\n",
      "Step 829: train loss: 2.239201307296753\n",
      "Step 829: val loss: 47.37836456298828\n",
      "Step 830: train loss: 2.2379088401794434\n",
      "Step 830: val loss: 47.405799865722656\n",
      "Step 831: train loss: 2.2366180419921875\n",
      "Step 831: val loss: 47.43317794799805\n",
      "Step 832: train loss: 2.235327959060669\n",
      "Step 832: val loss: 47.46050262451172\n",
      "Step 833: train loss: 2.234039306640625\n",
      "Step 833: val loss: 47.487770080566406\n",
      "Step 834: train loss: 2.2327516078948975\n",
      "Step 834: val loss: 47.514976501464844\n",
      "Step 835: train loss: 2.2314651012420654\n",
      "Step 835: val loss: 47.54212951660156\n",
      "Step 836: train loss: 2.230180501937866\n",
      "Step 836: val loss: 47.5692138671875\n",
      "Step 837: train loss: 2.2288975715637207\n",
      "Step 837: val loss: 47.59624481201172\n",
      "Step 838: train loss: 2.2276151180267334\n",
      "Step 838: val loss: 47.62321090698242\n",
      "Step 839: train loss: 2.2263340950012207\n",
      "Step 839: val loss: 47.650123596191406\n",
      "Step 840: train loss: 2.2250547409057617\n",
      "Step 840: val loss: 47.67697525024414\n",
      "Step 841: train loss: 2.223776340484619\n",
      "Step 841: val loss: 47.70378112792969\n",
      "Step 842: train loss: 2.222498893737793\n",
      "Step 842: val loss: 47.730506896972656\n",
      "Step 843: train loss: 2.2212233543395996\n",
      "Step 843: val loss: 47.7571907043457\n",
      "Step 844: train loss: 2.2199482917785645\n",
      "Step 844: val loss: 47.78380584716797\n",
      "Step 845: train loss: 2.218675136566162\n",
      "Step 845: val loss: 47.81037139892578\n",
      "Step 846: train loss: 2.2174031734466553\n",
      "Step 846: val loss: 47.83687973022461\n",
      "Step 847: train loss: 2.216132402420044\n",
      "Step 847: val loss: 47.86332321166992\n",
      "Step 848: train loss: 2.214862585067749\n",
      "Step 848: val loss: 47.88970184326172\n",
      "Step 849: train loss: 2.213594436645508\n",
      "Step 849: val loss: 47.91604232788086\n",
      "Step 850: train loss: 2.212327241897583\n",
      "Step 850: val loss: 47.94231414794922\n",
      "Step 851: train loss: 2.2110612392425537\n",
      "Step 851: val loss: 47.96852493286133\n",
      "Step 852: train loss: 2.209796667098999\n",
      "Step 852: val loss: 47.99467468261719\n",
      "Step 853: train loss: 2.2085342407226562\n",
      "Step 853: val loss: 48.02075958251953\n",
      "Step 854: train loss: 2.2072715759277344\n",
      "Step 854: val loss: 48.04680252075195\n",
      "Step 855: train loss: 2.2060108184814453\n",
      "Step 855: val loss: 48.072784423828125\n",
      "Step 856: train loss: 2.2047510147094727\n",
      "Step 856: val loss: 48.09870147705078\n",
      "Step 857: train loss: 2.2034928798675537\n",
      "Step 857: val loss: 48.12455749511719\n",
      "Step 858: train loss: 2.202235698699951\n",
      "Step 858: val loss: 48.150367736816406\n",
      "Step 859: train loss: 2.200979709625244\n",
      "Step 859: val loss: 48.17611312866211\n",
      "Step 860: train loss: 2.199725389480591\n",
      "Step 860: val loss: 48.20179748535156\n",
      "Step 861: train loss: 2.198471784591675\n",
      "Step 861: val loss: 48.227420806884766\n",
      "Step 862: train loss: 2.1972193717956543\n",
      "Step 862: val loss: 48.25300598144531\n",
      "Step 863: train loss: 2.1959683895111084\n",
      "Step 863: val loss: 48.27851486206055\n",
      "Step 864: train loss: 2.1947193145751953\n",
      "Step 864: val loss: 48.3039665222168\n",
      "Step 865: train loss: 2.1934704780578613\n",
      "Step 865: val loss: 48.32936477661133\n",
      "Step 866: train loss: 2.192223072052002\n",
      "Step 866: val loss: 48.35470199584961\n",
      "Step 867: train loss: 2.190976619720459\n",
      "Step 867: val loss: 48.3799934387207\n",
      "Step 868: train loss: 2.189732074737549\n",
      "Step 868: val loss: 48.405208587646484\n",
      "Step 869: train loss: 2.188488245010376\n",
      "Step 869: val loss: 48.43037033081055\n",
      "Step 870: train loss: 2.1872456073760986\n",
      "Step 870: val loss: 48.45547866821289\n",
      "Step 871: train loss: 2.186005115509033\n",
      "Step 871: val loss: 48.48052978515625\n",
      "Step 872: train loss: 2.184765100479126\n",
      "Step 872: val loss: 48.50551986694336\n",
      "Step 873: train loss: 2.183525562286377\n",
      "Step 873: val loss: 48.53044891357422\n",
      "Step 874: train loss: 2.18228816986084\n",
      "Step 874: val loss: 48.55532455444336\n",
      "Step 875: train loss: 2.181051731109619\n",
      "Step 875: val loss: 48.58015441894531\n",
      "Step 876: train loss: 2.179816484451294\n",
      "Step 876: val loss: 48.604896545410156\n",
      "Step 877: train loss: 2.1785826683044434\n",
      "Step 877: val loss: 48.629608154296875\n",
      "Step 878: train loss: 2.177349805831909\n",
      "Step 878: val loss: 48.65424728393555\n",
      "Step 879: train loss: 2.1761186122894287\n",
      "Step 879: val loss: 48.678836822509766\n",
      "Step 880: train loss: 2.174887180328369\n",
      "Step 880: val loss: 48.70335388183594\n",
      "Step 881: train loss: 2.173658847808838\n",
      "Step 881: val loss: 48.72782897949219\n",
      "Step 882: train loss: 2.1724302768707275\n",
      "Step 882: val loss: 48.752254486083984\n",
      "Step 883: train loss: 2.171203136444092\n",
      "Step 883: val loss: 48.77659606933594\n",
      "Step 884: train loss: 2.1699774265289307\n",
      "Step 884: val loss: 48.800899505615234\n",
      "Step 885: train loss: 2.168753147125244\n",
      "Step 885: val loss: 48.825138092041016\n",
      "Step 886: train loss: 2.167529821395874\n",
      "Step 886: val loss: 48.84932327270508\n",
      "Step 887: train loss: 2.1663074493408203\n",
      "Step 887: val loss: 48.87344741821289\n",
      "Step 888: train loss: 2.1650867462158203\n",
      "Step 888: val loss: 48.89751434326172\n",
      "Step 889: train loss: 2.1638667583465576\n",
      "Step 889: val loss: 48.92152404785156\n",
      "Step 890: train loss: 2.1626484394073486\n",
      "Step 890: val loss: 48.94548034667969\n",
      "Step 891: train loss: 2.161430835723877\n",
      "Step 891: val loss: 48.9693717956543\n",
      "Step 892: train loss: 2.160214424133301\n",
      "Step 892: val loss: 48.993202209472656\n",
      "Step 893: train loss: 2.158999443054199\n",
      "Step 893: val loss: 49.01697540283203\n",
      "Step 894: train loss: 2.157785177230835\n",
      "Step 894: val loss: 49.040706634521484\n",
      "Step 895: train loss: 2.1565723419189453\n",
      "Step 895: val loss: 49.06437683105469\n",
      "Step 896: train loss: 2.155360698699951\n",
      "Step 896: val loss: 49.08797836303711\n",
      "Step 897: train loss: 2.1541502475738525\n",
      "Step 897: val loss: 49.11153030395508\n",
      "Step 898: train loss: 2.1529409885406494\n",
      "Step 898: val loss: 49.13502883911133\n",
      "Step 899: train loss: 2.1517324447631836\n",
      "Step 899: val loss: 49.158470153808594\n",
      "Step 900: train loss: 2.1505253314971924\n",
      "Step 900: val loss: 49.18183517456055\n",
      "Step 901: train loss: 2.149319887161255\n",
      "Step 901: val loss: 49.20516586303711\n",
      "Step 902: train loss: 2.1481146812438965\n",
      "Step 902: val loss: 49.228424072265625\n",
      "Step 903: train loss: 2.1469109058380127\n",
      "Step 903: val loss: 49.25164031982422\n",
      "Step 904: train loss: 2.1457085609436035\n",
      "Step 904: val loss: 49.274776458740234\n",
      "Step 905: train loss: 2.144507884979248\n",
      "Step 905: val loss: 49.297882080078125\n",
      "Step 906: train loss: 2.1433069705963135\n",
      "Step 906: val loss: 49.320919036865234\n",
      "Step 907: train loss: 2.1421077251434326\n",
      "Step 907: val loss: 49.343894958496094\n",
      "Step 908: train loss: 2.1409101486206055\n",
      "Step 908: val loss: 49.3668212890625\n",
      "Step 909: train loss: 2.1397128105163574\n",
      "Step 909: val loss: 49.38969039916992\n",
      "Step 910: train loss: 2.138517379760742\n",
      "Step 910: val loss: 49.412498474121094\n",
      "Step 911: train loss: 2.1373229026794434\n",
      "Step 911: val loss: 49.43524169921875\n",
      "Step 912: train loss: 2.13612961769104\n",
      "Step 912: val loss: 49.45793914794922\n",
      "Step 913: train loss: 2.134937286376953\n",
      "Step 913: val loss: 49.48058319091797\n",
      "Step 914: train loss: 2.1337461471557617\n",
      "Step 914: val loss: 49.5031623840332\n",
      "Step 915: train loss: 2.1325557231903076\n",
      "Step 915: val loss: 49.525691986083984\n",
      "Step 916: train loss: 2.1313674449920654\n",
      "Step 916: val loss: 49.54814529418945\n",
      "Step 917: train loss: 2.1301798820495605\n",
      "Step 917: val loss: 49.570552825927734\n",
      "Step 918: train loss: 2.128992795944214\n",
      "Step 918: val loss: 49.59291458129883\n",
      "Step 919: train loss: 2.1278076171875\n",
      "Step 919: val loss: 49.615211486816406\n",
      "Step 920: train loss: 2.1266226768493652\n",
      "Step 920: val loss: 49.637454986572266\n",
      "Step 921: train loss: 2.1254396438598633\n",
      "Step 921: val loss: 49.659637451171875\n",
      "Step 922: train loss: 2.1242575645446777\n",
      "Step 922: val loss: 49.681766510009766\n",
      "Step 923: train loss: 2.1230759620666504\n",
      "Step 923: val loss: 49.70384216308594\n",
      "Step 924: train loss: 2.121896266937256\n",
      "Step 924: val loss: 49.72584915161133\n",
      "Step 925: train loss: 2.1207175254821777\n",
      "Step 925: val loss: 49.7478141784668\n",
      "Step 926: train loss: 2.119539737701416\n",
      "Step 926: val loss: 49.769718170166016\n",
      "Step 927: train loss: 2.1183629035949707\n",
      "Step 927: val loss: 49.791561126708984\n",
      "Step 928: train loss: 2.117187023162842\n",
      "Step 928: val loss: 49.81334686279297\n",
      "Step 929: train loss: 2.1160123348236084\n",
      "Step 929: val loss: 49.8350830078125\n",
      "Step 930: train loss: 2.1148393154144287\n",
      "Step 930: val loss: 49.85675048828125\n",
      "Step 931: train loss: 2.1136667728424072\n",
      "Step 931: val loss: 49.87837600708008\n",
      "Step 932: train loss: 2.1124958992004395\n",
      "Step 932: val loss: 49.899940490722656\n",
      "Step 933: train loss: 2.11132550239563\n",
      "Step 933: val loss: 49.92145919799805\n",
      "Step 934: train loss: 2.110156536102295\n",
      "Step 934: val loss: 49.94289779663086\n",
      "Step 935: train loss: 2.1089885234832764\n",
      "Step 935: val loss: 49.96429443359375\n",
      "Step 936: train loss: 2.1078217029571533\n",
      "Step 936: val loss: 49.98564529418945\n",
      "Step 937: train loss: 2.106656074523926\n",
      "Step 937: val loss: 50.00690841674805\n",
      "Step 938: train loss: 2.1054909229278564\n",
      "Step 938: val loss: 50.02814483642578\n",
      "Step 939: train loss: 2.104327440261841\n",
      "Step 939: val loss: 50.04932403564453\n",
      "Step 940: train loss: 2.1031646728515625\n",
      "Step 940: val loss: 50.070438385009766\n",
      "Step 941: train loss: 2.102003335952759\n",
      "Step 941: val loss: 50.091487884521484\n",
      "Step 942: train loss: 2.1008427143096924\n",
      "Step 942: val loss: 50.11249923706055\n",
      "Step 943: train loss: 2.099684000015259\n",
      "Step 943: val loss: 50.13344955444336\n",
      "Step 944: train loss: 2.0985257625579834\n",
      "Step 944: val loss: 50.15433120727539\n",
      "Step 945: train loss: 2.097368001937866\n",
      "Step 945: val loss: 50.175167083740234\n",
      "Step 946: train loss: 2.0962119102478027\n",
      "Step 946: val loss: 50.19595718383789\n",
      "Step 947: train loss: 2.095057249069214\n",
      "Step 947: val loss: 50.216678619384766\n",
      "Step 948: train loss: 2.093903064727783\n",
      "Step 948: val loss: 50.23734664916992\n",
      "Step 949: train loss: 2.092749834060669\n",
      "Step 949: val loss: 50.25796890258789\n",
      "Step 950: train loss: 2.0915980339050293\n",
      "Step 950: val loss: 50.27851486206055\n",
      "Step 951: train loss: 2.090447187423706\n",
      "Step 951: val loss: 50.29902267456055\n",
      "Step 952: train loss: 2.0892977714538574\n",
      "Step 952: val loss: 50.3194694519043\n",
      "Step 953: train loss: 2.088149309158325\n",
      "Step 953: val loss: 50.33985900878906\n",
      "Step 954: train loss: 2.087001323699951\n",
      "Step 954: val loss: 50.36019515991211\n",
      "Step 955: train loss: 2.0858542919158936\n",
      "Step 955: val loss: 50.380489349365234\n",
      "Step 956: train loss: 2.0847091674804688\n",
      "Step 956: val loss: 50.40070343017578\n",
      "Step 957: train loss: 2.083564519882202\n",
      "Step 957: val loss: 50.420875549316406\n",
      "Step 958: train loss: 2.082421064376831\n",
      "Step 958: val loss: 50.440982818603516\n",
      "Step 959: train loss: 2.0812792778015137\n",
      "Step 959: val loss: 50.461055755615234\n",
      "Step 960: train loss: 2.0801377296447754\n",
      "Step 960: val loss: 50.48104476928711\n",
      "Step 961: train loss: 2.0789971351623535\n",
      "Step 961: val loss: 50.501007080078125\n",
      "Step 962: train loss: 2.0778579711914062\n",
      "Step 962: val loss: 50.520896911621094\n",
      "Step 963: train loss: 2.076719284057617\n",
      "Step 963: val loss: 50.54073715209961\n",
      "Step 964: train loss: 2.0755820274353027\n",
      "Step 964: val loss: 50.560523986816406\n",
      "Step 965: train loss: 2.074445962905884\n",
      "Step 965: val loss: 50.58024978637695\n",
      "Step 966: train loss: 2.073310613632202\n",
      "Step 966: val loss: 50.59992218017578\n",
      "Step 967: train loss: 2.072176456451416\n",
      "Step 967: val loss: 50.619544982910156\n",
      "Step 968: train loss: 2.0710434913635254\n",
      "Step 968: val loss: 50.63910675048828\n",
      "Step 969: train loss: 2.069911479949951\n",
      "Step 969: val loss: 50.65861892700195\n",
      "Step 970: train loss: 2.0687804222106934\n",
      "Step 970: val loss: 50.67806625366211\n",
      "Step 971: train loss: 2.0676512718200684\n",
      "Step 971: val loss: 50.697452545166016\n",
      "Step 972: train loss: 2.0665218830108643\n",
      "Step 972: val loss: 50.7168083190918\n",
      "Step 973: train loss: 2.0653936862945557\n",
      "Step 973: val loss: 50.7360954284668\n",
      "Step 974: train loss: 2.0642666816711426\n",
      "Step 974: val loss: 50.755332946777344\n",
      "Step 975: train loss: 2.063140869140625\n",
      "Step 975: val loss: 50.77450942993164\n",
      "Step 976: train loss: 2.062016487121582\n",
      "Step 976: val loss: 50.79362487792969\n",
      "Step 977: train loss: 2.060892105102539\n",
      "Step 977: val loss: 50.81269836425781\n",
      "Step 978: train loss: 2.05976939201355\n",
      "Step 978: val loss: 50.83172607421875\n",
      "Step 979: train loss: 2.058647394180298\n",
      "Step 979: val loss: 50.85068130493164\n",
      "Step 980: train loss: 2.0575270652770996\n",
      "Step 980: val loss: 50.86958312988281\n",
      "Step 981: train loss: 2.0564069747924805\n",
      "Step 981: val loss: 50.88844299316406\n",
      "Step 982: train loss: 2.055288076400757\n",
      "Step 982: val loss: 50.90724182128906\n",
      "Step 983: train loss: 2.0541701316833496\n",
      "Step 983: val loss: 50.92598342895508\n",
      "Step 984: train loss: 2.053053617477417\n",
      "Step 984: val loss: 50.94466781616211\n",
      "Step 985: train loss: 2.0519378185272217\n",
      "Step 985: val loss: 50.96330642700195\n",
      "Step 986: train loss: 2.050823211669922\n",
      "Step 986: val loss: 50.98188400268555\n",
      "Step 987: train loss: 2.0497093200683594\n",
      "Step 987: val loss: 51.00041198730469\n",
      "Step 988: train loss: 2.0485968589782715\n",
      "Step 988: val loss: 51.018882751464844\n",
      "Step 989: train loss: 2.0474841594696045\n",
      "Step 989: val loss: 51.037315368652344\n",
      "Step 990: train loss: 2.0463736057281494\n",
      "Step 990: val loss: 51.05567169189453\n",
      "Step 991: train loss: 2.0452640056610107\n",
      "Step 991: val loss: 51.0739860534668\n",
      "Step 992: train loss: 2.0441553592681885\n",
      "Step 992: val loss: 51.09224319458008\n",
      "Step 993: train loss: 2.0430469512939453\n",
      "Step 993: val loss: 51.110443115234375\n",
      "Step 994: train loss: 2.0419399738311768\n",
      "Step 994: val loss: 51.12858963012695\n",
      "Step 995: train loss: 2.0408339500427246\n",
      "Step 995: val loss: 51.14668273925781\n",
      "Step 996: train loss: 2.039729118347168\n",
      "Step 996: val loss: 51.16473388671875\n",
      "Step 997: train loss: 2.038625955581665\n",
      "Step 997: val loss: 51.182716369628906\n",
      "Step 998: train loss: 2.037522792816162\n",
      "Step 998: val loss: 51.200645446777344\n",
      "Step 999: train loss: 2.036421060562134\n",
      "Step 999: val loss: 51.21853256225586\n",
      "Step 1000: train loss: 2.0353198051452637\n",
      "Step 1000: val loss: 51.236358642578125\n",
      "Step 1001: train loss: 2.034219741821289\n",
      "Step 1001: val loss: 51.25413131713867\n",
      "Step 1002: train loss: 2.03312087059021\n",
      "Step 1002: val loss: 51.27184295654297\n",
      "Step 1003: train loss: 2.0320231914520264\n",
      "Step 1003: val loss: 51.289520263671875\n",
      "Step 1004: train loss: 2.030925989151001\n",
      "Step 1004: val loss: 51.3071174621582\n",
      "Step 1005: train loss: 2.029829263687134\n",
      "Step 1005: val loss: 51.32469940185547\n",
      "Step 1006: train loss: 2.028733968734741\n",
      "Step 1006: val loss: 51.34218978881836\n",
      "Step 1007: train loss: 2.027639865875244\n",
      "Step 1007: val loss: 51.359649658203125\n",
      "Step 1008: train loss: 2.0265471935272217\n",
      "Step 1008: val loss: 51.37704849243164\n",
      "Step 1009: train loss: 2.025454521179199\n",
      "Step 1009: val loss: 51.39439010620117\n",
      "Step 1010: train loss: 2.0243639945983887\n",
      "Step 1010: val loss: 51.411678314208984\n",
      "Step 1011: train loss: 2.023273229598999\n",
      "Step 1011: val loss: 51.42891311645508\n",
      "Step 1012: train loss: 2.022184133529663\n",
      "Step 1012: val loss: 51.446109771728516\n",
      "Step 1013: train loss: 2.0210952758789062\n",
      "Step 1013: val loss: 51.463233947753906\n",
      "Step 1014: train loss: 2.020007848739624\n",
      "Step 1014: val loss: 51.480323791503906\n",
      "Step 1015: train loss: 2.0189218521118164\n",
      "Step 1015: val loss: 51.497344970703125\n",
      "Step 1016: train loss: 2.017836093902588\n",
      "Step 1016: val loss: 51.514320373535156\n",
      "Step 1017: train loss: 2.016751766204834\n",
      "Step 1017: val loss: 51.53123474121094\n",
      "Step 1018: train loss: 2.0156681537628174\n",
      "Step 1018: val loss: 51.5481071472168\n",
      "Step 1019: train loss: 2.014585494995117\n",
      "Step 1019: val loss: 51.56492614746094\n",
      "Step 1020: train loss: 2.0135040283203125\n",
      "Step 1020: val loss: 51.58168029785156\n",
      "Step 1021: train loss: 2.012422800064087\n",
      "Step 1021: val loss: 51.598392486572266\n",
      "Step 1022: train loss: 2.011343002319336\n",
      "Step 1022: val loss: 51.615055084228516\n",
      "Step 1023: train loss: 2.0102639198303223\n",
      "Step 1023: val loss: 51.631656646728516\n",
      "Step 1024: train loss: 2.009186267852783\n",
      "Step 1024: val loss: 51.64821243286133\n",
      "Step 1025: train loss: 2.0081093311309814\n",
      "Step 1025: val loss: 51.664703369140625\n",
      "Step 1026: train loss: 2.007033109664917\n",
      "Step 1026: val loss: 51.681148529052734\n",
      "Step 1027: train loss: 2.005958080291748\n",
      "Step 1027: val loss: 51.69754409790039\n",
      "Step 1028: train loss: 2.0048842430114746\n",
      "Step 1028: val loss: 51.7138786315918\n",
      "Step 1029: train loss: 2.0038108825683594\n",
      "Step 1029: val loss: 51.73017501831055\n",
      "Step 1030: train loss: 2.0027384757995605\n",
      "Step 1030: val loss: 51.74641036987305\n",
      "Step 1031: train loss: 2.0016674995422363\n",
      "Step 1031: val loss: 51.76259231567383\n",
      "Step 1032: train loss: 2.000596523284912\n",
      "Step 1032: val loss: 51.77873992919922\n",
      "Step 1033: train loss: 1.9995269775390625\n",
      "Step 1033: val loss: 51.79480743408203\n",
      "Step 1034: train loss: 1.9984585046768188\n",
      "Step 1034: val loss: 51.810829162597656\n",
      "Step 1035: train loss: 1.9973911046981812\n",
      "Step 1035: val loss: 51.826812744140625\n",
      "Step 1036: train loss: 1.9963245391845703\n",
      "Step 1036: val loss: 51.84273910522461\n",
      "Step 1037: train loss: 1.9952586889266968\n",
      "Step 1037: val loss: 51.85860061645508\n",
      "Step 1038: train loss: 1.9941937923431396\n",
      "Step 1038: val loss: 51.87440490722656\n",
      "Step 1039: train loss: 1.9931297302246094\n",
      "Step 1039: val loss: 51.89018630981445\n",
      "Step 1040: train loss: 1.9920666217803955\n",
      "Step 1040: val loss: 51.90590286254883\n",
      "Step 1041: train loss: 1.9910047054290771\n",
      "Step 1041: val loss: 51.92156219482422\n",
      "Step 1042: train loss: 1.9899437427520752\n",
      "Step 1042: val loss: 51.93718338012695\n",
      "Step 1043: train loss: 1.9888832569122314\n",
      "Step 1043: val loss: 51.95273971557617\n",
      "Step 1044: train loss: 1.9878238439559937\n",
      "Step 1044: val loss: 51.9682502746582\n",
      "Step 1045: train loss: 1.9867652654647827\n",
      "Step 1045: val loss: 51.983707427978516\n",
      "Step 1046: train loss: 1.9857076406478882\n",
      "Step 1046: val loss: 51.999107360839844\n",
      "Step 1047: train loss: 1.98465096950531\n",
      "Step 1047: val loss: 52.014461517333984\n",
      "Step 1048: train loss: 1.9835952520370483\n",
      "Step 1048: val loss: 52.02976608276367\n",
      "Step 1049: train loss: 1.9825407266616821\n",
      "Step 1049: val loss: 52.04500961303711\n",
      "Step 1050: train loss: 1.9814865589141846\n",
      "Step 1050: val loss: 52.06021499633789\n",
      "Step 1051: train loss: 1.9804338216781616\n",
      "Step 1051: val loss: 52.075347900390625\n",
      "Step 1052: train loss: 1.979381799697876\n",
      "Step 1052: val loss: 52.09046173095703\n",
      "Step 1053: train loss: 1.9783302545547485\n",
      "Step 1053: val loss: 52.10550308227539\n",
      "Step 1054: train loss: 1.9772800207138062\n",
      "Step 1054: val loss: 52.1204833984375\n",
      "Step 1055: train loss: 1.9762309789657593\n",
      "Step 1055: val loss: 52.13542556762695\n",
      "Step 1056: train loss: 1.9751825332641602\n",
      "Step 1056: val loss: 52.15032196044922\n",
      "Step 1057: train loss: 1.9741348028182983\n",
      "Step 1057: val loss: 52.165164947509766\n",
      "Step 1058: train loss: 1.9730885028839111\n",
      "Step 1058: val loss: 52.17994689941406\n",
      "Step 1059: train loss: 1.972042441368103\n",
      "Step 1059: val loss: 52.19468307495117\n",
      "Step 1060: train loss: 1.9709978103637695\n",
      "Step 1060: val loss: 52.20936965942383\n",
      "Step 1061: train loss: 1.9699535369873047\n",
      "Step 1061: val loss: 52.2239990234375\n",
      "Step 1062: train loss: 1.9689104557037354\n",
      "Step 1062: val loss: 52.238590240478516\n",
      "Step 1063: train loss: 1.9678676128387451\n",
      "Step 1063: val loss: 52.25312042236328\n",
      "Step 1064: train loss: 1.9668266773223877\n",
      "Step 1064: val loss: 52.267616271972656\n",
      "Step 1065: train loss: 1.9657866954803467\n",
      "Step 1065: val loss: 52.282039642333984\n",
      "Step 1066: train loss: 1.9647467136383057\n",
      "Step 1066: val loss: 52.296417236328125\n",
      "Step 1067: train loss: 1.9637082815170288\n",
      "Step 1067: val loss: 52.31074142456055\n",
      "Step 1068: train loss: 1.9626703262329102\n",
      "Step 1068: val loss: 52.325035095214844\n",
      "Step 1069: train loss: 1.961633324623108\n",
      "Step 1069: val loss: 52.33926010131836\n",
      "Step 1070: train loss: 1.9605976343154907\n",
      "Step 1070: val loss: 52.353431701660156\n",
      "Step 1071: train loss: 1.959562063217163\n",
      "Step 1071: val loss: 52.36756896972656\n",
      "Step 1072: train loss: 1.9585278034210205\n",
      "Step 1072: val loss: 52.38164138793945\n",
      "Step 1073: train loss: 1.9574943780899048\n",
      "Step 1073: val loss: 52.39566421508789\n",
      "Step 1074: train loss: 1.9564615488052368\n",
      "Step 1074: val loss: 52.40964889526367\n",
      "Step 1075: train loss: 1.9554306268692017\n",
      "Step 1075: val loss: 52.42356491088867\n",
      "Step 1076: train loss: 1.9543994665145874\n",
      "Step 1076: val loss: 52.43745422363281\n",
      "Step 1077: train loss: 1.9533694982528687\n",
      "Step 1077: val loss: 52.45127868652344\n",
      "Step 1078: train loss: 1.9523407220840454\n",
      "Step 1078: val loss: 52.46504592895508\n",
      "Step 1079: train loss: 1.9513123035430908\n",
      "Step 1079: val loss: 52.47876739501953\n",
      "Step 1080: train loss: 1.9502849578857422\n",
      "Step 1080: val loss: 52.492454528808594\n",
      "Step 1081: train loss: 1.949258804321289\n",
      "Step 1081: val loss: 52.50607681274414\n",
      "Step 1082: train loss: 1.9482332468032837\n",
      "Step 1082: val loss: 52.5196418762207\n",
      "Step 1083: train loss: 1.9472081661224365\n",
      "Step 1083: val loss: 52.53318405151367\n",
      "Step 1084: train loss: 1.946184515953064\n",
      "Step 1084: val loss: 52.54666519165039\n",
      "Step 1085: train loss: 1.9451614618301392\n",
      "Step 1085: val loss: 52.56007766723633\n",
      "Step 1086: train loss: 1.9441391229629517\n",
      "Step 1086: val loss: 52.57345962524414\n",
      "Step 1087: train loss: 1.9431174993515015\n",
      "Step 1087: val loss: 52.5867805480957\n",
      "Step 1088: train loss: 1.9420973062515259\n",
      "Step 1088: val loss: 52.600059509277344\n",
      "Step 1089: train loss: 1.9410775899887085\n",
      "Step 1089: val loss: 52.6132926940918\n",
      "Step 1090: train loss: 1.9400594234466553\n",
      "Step 1090: val loss: 52.62646484375\n",
      "Step 1091: train loss: 1.9390417337417603\n",
      "Step 1091: val loss: 52.63959503173828\n",
      "Step 1092: train loss: 1.9380242824554443\n",
      "Step 1092: val loss: 52.65267562866211\n",
      "Step 1093: train loss: 1.9370086193084717\n",
      "Step 1093: val loss: 52.66570281982422\n",
      "Step 1094: train loss: 1.935992956161499\n",
      "Step 1094: val loss: 52.678680419921875\n",
      "Step 1095: train loss: 1.9349782466888428\n",
      "Step 1095: val loss: 52.691612243652344\n",
      "Step 1096: train loss: 1.9339649677276611\n",
      "Step 1096: val loss: 52.70448303222656\n",
      "Step 1097: train loss: 1.9329519271850586\n",
      "Step 1097: val loss: 52.717323303222656\n",
      "Step 1098: train loss: 1.9319400787353516\n",
      "Step 1098: val loss: 52.73009490966797\n",
      "Step 1099: train loss: 1.9309295415878296\n",
      "Step 1099: val loss: 52.742828369140625\n",
      "Step 1100: train loss: 1.9299192428588867\n",
      "Step 1100: val loss: 52.755516052246094\n",
      "Step 1101: train loss: 1.9289090633392334\n",
      "Step 1101: val loss: 52.76815414428711\n",
      "Step 1102: train loss: 1.9279006719589233\n",
      "Step 1102: val loss: 52.780738830566406\n",
      "Step 1103: train loss: 1.9268929958343506\n",
      "Step 1103: val loss: 52.79327392578125\n",
      "Step 1104: train loss: 1.9258863925933838\n",
      "Step 1104: val loss: 52.80576705932617\n",
      "Step 1105: train loss: 1.9248802661895752\n",
      "Step 1105: val loss: 52.81821060180664\n",
      "Step 1106: train loss: 1.923874855041504\n",
      "Step 1106: val loss: 52.83059310913086\n",
      "Step 1107: train loss: 1.922870397567749\n",
      "Step 1107: val loss: 52.84293746948242\n",
      "Step 1108: train loss: 1.9218670129776\n",
      "Step 1108: val loss: 52.8552360534668\n",
      "Step 1109: train loss: 1.9208641052246094\n",
      "Step 1109: val loss: 52.86747741699219\n",
      "Step 1110: train loss: 1.919862985610962\n",
      "Step 1110: val loss: 52.87965774536133\n",
      "Step 1111: train loss: 1.9188613891601562\n",
      "Step 1111: val loss: 52.89181137084961\n",
      "Step 1112: train loss: 1.917860984802246\n",
      "Step 1112: val loss: 52.90391540527344\n",
      "Step 1113: train loss: 1.916861891746521\n",
      "Step 1113: val loss: 52.91596221923828\n",
      "Step 1114: train loss: 1.915863037109375\n",
      "Step 1114: val loss: 52.92795944213867\n",
      "Step 1115: train loss: 1.914865255355835\n",
      "Step 1115: val loss: 52.93991470336914\n",
      "Step 1116: train loss: 1.9138683080673218\n",
      "Step 1116: val loss: 52.951812744140625\n",
      "Step 1117: train loss: 1.9128719568252563\n",
      "Step 1117: val loss: 52.96367645263672\n",
      "Step 1118: train loss: 1.9118766784667969\n",
      "Step 1118: val loss: 52.97547912597656\n",
      "Step 1119: train loss: 1.9108823537826538\n",
      "Step 1119: val loss: 52.987239837646484\n",
      "Step 1120: train loss: 1.9098880290985107\n",
      "Step 1120: val loss: 52.99895095825195\n",
      "Step 1121: train loss: 1.90889573097229\n",
      "Step 1121: val loss: 53.01060485839844\n",
      "Step 1122: train loss: 1.9079039096832275\n",
      "Step 1122: val loss: 53.02223205566406\n",
      "Step 1123: train loss: 1.9069130420684814\n",
      "Step 1123: val loss: 53.03378677368164\n",
      "Step 1124: train loss: 1.905922293663025\n",
      "Step 1124: val loss: 53.04530715942383\n",
      "Step 1125: train loss: 1.9049326181411743\n",
      "Step 1125: val loss: 53.05678939819336\n",
      "Step 1126: train loss: 1.9039437770843506\n",
      "Step 1126: val loss: 53.068214416503906\n",
      "Step 1127: train loss: 1.9029563665390015\n",
      "Step 1127: val loss: 53.07957458496094\n",
      "Step 1128: train loss: 1.9019685983657837\n",
      "Step 1128: val loss: 53.09091567993164\n",
      "Step 1129: train loss: 1.9009822607040405\n",
      "Step 1129: val loss: 53.10218811035156\n",
      "Step 1130: train loss: 1.8999967575073242\n",
      "Step 1130: val loss: 53.11341857910156\n",
      "Step 1131: train loss: 1.8990119695663452\n",
      "Step 1131: val loss: 53.124610900878906\n",
      "Step 1132: train loss: 1.898028016090393\n",
      "Step 1132: val loss: 53.13574981689453\n",
      "Step 1133: train loss: 1.8970448970794678\n",
      "Step 1133: val loss: 53.1468391418457\n",
      "Step 1134: train loss: 1.8960624933242798\n",
      "Step 1134: val loss: 53.157875061035156\n",
      "Step 1135: train loss: 1.8950809240341187\n",
      "Step 1135: val loss: 53.16887664794922\n",
      "Step 1136: train loss: 1.8941001892089844\n",
      "Step 1136: val loss: 53.17981719970703\n",
      "Step 1137: train loss: 1.8931201696395874\n",
      "Step 1137: val loss: 53.19071578979492\n",
      "Step 1138: train loss: 1.892141580581665\n",
      "Step 1138: val loss: 53.2015495300293\n",
      "Step 1139: train loss: 1.8911625146865845\n",
      "Step 1139: val loss: 53.21238327026367\n",
      "Step 1140: train loss: 1.8901851177215576\n",
      "Step 1140: val loss: 53.22313690185547\n",
      "Step 1141: train loss: 1.8892087936401367\n",
      "Step 1141: val loss: 53.23384475708008\n",
      "Step 1142: train loss: 1.8882328271865845\n",
      "Step 1142: val loss: 53.24449920654297\n",
      "Step 1143: train loss: 1.8872573375701904\n",
      "Step 1143: val loss: 53.255123138427734\n",
      "Step 1144: train loss: 1.8862829208374023\n",
      "Step 1144: val loss: 53.265689849853516\n",
      "Step 1145: train loss: 1.8853098154067993\n",
      "Step 1145: val loss: 53.276214599609375\n",
      "Step 1146: train loss: 1.8843368291854858\n",
      "Step 1146: val loss: 53.286685943603516\n",
      "Step 1147: train loss: 1.8833644390106201\n",
      "Step 1147: val loss: 53.297122955322266\n",
      "Step 1148: train loss: 1.8823931217193604\n",
      "Step 1148: val loss: 53.30750274658203\n",
      "Step 1149: train loss: 1.8814226388931274\n",
      "Step 1149: val loss: 53.31784439086914\n",
      "Step 1150: train loss: 1.8804534673690796\n",
      "Step 1150: val loss: 53.328125\n",
      "Step 1151: train loss: 1.879483938217163\n",
      "Step 1151: val loss: 53.3383674621582\n",
      "Step 1152: train loss: 1.8785159587860107\n",
      "Step 1152: val loss: 53.34856414794922\n",
      "Step 1153: train loss: 1.8775484561920166\n",
      "Step 1153: val loss: 53.35872268676758\n",
      "Step 1154: train loss: 1.876582145690918\n",
      "Step 1154: val loss: 53.368804931640625\n",
      "Step 1155: train loss: 1.8756160736083984\n",
      "Step 1155: val loss: 53.37887954711914\n",
      "Step 1156: train loss: 1.8746511936187744\n",
      "Step 1156: val loss: 53.388885498046875\n",
      "Step 1157: train loss: 1.8736870288848877\n",
      "Step 1157: val loss: 53.398841857910156\n",
      "Step 1158: train loss: 1.8727233409881592\n",
      "Step 1158: val loss: 53.40876388549805\n",
      "Step 1159: train loss: 1.8717607259750366\n",
      "Step 1159: val loss: 53.41863250732422\n",
      "Step 1160: train loss: 1.8707990646362305\n",
      "Step 1160: val loss: 53.4284553527832\n",
      "Step 1161: train loss: 1.8698375225067139\n",
      "Step 1161: val loss: 53.4382438659668\n",
      "Step 1162: train loss: 1.8688777685165405\n",
      "Step 1162: val loss: 53.44796371459961\n",
      "Step 1163: train loss: 1.8679182529449463\n",
      "Step 1163: val loss: 53.4576530456543\n",
      "Step 1164: train loss: 1.8669586181640625\n",
      "Step 1164: val loss: 53.46729278564453\n",
      "Step 1165: train loss: 1.866000771522522\n",
      "Step 1165: val loss: 53.476890563964844\n",
      "Step 1166: train loss: 1.8650434017181396\n",
      "Step 1166: val loss: 53.48644256591797\n",
      "Step 1167: train loss: 1.8640871047973633\n",
      "Step 1167: val loss: 53.49593734741211\n",
      "Step 1168: train loss: 1.8631317615509033\n",
      "Step 1168: val loss: 53.50539779663086\n",
      "Step 1169: train loss: 1.862176537513733\n",
      "Step 1169: val loss: 53.514808654785156\n",
      "Step 1170: train loss: 1.8612220287322998\n",
      "Step 1170: val loss: 53.52417755126953\n",
      "Step 1171: train loss: 1.8602683544158936\n",
      "Step 1171: val loss: 53.53349685668945\n",
      "Step 1172: train loss: 1.859316110610962\n",
      "Step 1172: val loss: 53.54277420043945\n",
      "Step 1173: train loss: 1.8583637475967407\n",
      "Step 1173: val loss: 53.551998138427734\n",
      "Step 1174: train loss: 1.8574126958847046\n",
      "Step 1174: val loss: 53.561180114746094\n",
      "Step 1175: train loss: 1.8564622402191162\n",
      "Step 1175: val loss: 53.5703125\n",
      "Step 1176: train loss: 1.855512261390686\n",
      "Step 1176: val loss: 53.57941818237305\n",
      "Step 1177: train loss: 1.8545639514923096\n",
      "Step 1177: val loss: 53.58845520019531\n",
      "Step 1178: train loss: 1.853615403175354\n",
      "Step 1178: val loss: 53.59745407104492\n",
      "Step 1179: train loss: 1.8526678085327148\n",
      "Step 1179: val loss: 53.60641860961914\n",
      "Step 1180: train loss: 1.8517210483551025\n",
      "Step 1180: val loss: 53.61532974243164\n",
      "Step 1181: train loss: 1.850775122642517\n",
      "Step 1181: val loss: 53.62419509887695\n",
      "Step 1182: train loss: 1.8498300313949585\n",
      "Step 1182: val loss: 53.63300704956055\n",
      "Step 1183: train loss: 1.8488857746124268\n",
      "Step 1183: val loss: 53.641788482666016\n",
      "Step 1184: train loss: 1.8479419946670532\n",
      "Step 1184: val loss: 53.65053176879883\n",
      "Step 1185: train loss: 1.8469985723495483\n",
      "Step 1185: val loss: 53.65920639038086\n",
      "Step 1186: train loss: 1.846056580543518\n",
      "Step 1186: val loss: 53.6678466796875\n",
      "Step 1187: train loss: 1.8451149463653564\n",
      "Step 1187: val loss: 53.67644500732422\n",
      "Step 1188: train loss: 1.8441743850708008\n",
      "Step 1188: val loss: 53.68498992919922\n",
      "Step 1189: train loss: 1.843234658241272\n",
      "Step 1189: val loss: 53.69349670410156\n",
      "Step 1190: train loss: 1.8422954082489014\n",
      "Step 1190: val loss: 53.70196533203125\n",
      "Step 1191: train loss: 1.841356635093689\n",
      "Step 1191: val loss: 53.71037673950195\n",
      "Step 1192: train loss: 1.8404191732406616\n",
      "Step 1192: val loss: 53.718746185302734\n",
      "Step 1193: train loss: 1.8394817113876343\n",
      "Step 1193: val loss: 53.72708511352539\n",
      "Step 1194: train loss: 1.838545560836792\n",
      "Step 1194: val loss: 53.73537063598633\n",
      "Step 1195: train loss: 1.8376096487045288\n",
      "Step 1195: val loss: 53.74360656738281\n",
      "Step 1196: train loss: 1.8366752862930298\n",
      "Step 1196: val loss: 53.751800537109375\n",
      "Step 1197: train loss: 1.8357406854629517\n",
      "Step 1197: val loss: 53.759952545166016\n",
      "Step 1198: train loss: 1.8348079919815063\n",
      "Step 1198: val loss: 53.76805877685547\n",
      "Step 1199: train loss: 1.833875060081482\n",
      "Step 1199: val loss: 53.77611541748047\n",
      "Step 1200: train loss: 1.8329432010650635\n",
      "Step 1200: val loss: 53.78413391113281\n",
      "Step 1201: train loss: 1.8320121765136719\n",
      "Step 1201: val loss: 53.7921028137207\n",
      "Step 1202: train loss: 1.8310816287994385\n",
      "Step 1202: val loss: 53.800045013427734\n",
      "Step 1203: train loss: 1.8301517963409424\n",
      "Step 1203: val loss: 53.80792236328125\n",
      "Step 1204: train loss: 1.8292226791381836\n",
      "Step 1204: val loss: 53.81576156616211\n",
      "Step 1205: train loss: 1.8282945156097412\n",
      "Step 1205: val loss: 53.82356262207031\n",
      "Step 1206: train loss: 1.827366590499878\n",
      "Step 1206: val loss: 53.83132553100586\n",
      "Step 1207: train loss: 1.8264405727386475\n",
      "Step 1207: val loss: 53.839027404785156\n",
      "Step 1208: train loss: 1.8255141973495483\n",
      "Step 1208: val loss: 53.8466911315918\n",
      "Step 1209: train loss: 1.8245885372161865\n",
      "Step 1209: val loss: 53.85431671142578\n",
      "Step 1210: train loss: 1.8236640691757202\n",
      "Step 1210: val loss: 53.86188507080078\n",
      "Step 1211: train loss: 1.8227401971817017\n",
      "Step 1211: val loss: 53.86943435668945\n",
      "Step 1212: train loss: 1.8218170404434204\n",
      "Step 1212: val loss: 53.87692642211914\n",
      "Step 1213: train loss: 1.8208945989608765\n",
      "Step 1213: val loss: 53.88436508178711\n",
      "Step 1214: train loss: 1.8199723958969116\n",
      "Step 1214: val loss: 53.891780853271484\n",
      "Step 1215: train loss: 1.8190515041351318\n",
      "Step 1215: val loss: 53.899131774902344\n",
      "Step 1216: train loss: 1.8181310892105103\n",
      "Step 1216: val loss: 53.90644836425781\n",
      "Step 1217: train loss: 1.8172115087509155\n",
      "Step 1217: val loss: 53.913719177246094\n",
      "Step 1218: train loss: 1.8162927627563477\n",
      "Step 1218: val loss: 53.92094802856445\n",
      "Step 1219: train loss: 1.8153737783432007\n",
      "Step 1219: val loss: 53.92815017700195\n",
      "Step 1220: train loss: 1.8144567012786865\n",
      "Step 1220: val loss: 53.935279846191406\n",
      "Step 1221: train loss: 1.813539743423462\n",
      "Step 1221: val loss: 53.9423828125\n",
      "Step 1222: train loss: 1.8126236200332642\n",
      "Step 1222: val loss: 53.94944381713867\n",
      "Step 1223: train loss: 1.8117090463638306\n",
      "Step 1223: val loss: 53.956459045410156\n",
      "Step 1224: train loss: 1.8107938766479492\n",
      "Step 1224: val loss: 53.96343994140625\n",
      "Step 1225: train loss: 1.809880018234253\n",
      "Step 1225: val loss: 53.9703483581543\n",
      "Step 1226: train loss: 1.808966875076294\n",
      "Step 1226: val loss: 53.97724914550781\n",
      "Step 1227: train loss: 1.8080542087554932\n",
      "Step 1227: val loss: 53.984092712402344\n",
      "Step 1228: train loss: 1.807141900062561\n",
      "Step 1228: val loss: 53.99089431762695\n",
      "Step 1229: train loss: 1.8062312602996826\n",
      "Step 1229: val loss: 53.997642517089844\n",
      "Step 1230: train loss: 1.8053207397460938\n",
      "Step 1230: val loss: 54.004371643066406\n",
      "Step 1231: train loss: 1.8044111728668213\n",
      "Step 1231: val loss: 54.01103210449219\n",
      "Step 1232: train loss: 1.8035014867782593\n",
      "Step 1232: val loss: 54.017669677734375\n",
      "Step 1233: train loss: 1.802593469619751\n",
      "Step 1233: val loss: 54.024253845214844\n",
      "Step 1234: train loss: 1.8016856908798218\n",
      "Step 1234: val loss: 54.030792236328125\n",
      "Step 1235: train loss: 1.8007789850234985\n",
      "Step 1235: val loss: 54.03730010986328\n",
      "Step 1236: train loss: 1.7998723983764648\n",
      "Step 1236: val loss: 54.043766021728516\n",
      "Step 1237: train loss: 1.7989671230316162\n",
      "Step 1237: val loss: 54.05017852783203\n",
      "Step 1238: train loss: 1.7980619668960571\n",
      "Step 1238: val loss: 54.056549072265625\n",
      "Step 1239: train loss: 1.7971584796905518\n",
      "Step 1239: val loss: 54.06288146972656\n",
      "Step 1240: train loss: 1.7962545156478882\n",
      "Step 1240: val loss: 54.069175720214844\n",
      "Step 1241: train loss: 1.7953516244888306\n",
      "Step 1241: val loss: 54.07542419433594\n",
      "Step 1242: train loss: 1.7944495677947998\n",
      "Step 1242: val loss: 54.081634521484375\n",
      "Step 1243: train loss: 1.7935479879379272\n",
      "Step 1243: val loss: 54.08780288696289\n",
      "Step 1244: train loss: 1.7926467657089233\n",
      "Step 1244: val loss: 54.09391784667969\n",
      "Step 1245: train loss: 1.7917473316192627\n",
      "Step 1245: val loss: 54.0999870300293\n",
      "Step 1246: train loss: 1.7908477783203125\n",
      "Step 1246: val loss: 54.10604476928711\n",
      "Step 1247: train loss: 1.7899490594863892\n",
      "Step 1247: val loss: 54.11204147338867\n",
      "Step 1248: train loss: 1.7890506982803345\n",
      "Step 1248: val loss: 54.11799621582031\n",
      "Step 1249: train loss: 1.788153886795044\n",
      "Step 1249: val loss: 54.12390899658203\n",
      "Step 1250: train loss: 1.7872568368911743\n",
      "Step 1250: val loss: 54.12978744506836\n",
      "Step 1251: train loss: 1.7863609790802002\n",
      "Step 1251: val loss: 54.135616302490234\n",
      "Step 1252: train loss: 1.7854655981063843\n",
      "Step 1252: val loss: 54.14139938354492\n",
      "Step 1253: train loss: 1.7845714092254639\n",
      "Step 1253: val loss: 54.14714431762695\n",
      "Step 1254: train loss: 1.7836774587631226\n",
      "Step 1254: val loss: 54.15284729003906\n",
      "Step 1255: train loss: 1.78278386592865\n",
      "Step 1255: val loss: 54.158512115478516\n",
      "Step 1256: train loss: 1.7818915843963623\n",
      "Step 1256: val loss: 54.16413116455078\n",
      "Step 1257: train loss: 1.7809995412826538\n",
      "Step 1257: val loss: 54.169715881347656\n",
      "Step 1258: train loss: 1.7801082134246826\n",
      "Step 1258: val loss: 54.175254821777344\n",
      "Step 1259: train loss: 1.7792179584503174\n",
      "Step 1259: val loss: 54.180755615234375\n",
      "Step 1260: train loss: 1.7783277034759521\n",
      "Step 1260: val loss: 54.18620681762695\n",
      "Step 1261: train loss: 1.777438759803772\n",
      "Step 1261: val loss: 54.19162368774414\n",
      "Step 1262: train loss: 1.77655029296875\n",
      "Step 1262: val loss: 54.19700622558594\n",
      "Step 1263: train loss: 1.7756621837615967\n",
      "Step 1263: val loss: 54.202335357666016\n",
      "Step 1264: train loss: 1.7747751474380493\n",
      "Step 1264: val loss: 54.2076301574707\n",
      "Step 1265: train loss: 1.7738885879516602\n",
      "Step 1265: val loss: 54.21288299560547\n",
      "Step 1266: train loss: 1.7730028629302979\n",
      "Step 1266: val loss: 54.21809768676758\n",
      "Step 1267: train loss: 1.7721178531646729\n",
      "Step 1267: val loss: 54.22325897216797\n",
      "Step 1268: train loss: 1.7712329626083374\n",
      "Step 1268: val loss: 54.228389739990234\n",
      "Step 1269: train loss: 1.7703492641448975\n",
      "Step 1269: val loss: 54.23347473144531\n",
      "Step 1270: train loss: 1.7694660425186157\n",
      "Step 1270: val loss: 54.23853302001953\n",
      "Step 1271: train loss: 1.7685836553573608\n",
      "Step 1271: val loss: 54.24353790283203\n",
      "Step 1272: train loss: 1.7677018642425537\n",
      "Step 1272: val loss: 54.248497009277344\n",
      "Step 1273: train loss: 1.7668203115463257\n",
      "Step 1273: val loss: 54.25343322753906\n",
      "Step 1274: train loss: 1.7659403085708618\n",
      "Step 1274: val loss: 54.25830841064453\n",
      "Step 1275: train loss: 1.7650604248046875\n",
      "Step 1275: val loss: 54.26316452026367\n",
      "Step 1276: train loss: 1.7641805410385132\n",
      "Step 1276: val loss: 54.267967224121094\n",
      "Step 1277: train loss: 1.763302206993103\n",
      "Step 1277: val loss: 54.27273178100586\n",
      "Step 1278: train loss: 1.7624247074127197\n",
      "Step 1278: val loss: 54.277462005615234\n",
      "Step 1279: train loss: 1.7615470886230469\n",
      "Step 1279: val loss: 54.28215408325195\n",
      "Step 1280: train loss: 1.7606704235076904\n",
      "Step 1280: val loss: 54.28679656982422\n",
      "Step 1281: train loss: 1.759793996810913\n",
      "Step 1281: val loss: 54.29139709472656\n",
      "Step 1282: train loss: 1.7589187622070312\n",
      "Step 1282: val loss: 54.29595947265625\n",
      "Step 1283: train loss: 1.7580444812774658\n",
      "Step 1283: val loss: 54.30048370361328\n",
      "Step 1284: train loss: 1.7571704387664795\n",
      "Step 1284: val loss: 54.304969787597656\n",
      "Step 1285: train loss: 1.7562968730926514\n",
      "Step 1285: val loss: 54.30942153930664\n",
      "Step 1286: train loss: 1.7554242610931396\n",
      "Step 1286: val loss: 54.31382369995117\n",
      "Step 1287: train loss: 1.7545526027679443\n",
      "Step 1287: val loss: 54.31818389892578\n",
      "Step 1288: train loss: 1.7536811828613281\n",
      "Step 1288: val loss: 54.322505950927734\n",
      "Step 1289: train loss: 1.7528101205825806\n",
      "Step 1289: val loss: 54.3267936706543\n",
      "Step 1290: train loss: 1.751940369606018\n",
      "Step 1290: val loss: 54.33104705810547\n",
      "Step 1291: train loss: 1.7510709762573242\n",
      "Step 1291: val loss: 54.33525466918945\n",
      "Step 1292: train loss: 1.7502024173736572\n",
      "Step 1292: val loss: 54.33940505981445\n",
      "Step 1293: train loss: 1.7493339776992798\n",
      "Step 1293: val loss: 54.343536376953125\n",
      "Step 1294: train loss: 1.7484662532806396\n",
      "Step 1294: val loss: 54.347633361816406\n",
      "Step 1295: train loss: 1.7475990056991577\n",
      "Step 1295: val loss: 54.351680755615234\n",
      "Step 1296: train loss: 1.7467329502105713\n",
      "Step 1296: val loss: 54.35568618774414\n",
      "Step 1297: train loss: 1.745867371559143\n",
      "Step 1297: val loss: 54.359649658203125\n",
      "Step 1298: train loss: 1.7450027465820312\n",
      "Step 1298: val loss: 54.36358642578125\n",
      "Step 1299: train loss: 1.7441383600234985\n",
      "Step 1299: val loss: 54.367462158203125\n",
      "Step 1300: train loss: 1.7432750463485718\n",
      "Step 1300: val loss: 54.37131118774414\n",
      "Step 1301: train loss: 1.7424116134643555\n",
      "Step 1301: val loss: 54.375125885009766\n",
      "Step 1302: train loss: 1.7415494918823242\n",
      "Step 1302: val loss: 54.3788948059082\n",
      "Step 1303: train loss: 1.7406879663467407\n",
      "Step 1303: val loss: 54.382633209228516\n",
      "Step 1304: train loss: 1.7398265600204468\n",
      "Step 1304: val loss: 54.38632583618164\n",
      "Step 1305: train loss: 1.7389659881591797\n",
      "Step 1305: val loss: 54.389984130859375\n",
      "Step 1306: train loss: 1.7381060123443604\n",
      "Step 1306: val loss: 54.39360427856445\n",
      "Step 1307: train loss: 1.7372468709945679\n",
      "Step 1307: val loss: 54.39717483520508\n",
      "Step 1308: train loss: 1.7363885641098022\n",
      "Step 1308: val loss: 54.40071105957031\n",
      "Step 1309: train loss: 1.7355304956436157\n",
      "Step 1309: val loss: 54.40422058105469\n",
      "Step 1310: train loss: 1.7346731424331665\n",
      "Step 1310: val loss: 54.407676696777344\n",
      "Step 1311: train loss: 1.733816385269165\n",
      "Step 1311: val loss: 54.411102294921875\n",
      "Step 1312: train loss: 1.7329601049423218\n",
      "Step 1312: val loss: 54.41448974609375\n",
      "Step 1313: train loss: 1.7321048974990845\n",
      "Step 1313: val loss: 54.41783905029297\n",
      "Step 1314: train loss: 1.7312504053115845\n",
      "Step 1314: val loss: 54.421138763427734\n",
      "Step 1315: train loss: 1.730396032333374\n",
      "Step 1315: val loss: 54.42441177368164\n",
      "Step 1316: train loss: 1.7295421361923218\n",
      "Step 1316: val loss: 54.427642822265625\n",
      "Step 1317: train loss: 1.728689193725586\n",
      "Step 1317: val loss: 54.43083953857422\n",
      "Step 1318: train loss: 1.7278369665145874\n",
      "Step 1318: val loss: 54.433998107910156\n",
      "Step 1319: train loss: 1.7269856929779053\n",
      "Step 1319: val loss: 54.437103271484375\n",
      "Step 1320: train loss: 1.7261345386505127\n",
      "Step 1320: val loss: 54.440181732177734\n",
      "Step 1321: train loss: 1.7252838611602783\n",
      "Step 1321: val loss: 54.44321823120117\n",
      "Step 1322: train loss: 1.7244338989257812\n",
      "Step 1322: val loss: 54.44622802734375\n",
      "Step 1323: train loss: 1.723584532737732\n",
      "Step 1323: val loss: 54.449180603027344\n",
      "Step 1324: train loss: 1.7227363586425781\n",
      "Step 1324: val loss: 54.45210266113281\n",
      "Step 1325: train loss: 1.721888780593872\n",
      "Step 1325: val loss: 54.454994201660156\n",
      "Step 1326: train loss: 1.7210410833358765\n",
      "Step 1326: val loss: 54.457847595214844\n",
      "Step 1327: train loss: 1.7201942205429077\n",
      "Step 1327: val loss: 54.46064376831055\n",
      "Step 1328: train loss: 1.7193481922149658\n",
      "Step 1328: val loss: 54.46342468261719\n",
      "Step 1329: train loss: 1.7185032367706299\n",
      "Step 1329: val loss: 54.466163635253906\n",
      "Step 1330: train loss: 1.7176576852798462\n",
      "Step 1330: val loss: 54.46885681152344\n",
      "Step 1331: train loss: 1.7168134450912476\n",
      "Step 1331: val loss: 54.47152328491211\n",
      "Step 1332: train loss: 1.7159696817398071\n",
      "Step 1332: val loss: 54.474143981933594\n",
      "Step 1333: train loss: 1.7151273488998413\n",
      "Step 1333: val loss: 54.47672653198242\n",
      "Step 1334: train loss: 1.7142845392227173\n",
      "Step 1334: val loss: 54.479278564453125\n",
      "Step 1335: train loss: 1.713443398475647\n",
      "Step 1335: val loss: 54.48176956176758\n",
      "Step 1336: train loss: 1.712601900100708\n",
      "Step 1336: val loss: 54.4842529296875\n",
      "Step 1337: train loss: 1.7117611169815063\n",
      "Step 1337: val loss: 54.48668670654297\n",
      "Step 1338: train loss: 1.710921287536621\n",
      "Step 1338: val loss: 54.489078521728516\n",
      "Step 1339: train loss: 1.7100818157196045\n",
      "Step 1339: val loss: 54.4914436340332\n",
      "Step 1340: train loss: 1.709242820739746\n",
      "Step 1340: val loss: 54.4937744140625\n",
      "Step 1341: train loss: 1.7084048986434937\n",
      "Step 1341: val loss: 54.49605178833008\n",
      "Step 1342: train loss: 1.7075669765472412\n",
      "Step 1342: val loss: 54.49829864501953\n",
      "Step 1343: train loss: 1.7067304849624634\n",
      "Step 1343: val loss: 54.50051498413086\n",
      "Step 1344: train loss: 1.705893874168396\n",
      "Step 1344: val loss: 54.5026969909668\n",
      "Step 1345: train loss: 1.7050583362579346\n",
      "Step 1345: val loss: 54.50482940673828\n",
      "Step 1346: train loss: 1.7042232751846313\n",
      "Step 1346: val loss: 54.506919860839844\n",
      "Step 1347: train loss: 1.7033882141113281\n",
      "Step 1347: val loss: 54.508995056152344\n",
      "Step 1348: train loss: 1.70255446434021\n",
      "Step 1348: val loss: 54.511016845703125\n",
      "Step 1349: train loss: 1.701721429824829\n",
      "Step 1349: val loss: 54.512996673583984\n",
      "Step 1350: train loss: 1.7008883953094482\n",
      "Step 1350: val loss: 54.51496124267578\n",
      "Step 1351: train loss: 1.7000566720962524\n",
      "Step 1351: val loss: 54.516883850097656\n",
      "Step 1352: train loss: 1.6992242336273193\n",
      "Step 1352: val loss: 54.51876449584961\n",
      "Step 1353: train loss: 1.6983938217163086\n",
      "Step 1353: val loss: 54.52060317993164\n",
      "Step 1354: train loss: 1.6975631713867188\n",
      "Step 1354: val loss: 54.52241897583008\n",
      "Step 1355: train loss: 1.6967337131500244\n",
      "Step 1355: val loss: 54.5241813659668\n",
      "Step 1356: train loss: 1.6959047317504883\n",
      "Step 1356: val loss: 54.525909423828125\n",
      "Step 1357: train loss: 1.6950763463974\n",
      "Step 1357: val loss: 54.52761459350586\n",
      "Step 1358: train loss: 1.694247841835022\n",
      "Step 1358: val loss: 54.52927780151367\n",
      "Step 1359: train loss: 1.693420648574829\n",
      "Step 1359: val loss: 54.53090286254883\n",
      "Step 1360: train loss: 1.6925938129425049\n",
      "Step 1360: val loss: 54.53248977661133\n",
      "Step 1361: train loss: 1.691767930984497\n",
      "Step 1361: val loss: 54.534034729003906\n",
      "Step 1362: train loss: 1.6909421682357788\n",
      "Step 1362: val loss: 54.53556823730469\n",
      "Step 1363: train loss: 1.6901171207427979\n",
      "Step 1363: val loss: 54.53704071044922\n",
      "Step 1364: train loss: 1.6892926692962646\n",
      "Step 1364: val loss: 54.538482666015625\n",
      "Step 1365: train loss: 1.6884691715240479\n",
      "Step 1365: val loss: 54.53989028930664\n",
      "Step 1366: train loss: 1.6876453161239624\n",
      "Step 1366: val loss: 54.54126739501953\n",
      "Step 1367: train loss: 1.686822533607483\n",
      "Step 1367: val loss: 54.54261016845703\n",
      "Step 1368: train loss: 1.6860005855560303\n",
      "Step 1368: val loss: 54.543907165527344\n",
      "Step 1369: train loss: 1.6851794719696045\n",
      "Step 1369: val loss: 54.5451774597168\n",
      "Step 1370: train loss: 1.6843593120574951\n",
      "Step 1370: val loss: 54.546390533447266\n",
      "Step 1371: train loss: 1.6835381984710693\n",
      "Step 1371: val loss: 54.547584533691406\n",
      "Step 1372: train loss: 1.6827179193496704\n",
      "Step 1372: val loss: 54.548744201660156\n",
      "Step 1373: train loss: 1.6818996667861938\n",
      "Step 1373: val loss: 54.54985427856445\n",
      "Step 1374: train loss: 1.6810808181762695\n",
      "Step 1374: val loss: 54.550960540771484\n",
      "Step 1375: train loss: 1.6802623271942139\n",
      "Step 1375: val loss: 54.5520133972168\n",
      "Step 1376: train loss: 1.6794450283050537\n",
      "Step 1376: val loss: 54.55302047729492\n",
      "Step 1377: train loss: 1.6786285638809204\n",
      "Step 1377: val loss: 54.55399703979492\n",
      "Step 1378: train loss: 1.6778122186660767\n",
      "Step 1378: val loss: 54.55495071411133\n",
      "Step 1379: train loss: 1.6769965887069702\n",
      "Step 1379: val loss: 54.55583953857422\n",
      "Step 1380: train loss: 1.676181435585022\n",
      "Step 1380: val loss: 54.55672836303711\n",
      "Step 1381: train loss: 1.675366759300232\n",
      "Step 1381: val loss: 54.55757522583008\n",
      "Step 1382: train loss: 1.6745526790618896\n",
      "Step 1382: val loss: 54.55836868286133\n",
      "Step 1383: train loss: 1.6737396717071533\n",
      "Step 1383: val loss: 54.55913543701172\n",
      "Step 1384: train loss: 1.672926664352417\n",
      "Step 1384: val loss: 54.55986404418945\n",
      "Step 1385: train loss: 1.672114372253418\n",
      "Step 1385: val loss: 54.56056213378906\n",
      "Step 1386: train loss: 1.6713027954101562\n",
      "Step 1386: val loss: 54.56122589111328\n",
      "Step 1387: train loss: 1.670492172241211\n",
      "Step 1387: val loss: 54.56186294555664\n",
      "Step 1388: train loss: 1.6696809530258179\n",
      "Step 1388: val loss: 54.56245422363281\n",
      "Step 1389: train loss: 1.668871521949768\n",
      "Step 1389: val loss: 54.56300735473633\n",
      "Step 1390: train loss: 1.668062448501587\n",
      "Step 1390: val loss: 54.563533782958984\n",
      "Step 1391: train loss: 1.6672531366348267\n",
      "Step 1391: val loss: 54.56401824951172\n",
      "Step 1392: train loss: 1.6664453744888306\n",
      "Step 1392: val loss: 54.56446838378906\n",
      "Step 1393: train loss: 1.6656373739242554\n",
      "Step 1393: val loss: 54.56490707397461\n",
      "Step 1394: train loss: 1.6648305654525757\n",
      "Step 1394: val loss: 54.56528091430664\n",
      "Step 1395: train loss: 1.664023756980896\n",
      "Step 1395: val loss: 54.56563949584961\n",
      "Step 1396: train loss: 1.663217544555664\n",
      "Step 1396: val loss: 54.565956115722656\n",
      "Step 1397: train loss: 1.6624126434326172\n",
      "Step 1397: val loss: 54.56623458862305\n",
      "Step 1398: train loss: 1.6616077423095703\n",
      "Step 1398: val loss: 54.56647491455078\n",
      "Step 1399: train loss: 1.6608034372329712\n",
      "Step 1399: val loss: 54.56669235229492\n",
      "Step 1400: train loss: 1.6599996089935303\n",
      "Step 1400: val loss: 54.56686782836914\n",
      "Step 1401: train loss: 1.6591966152191162\n",
      "Step 1401: val loss: 54.5670166015625\n",
      "Step 1402: train loss: 1.65839421749115\n",
      "Step 1402: val loss: 54.56712341308594\n",
      "Step 1403: train loss: 1.6575924158096313\n",
      "Step 1403: val loss: 54.56719207763672\n",
      "Step 1404: train loss: 1.6567904949188232\n",
      "Step 1404: val loss: 54.56723403930664\n",
      "Step 1405: train loss: 1.6559898853302002\n",
      "Step 1405: val loss: 54.567237854003906\n",
      "Step 1406: train loss: 1.6551892757415771\n",
      "Step 1406: val loss: 54.56721878051758\n",
      "Step 1407: train loss: 1.6543893814086914\n",
      "Step 1407: val loss: 54.5671501159668\n",
      "Step 1408: train loss: 1.653590202331543\n",
      "Step 1408: val loss: 54.56706619262695\n",
      "Step 1409: train loss: 1.6527913808822632\n",
      "Step 1409: val loss: 54.56692886352539\n",
      "Step 1410: train loss: 1.6519930362701416\n",
      "Step 1410: val loss: 54.56678009033203\n",
      "Step 1411: train loss: 1.6511958837509155\n",
      "Step 1411: val loss: 54.56657028198242\n",
      "Step 1412: train loss: 1.6503989696502686\n",
      "Step 1412: val loss: 54.566349029541016\n",
      "Step 1413: train loss: 1.6496022939682007\n",
      "Step 1413: val loss: 54.566078186035156\n",
      "Step 1414: train loss: 1.6488063335418701\n",
      "Step 1414: val loss: 54.5657844543457\n",
      "Step 1415: train loss: 1.6480108499526978\n",
      "Step 1415: val loss: 54.56545639038086\n",
      "Step 1416: train loss: 1.647216558456421\n",
      "Step 1416: val loss: 54.565086364746094\n",
      "Step 1417: train loss: 1.6464223861694336\n",
      "Step 1417: val loss: 54.5646858215332\n",
      "Step 1418: train loss: 1.645628571510315\n",
      "Step 1418: val loss: 54.56425476074219\n",
      "Step 1419: train loss: 1.644835114479065\n",
      "Step 1419: val loss: 54.56378936767578\n",
      "Step 1420: train loss: 1.6440426111221313\n",
      "Step 1420: val loss: 54.563289642333984\n",
      "Step 1421: train loss: 1.6432504653930664\n",
      "Step 1421: val loss: 54.562767028808594\n",
      "Step 1422: train loss: 1.642459511756897\n",
      "Step 1422: val loss: 54.562198638916016\n",
      "Step 1423: train loss: 1.6416685581207275\n",
      "Step 1423: val loss: 54.56159210205078\n",
      "Step 1424: train loss: 1.6408780813217163\n",
      "Step 1424: val loss: 54.56096267700195\n",
      "Step 1425: train loss: 1.640088438987732\n",
      "Step 1425: val loss: 54.560298919677734\n",
      "Step 1426: train loss: 1.639298677444458\n",
      "Step 1426: val loss: 54.559600830078125\n",
      "Step 1427: train loss: 1.63850998878479\n",
      "Step 1427: val loss: 54.55887985229492\n",
      "Step 1428: train loss: 1.6377222537994385\n",
      "Step 1428: val loss: 54.55811309814453\n",
      "Step 1429: train loss: 1.6369342803955078\n",
      "Step 1429: val loss: 54.55731201171875\n",
      "Step 1430: train loss: 1.6361474990844727\n",
      "Step 1430: val loss: 54.556480407714844\n",
      "Step 1431: train loss: 1.6353607177734375\n",
      "Step 1431: val loss: 54.55562210083008\n",
      "Step 1432: train loss: 1.63457453250885\n",
      "Step 1432: val loss: 54.554725646972656\n",
      "Step 1433: train loss: 1.633789300918579\n",
      "Step 1433: val loss: 54.55379867553711\n",
      "Step 1434: train loss: 1.6330044269561768\n",
      "Step 1434: val loss: 54.55284118652344\n",
      "Step 1435: train loss: 1.6322200298309326\n",
      "Step 1435: val loss: 54.551841735839844\n",
      "Step 1436: train loss: 1.6314359903335571\n",
      "Step 1436: val loss: 54.55080795288086\n",
      "Step 1437: train loss: 1.630652666091919\n",
      "Step 1437: val loss: 54.54976272583008\n",
      "Step 1438: train loss: 1.6298696994781494\n",
      "Step 1438: val loss: 54.548667907714844\n",
      "Step 1439: train loss: 1.6290876865386963\n",
      "Step 1439: val loss: 54.547550201416016\n",
      "Step 1440: train loss: 1.6283057928085327\n",
      "Step 1440: val loss: 54.546382904052734\n",
      "Step 1441: train loss: 1.627524733543396\n",
      "Step 1441: val loss: 54.545196533203125\n",
      "Step 1442: train loss: 1.6267436742782593\n",
      "Step 1442: val loss: 54.543983459472656\n",
      "Step 1443: train loss: 1.6259639263153076\n",
      "Step 1443: val loss: 54.542728424072266\n",
      "Step 1444: train loss: 1.625184416770935\n",
      "Step 1444: val loss: 54.54143524169922\n",
      "Step 1445: train loss: 1.6244051456451416\n",
      "Step 1445: val loss: 54.54012680053711\n",
      "Step 1446: train loss: 1.623626470565796\n",
      "Step 1446: val loss: 54.53877258300781\n",
      "Step 1447: train loss: 1.6228488683700562\n",
      "Step 1447: val loss: 54.537391662597656\n",
      "Step 1448: train loss: 1.6220715045928955\n",
      "Step 1448: val loss: 54.535980224609375\n",
      "Step 1449: train loss: 1.621294617652893\n",
      "Step 1449: val loss: 54.534523010253906\n",
      "Step 1450: train loss: 1.6205179691314697\n",
      "Step 1450: val loss: 54.53304672241211\n",
      "Step 1451: train loss: 1.6197423934936523\n",
      "Step 1451: val loss: 54.53153991699219\n",
      "Step 1452: train loss: 1.6189664602279663\n",
      "Step 1452: val loss: 54.52999496459961\n",
      "Step 1453: train loss: 1.6181923151016235\n",
      "Step 1453: val loss: 54.52842330932617\n",
      "Step 1454: train loss: 1.6174179315567017\n",
      "Step 1454: val loss: 54.52682113647461\n",
      "Step 1455: train loss: 1.6166441440582275\n",
      "Step 1455: val loss: 54.52518081665039\n",
      "Step 1456: train loss: 1.615870714187622\n",
      "Step 1456: val loss: 54.52350997924805\n",
      "Step 1457: train loss: 1.615098237991333\n",
      "Step 1457: val loss: 54.52180099487305\n",
      "Step 1458: train loss: 1.6143261194229126\n",
      "Step 1458: val loss: 54.52007293701172\n",
      "Step 1459: train loss: 1.61355459690094\n",
      "Step 1459: val loss: 54.518306732177734\n",
      "Step 1460: train loss: 1.6127830743789673\n",
      "Step 1460: val loss: 54.516510009765625\n",
      "Step 1461: train loss: 1.6120128631591797\n",
      "Step 1461: val loss: 54.51468276977539\n",
      "Step 1462: train loss: 1.611242651939392\n",
      "Step 1462: val loss: 54.51283264160156\n",
      "Step 1463: train loss: 1.6104732751846313\n",
      "Step 1463: val loss: 54.51093673706055\n",
      "Step 1464: train loss: 1.6097043752670288\n",
      "Step 1464: val loss: 54.50901412963867\n",
      "Step 1465: train loss: 1.6089353561401367\n",
      "Step 1465: val loss: 54.5070686340332\n",
      "Step 1466: train loss: 1.6081680059432983\n",
      "Step 1466: val loss: 54.50507736206055\n",
      "Step 1467: train loss: 1.6074001789093018\n",
      "Step 1467: val loss: 54.50306701660156\n",
      "Step 1468: train loss: 1.606633186340332\n",
      "Step 1468: val loss: 54.501014709472656\n",
      "Step 1469: train loss: 1.60586678981781\n",
      "Step 1469: val loss: 54.49894714355469\n",
      "Step 1470: train loss: 1.6051009893417358\n",
      "Step 1470: val loss: 54.496829986572266\n",
      "Step 1471: train loss: 1.6043351888656616\n",
      "Step 1471: val loss: 54.49468994140625\n",
      "Step 1472: train loss: 1.6035709381103516\n",
      "Step 1472: val loss: 54.49251937866211\n",
      "Step 1473: train loss: 1.6028066873550415\n",
      "Step 1473: val loss: 54.49032211303711\n",
      "Step 1474: train loss: 1.602042555809021\n",
      "Step 1474: val loss: 54.48809051513672\n",
      "Step 1475: train loss: 1.6012792587280273\n",
      "Step 1475: val loss: 54.485816955566406\n",
      "Step 1476: train loss: 1.6005165576934814\n",
      "Step 1476: val loss: 54.48353576660156\n",
      "Step 1477: train loss: 1.5997545719146729\n",
      "Step 1477: val loss: 54.481204986572266\n",
      "Step 1478: train loss: 1.5989924669265747\n",
      "Step 1478: val loss: 54.478851318359375\n",
      "Step 1479: train loss: 1.5982308387756348\n",
      "Step 1479: val loss: 54.47646713256836\n",
      "Step 1480: train loss: 1.5974704027175903\n",
      "Step 1480: val loss: 54.47404479980469\n",
      "Step 1481: train loss: 1.5967100858688354\n",
      "Step 1481: val loss: 54.47160339355469\n",
      "Step 1482: train loss: 1.5959503650665283\n",
      "Step 1482: val loss: 54.469120025634766\n",
      "Step 1483: train loss: 1.5951911211013794\n",
      "Step 1483: val loss: 54.466609954833984\n",
      "Step 1484: train loss: 1.5944324731826782\n",
      "Step 1484: val loss: 54.46407699584961\n",
      "Step 1485: train loss: 1.5936743021011353\n",
      "Step 1485: val loss: 54.461509704589844\n",
      "Step 1486: train loss: 1.5929162502288818\n",
      "Step 1486: val loss: 54.45890808105469\n",
      "Step 1487: train loss: 1.5921592712402344\n",
      "Step 1487: val loss: 54.456268310546875\n",
      "Step 1488: train loss: 1.5914026498794556\n",
      "Step 1488: val loss: 54.453609466552734\n",
      "Step 1489: train loss: 1.5906469821929932\n",
      "Step 1489: val loss: 54.4509162902832\n",
      "Step 1490: train loss: 1.5898908376693726\n",
      "Step 1490: val loss: 54.448204040527344\n",
      "Step 1491: train loss: 1.589136004447937\n",
      "Step 1491: val loss: 54.4454460144043\n",
      "Step 1492: train loss: 1.5883811712265015\n",
      "Step 1492: val loss: 54.44266891479492\n",
      "Step 1493: train loss: 1.5876270532608032\n",
      "Step 1493: val loss: 54.439857482910156\n",
      "Step 1494: train loss: 1.5868732929229736\n",
      "Step 1494: val loss: 54.43701171875\n",
      "Step 1495: train loss: 1.58612060546875\n",
      "Step 1495: val loss: 54.434139251708984\n",
      "Step 1496: train loss: 1.5853675603866577\n",
      "Step 1496: val loss: 54.431236267089844\n",
      "Step 1497: train loss: 1.5846153497695923\n",
      "Step 1497: val loss: 54.428314208984375\n",
      "Step 1498: train loss: 1.5838639736175537\n",
      "Step 1498: val loss: 54.42534637451172\n",
      "Step 1499: train loss: 1.5831124782562256\n",
      "Step 1499: val loss: 54.422359466552734\n",
      "Step 1500: train loss: 1.5823622941970825\n",
      "Step 1500: val loss: 54.419342041015625\n",
      "Step 1501: train loss: 1.5816116333007812\n",
      "Step 1501: val loss: 54.41629409790039\n",
      "Step 1502: train loss: 1.5808624029159546\n",
      "Step 1502: val loss: 54.4132080078125\n",
      "Step 1503: train loss: 1.580113172531128\n",
      "Step 1503: val loss: 54.41009521484375\n",
      "Step 1504: train loss: 1.5793647766113281\n",
      "Step 1504: val loss: 54.406959533691406\n",
      "Step 1505: train loss: 1.5786163806915283\n",
      "Step 1505: val loss: 54.4037971496582\n",
      "Step 1506: train loss: 1.577868938446045\n",
      "Step 1506: val loss: 54.40060043334961\n",
      "Step 1507: train loss: 1.5771217346191406\n",
      "Step 1507: val loss: 54.39735794067383\n",
      "Step 1508: train loss: 1.5763752460479736\n",
      "Step 1508: val loss: 54.394107818603516\n",
      "Step 1509: train loss: 1.575629472732544\n",
      "Step 1509: val loss: 54.39081573486328\n",
      "Step 1510: train loss: 1.5748834609985352\n",
      "Step 1510: val loss: 54.38751220703125\n",
      "Step 1511: train loss: 1.5741380453109741\n",
      "Step 1511: val loss: 54.38417053222656\n",
      "Step 1512: train loss: 1.5733940601348877\n",
      "Step 1512: val loss: 54.38078689575195\n",
      "Step 1513: train loss: 1.5726494789123535\n",
      "Step 1513: val loss: 54.377384185791016\n",
      "Step 1514: train loss: 1.5719058513641357\n",
      "Step 1514: val loss: 54.37395095825195\n",
      "Step 1515: train loss: 1.5711628198623657\n",
      "Step 1515: val loss: 54.37049102783203\n",
      "Step 1516: train loss: 1.5704197883605957\n",
      "Step 1516: val loss: 54.36701202392578\n",
      "Step 1517: train loss: 1.5696778297424316\n",
      "Step 1517: val loss: 54.36349105834961\n",
      "Step 1518: train loss: 1.5689361095428467\n",
      "Step 1518: val loss: 54.359947204589844\n",
      "Step 1519: train loss: 1.56819486618042\n",
      "Step 1519: val loss: 54.35635757446289\n",
      "Step 1520: train loss: 1.5674537420272827\n",
      "Step 1520: val loss: 54.35276412963867\n",
      "Step 1521: train loss: 1.566713809967041\n",
      "Step 1521: val loss: 54.34912109375\n",
      "Step 1522: train loss: 1.565974235534668\n",
      "Step 1522: val loss: 54.34545135498047\n",
      "Step 1523: train loss: 1.5652347803115845\n",
      "Step 1523: val loss: 54.341766357421875\n",
      "Step 1524: train loss: 1.5644960403442383\n",
      "Step 1524: val loss: 54.33803939819336\n",
      "Step 1525: train loss: 1.5637580156326294\n",
      "Step 1525: val loss: 54.334285736083984\n",
      "Step 1526: train loss: 1.5630199909210205\n",
      "Step 1526: val loss: 54.33052444458008\n",
      "Step 1527: train loss: 1.5622831583023071\n",
      "Step 1527: val loss: 54.32670593261719\n",
      "Step 1528: train loss: 1.5615462064743042\n",
      "Step 1528: val loss: 54.3228759765625\n",
      "Step 1529: train loss: 1.560809850692749\n",
      "Step 1529: val loss: 54.31901550292969\n",
      "Step 1530: train loss: 1.560073971748352\n",
      "Step 1530: val loss: 54.31510925292969\n",
      "Step 1531: train loss: 1.5593388080596924\n",
      "Step 1531: val loss: 54.311187744140625\n",
      "Step 1532: train loss: 1.5586035251617432\n",
      "Step 1532: val loss: 54.30725860595703\n",
      "Step 1533: train loss: 1.5578689575195312\n",
      "Step 1533: val loss: 54.30327224731445\n",
      "Step 1534: train loss: 1.5571351051330566\n",
      "Step 1534: val loss: 54.29927062988281\n",
      "Step 1535: train loss: 1.5564018487930298\n",
      "Step 1535: val loss: 54.29523468017578\n",
      "Step 1536: train loss: 1.5556690692901611\n",
      "Step 1536: val loss: 54.29117202758789\n",
      "Step 1537: train loss: 1.554936170578003\n",
      "Step 1537: val loss: 54.28709411621094\n",
      "Step 1538: train loss: 1.5542043447494507\n",
      "Step 1538: val loss: 54.2829704284668\n",
      "Step 1539: train loss: 1.5534727573394775\n",
      "Step 1539: val loss: 54.2788200378418\n",
      "Step 1540: train loss: 1.5527416467666626\n",
      "Step 1540: val loss: 54.2746467590332\n",
      "Step 1541: train loss: 1.5520113706588745\n",
      "Step 1541: val loss: 54.27045440673828\n",
      "Step 1542: train loss: 1.551281213760376\n",
      "Step 1542: val loss: 54.26622009277344\n",
      "Step 1543: train loss: 1.5505512952804565\n",
      "Step 1543: val loss: 54.26197814941406\n",
      "Step 1544: train loss: 1.5498225688934326\n",
      "Step 1544: val loss: 54.257667541503906\n",
      "Step 1545: train loss: 1.5490939617156982\n",
      "Step 1545: val loss: 54.25336456298828\n",
      "Step 1546: train loss: 1.5483659505844116\n",
      "Step 1546: val loss: 54.24901580810547\n",
      "Step 1547: train loss: 1.547638177871704\n",
      "Step 1547: val loss: 54.24465560913086\n",
      "Step 1548: train loss: 1.5469110012054443\n",
      "Step 1548: val loss: 54.240257263183594\n",
      "Step 1549: train loss: 1.5461838245391846\n",
      "Step 1549: val loss: 54.235836029052734\n",
      "Step 1550: train loss: 1.5454576015472412\n",
      "Step 1550: val loss: 54.231388092041016\n",
      "Step 1551: train loss: 1.5447317361831665\n",
      "Step 1551: val loss: 54.226905822753906\n",
      "Step 1552: train loss: 1.544006586074829\n",
      "Step 1552: val loss: 54.22239685058594\n",
      "Step 1553: train loss: 1.5432816743850708\n",
      "Step 1553: val loss: 54.217872619628906\n",
      "Step 1554: train loss: 1.5425573587417603\n",
      "Step 1554: val loss: 54.213314056396484\n",
      "Step 1555: train loss: 1.5418332815170288\n",
      "Step 1555: val loss: 54.20872497558594\n",
      "Step 1556: train loss: 1.5411099195480347\n",
      "Step 1556: val loss: 54.204105377197266\n",
      "Step 1557: train loss: 1.5403869152069092\n",
      "Step 1557: val loss: 54.1994514465332\n",
      "Step 1558: train loss: 1.5396641492843628\n",
      "Step 1558: val loss: 54.194801330566406\n",
      "Step 1559: train loss: 1.5389423370361328\n",
      "Step 1559: val loss: 54.190093994140625\n",
      "Step 1560: train loss: 1.5382206439971924\n",
      "Step 1560: val loss: 54.18537521362305\n",
      "Step 1561: train loss: 1.5374994277954102\n",
      "Step 1561: val loss: 54.180625915527344\n",
      "Step 1562: train loss: 1.5367791652679443\n",
      "Step 1562: val loss: 54.175838470458984\n",
      "Step 1563: train loss: 1.536058783531189\n",
      "Step 1563: val loss: 54.171043395996094\n",
      "Step 1564: train loss: 1.5353388786315918\n",
      "Step 1564: val loss: 54.16619873046875\n",
      "Step 1565: train loss: 1.5346200466156006\n",
      "Step 1565: val loss: 54.161354064941406\n",
      "Step 1566: train loss: 1.5339014530181885\n",
      "Step 1566: val loss: 54.15645980834961\n",
      "Step 1567: train loss: 1.5331828594207764\n",
      "Step 1567: val loss: 54.151554107666016\n",
      "Step 1568: train loss: 1.5324649810791016\n",
      "Step 1568: val loss: 54.146610260009766\n",
      "Step 1569: train loss: 1.5317474603652954\n",
      "Step 1569: val loss: 54.141639709472656\n",
      "Step 1570: train loss: 1.5310308933258057\n",
      "Step 1570: val loss: 54.13665008544922\n",
      "Step 1571: train loss: 1.5303138494491577\n",
      "Step 1571: val loss: 54.13163375854492\n",
      "Step 1572: train loss: 1.5295981168746948\n",
      "Step 1572: val loss: 54.126583099365234\n",
      "Step 1573: train loss: 1.5288822650909424\n",
      "Step 1573: val loss: 54.121524810791016\n",
      "Step 1574: train loss: 1.5281673669815063\n",
      "Step 1574: val loss: 54.11641311645508\n",
      "Step 1575: train loss: 1.5274527072906494\n",
      "Step 1575: val loss: 54.111289978027344\n",
      "Step 1576: train loss: 1.5267386436462402\n",
      "Step 1576: val loss: 54.10614013671875\n",
      "Step 1577: train loss: 1.5260255336761475\n",
      "Step 1577: val loss: 54.10095977783203\n",
      "Step 1578: train loss: 1.5253121852874756\n",
      "Step 1578: val loss: 54.09574890136719\n",
      "Step 1579: train loss: 1.5245994329452515\n",
      "Step 1579: val loss: 54.09050750732422\n",
      "Step 1580: train loss: 1.523887038230896\n",
      "Step 1580: val loss: 54.085262298583984\n",
      "Step 1581: train loss: 1.5231750011444092\n",
      "Step 1581: val loss: 54.07997512817383\n",
      "Step 1582: train loss: 1.5224632024765015\n",
      "Step 1582: val loss: 54.07466506958008\n",
      "Step 1583: train loss: 1.5217525959014893\n",
      "Step 1583: val loss: 54.0693244934082\n",
      "Step 1584: train loss: 1.5210421085357666\n",
      "Step 1584: val loss: 54.06396484375\n",
      "Step 1585: train loss: 1.5203320980072021\n",
      "Step 1585: val loss: 54.058570861816406\n",
      "Step 1586: train loss: 1.5196229219436646\n",
      "Step 1586: val loss: 54.05316162109375\n",
      "Step 1587: train loss: 1.5189132690429688\n",
      "Step 1587: val loss: 54.04772186279297\n",
      "Step 1588: train loss: 1.518204689025879\n",
      "Step 1588: val loss: 54.04225540161133\n",
      "Step 1589: train loss: 1.5174968242645264\n",
      "Step 1589: val loss: 54.03675842285156\n",
      "Step 1590: train loss: 1.5167882442474365\n",
      "Step 1590: val loss: 54.03124237060547\n",
      "Step 1591: train loss: 1.51608145236969\n",
      "Step 1591: val loss: 54.025691986083984\n",
      "Step 1592: train loss: 1.5153748989105225\n",
      "Step 1592: val loss: 54.020118713378906\n",
      "Step 1593: train loss: 1.5146679878234863\n",
      "Step 1593: val loss: 54.01454162597656\n",
      "Step 1594: train loss: 1.5139625072479248\n",
      "Step 1594: val loss: 54.00890350341797\n",
      "Step 1595: train loss: 1.5132566690444946\n",
      "Step 1595: val loss: 54.00326919555664\n",
      "Step 1596: train loss: 1.5125516653060913\n",
      "Step 1596: val loss: 53.99758529663086\n",
      "Step 1597: train loss: 1.5118470191955566\n",
      "Step 1597: val loss: 53.991886138916016\n",
      "Step 1598: train loss: 1.5111428499221802\n",
      "Step 1598: val loss: 53.98615646362305\n",
      "Step 1599: train loss: 1.510439395904541\n",
      "Step 1599: val loss: 53.98041534423828\n",
      "Step 1600: train loss: 1.5097359418869019\n",
      "Step 1600: val loss: 53.97463607788086\n",
      "Step 1601: train loss: 1.5090328454971313\n",
      "Step 1601: val loss: 53.96883773803711\n",
      "Step 1602: train loss: 1.5083303451538086\n",
      "Step 1602: val loss: 53.9630126953125\n",
      "Step 1603: train loss: 1.5076287984848022\n",
      "Step 1603: val loss: 53.957157135009766\n",
      "Step 1604: train loss: 1.5069268941879272\n",
      "Step 1604: val loss: 53.951290130615234\n",
      "Step 1605: train loss: 1.5062263011932373\n",
      "Step 1605: val loss: 53.945377349853516\n",
      "Step 1606: train loss: 1.5055257081985474\n",
      "Step 1606: val loss: 53.93946075439453\n",
      "Step 1607: train loss: 1.5048253536224365\n",
      "Step 1607: val loss: 53.93349838256836\n",
      "Step 1608: train loss: 1.5041258335113525\n",
      "Step 1608: val loss: 53.927528381347656\n",
      "Step 1609: train loss: 1.5034265518188477\n",
      "Step 1609: val loss: 53.92152404785156\n",
      "Step 1610: train loss: 1.502727746963501\n",
      "Step 1610: val loss: 53.915496826171875\n",
      "Step 1611: train loss: 1.502029538154602\n",
      "Step 1611: val loss: 53.909446716308594\n",
      "Step 1612: train loss: 1.5013314485549927\n",
      "Step 1612: val loss: 53.90336990356445\n",
      "Step 1613: train loss: 1.5006341934204102\n",
      "Step 1613: val loss: 53.89725875854492\n",
      "Step 1614: train loss: 1.4999371767044067\n",
      "Step 1614: val loss: 53.89113998413086\n",
      "Step 1615: train loss: 1.4992403984069824\n",
      "Step 1615: val loss: 53.884979248046875\n",
      "Step 1616: train loss: 1.4985440969467163\n",
      "Step 1616: val loss: 53.87880325317383\n",
      "Step 1617: train loss: 1.4978482723236084\n",
      "Step 1617: val loss: 53.87260055541992\n",
      "Step 1618: train loss: 1.4971531629562378\n",
      "Step 1618: val loss: 53.86637878417969\n",
      "Step 1619: train loss: 1.496458649635315\n",
      "Step 1619: val loss: 53.8601188659668\n",
      "Step 1620: train loss: 1.4957642555236816\n",
      "Step 1620: val loss: 53.853843688964844\n",
      "Step 1621: train loss: 1.4950698614120483\n",
      "Step 1621: val loss: 53.84754943847656\n",
      "Step 1622: train loss: 1.4943767786026\n",
      "Step 1622: val loss: 53.84123229980469\n",
      "Step 1623: train loss: 1.4936835765838623\n",
      "Step 1623: val loss: 53.83486557006836\n",
      "Step 1624: train loss: 1.4929908514022827\n",
      "Step 1624: val loss: 53.8284912109375\n",
      "Step 1625: train loss: 1.4922988414764404\n",
      "Step 1625: val loss: 53.82210159301758\n",
      "Step 1626: train loss: 1.4916069507598877\n",
      "Step 1626: val loss: 53.81566619873047\n",
      "Step 1627: train loss: 1.4909155368804932\n",
      "Step 1627: val loss: 53.80922317504883\n",
      "Step 1628: train loss: 1.490224838256836\n",
      "Step 1628: val loss: 53.80276107788086\n",
      "Step 1629: train loss: 1.4895342588424683\n",
      "Step 1629: val loss: 53.79625701904297\n",
      "Step 1630: train loss: 1.4888441562652588\n",
      "Step 1630: val loss: 53.78973388671875\n",
      "Step 1631: train loss: 1.488154649734497\n",
      "Step 1631: val loss: 53.78319549560547\n",
      "Step 1632: train loss: 1.4874656200408936\n",
      "Step 1632: val loss: 53.77663040161133\n",
      "Step 1633: train loss: 1.4867767095565796\n",
      "Step 1633: val loss: 53.7700309753418\n",
      "Step 1634: train loss: 1.4860883951187134\n",
      "Step 1634: val loss: 53.7634162902832\n",
      "Step 1635: train loss: 1.4854004383087158\n",
      "Step 1635: val loss: 53.75677490234375\n",
      "Step 1636: train loss: 1.4847131967544556\n",
      "Step 1636: val loss: 53.75010681152344\n",
      "Step 1637: train loss: 1.4840257167816162\n",
      "Step 1637: val loss: 53.74342346191406\n",
      "Step 1638: train loss: 1.483339548110962\n",
      "Step 1638: val loss: 53.73670959472656\n",
      "Step 1639: train loss: 1.4826536178588867\n",
      "Step 1639: val loss: 53.7299690246582\n",
      "Step 1640: train loss: 1.4819679260253906\n",
      "Step 1640: val loss: 53.723201751708984\n",
      "Step 1641: train loss: 1.4812827110290527\n",
      "Step 1641: val loss: 53.71642303466797\n",
      "Step 1642: train loss: 1.4805973768234253\n",
      "Step 1642: val loss: 53.70962905883789\n",
      "Step 1643: train loss: 1.4799129962921143\n",
      "Step 1643: val loss: 53.702789306640625\n",
      "Step 1644: train loss: 1.4792293310165405\n",
      "Step 1644: val loss: 53.6959342956543\n",
      "Step 1645: train loss: 1.4785457849502563\n",
      "Step 1645: val loss: 53.689048767089844\n",
      "Step 1646: train loss: 1.4778625965118408\n",
      "Step 1646: val loss: 53.68215560913086\n",
      "Step 1647: train loss: 1.477179765701294\n",
      "Step 1647: val loss: 53.675235748291016\n",
      "Step 1648: train loss: 1.4764974117279053\n",
      "Step 1648: val loss: 53.66827392578125\n",
      "Step 1649: train loss: 1.4758154153823853\n",
      "Step 1649: val loss: 53.66130065917969\n",
      "Step 1650: train loss: 1.4751341342926025\n",
      "Step 1650: val loss: 53.65430450439453\n",
      "Step 1651: train loss: 1.474453330039978\n",
      "Step 1651: val loss: 53.64728546142578\n",
      "Step 1652: train loss: 1.4737728834152222\n",
      "Step 1652: val loss: 53.64024353027344\n",
      "Step 1653: train loss: 1.473092794418335\n",
      "Step 1653: val loss: 53.63316345214844\n",
      "Step 1654: train loss: 1.4724128246307373\n",
      "Step 1654: val loss: 53.6260871887207\n",
      "Step 1655: train loss: 1.471733570098877\n",
      "Step 1655: val loss: 53.61896514892578\n",
      "Step 1656: train loss: 1.4710543155670166\n",
      "Step 1656: val loss: 53.61183166503906\n",
      "Step 1657: train loss: 1.470375895500183\n",
      "Step 1657: val loss: 53.604671478271484\n",
      "Step 1658: train loss: 1.469698190689087\n",
      "Step 1658: val loss: 53.59748458862305\n",
      "Step 1659: train loss: 1.4690208435058594\n",
      "Step 1659: val loss: 53.59027862548828\n",
      "Step 1660: train loss: 1.4683432579040527\n",
      "Step 1660: val loss: 53.58304977416992\n",
      "Step 1661: train loss: 1.4676663875579834\n",
      "Step 1661: val loss: 53.5758056640625\n",
      "Step 1662: train loss: 1.4669902324676514\n",
      "Step 1662: val loss: 53.56852340698242\n",
      "Step 1663: train loss: 1.4663145542144775\n",
      "Step 1663: val loss: 53.561222076416016\n",
      "Step 1664: train loss: 1.4656387567520142\n",
      "Step 1664: val loss: 53.55390167236328\n",
      "Step 1665: train loss: 1.4649631977081299\n",
      "Step 1665: val loss: 53.54656219482422\n",
      "Step 1666: train loss: 1.4642889499664307\n",
      "Step 1666: val loss: 53.539188385009766\n",
      "Step 1667: train loss: 1.463614821434021\n",
      "Step 1667: val loss: 53.531795501708984\n",
      "Step 1668: train loss: 1.4629408121109009\n",
      "Step 1668: val loss: 53.524391174316406\n",
      "Step 1669: train loss: 1.4622676372528076\n",
      "Step 1669: val loss: 53.51694107055664\n",
      "Step 1670: train loss: 1.4615943431854248\n",
      "Step 1670: val loss: 53.509490966796875\n",
      "Step 1671: train loss: 1.4609216451644897\n",
      "Step 1671: val loss: 53.50200653076172\n",
      "Step 1672: train loss: 1.4602495431900024\n",
      "Step 1672: val loss: 53.49449920654297\n",
      "Step 1673: train loss: 1.4595776796340942\n",
      "Step 1673: val loss: 53.48698425292969\n",
      "Step 1674: train loss: 1.4589065313339233\n",
      "Step 1674: val loss: 53.47943115234375\n",
      "Step 1675: train loss: 1.458235502243042\n",
      "Step 1675: val loss: 53.47185516357422\n",
      "Step 1676: train loss: 1.4575649499893188\n",
      "Step 1676: val loss: 53.464263916015625\n",
      "Step 1677: train loss: 1.4568947553634644\n",
      "Step 1677: val loss: 53.45664978027344\n",
      "Step 1678: train loss: 1.4562252759933472\n",
      "Step 1678: val loss: 53.449012756347656\n",
      "Step 1679: train loss: 1.4555555582046509\n",
      "Step 1679: val loss: 53.44135284423828\n",
      "Step 1680: train loss: 1.4548869132995605\n",
      "Step 1680: val loss: 53.43366622924805\n",
      "Step 1681: train loss: 1.4542181491851807\n",
      "Step 1681: val loss: 53.42596435546875\n",
      "Step 1682: train loss: 1.4535499811172485\n",
      "Step 1682: val loss: 53.41824722290039\n",
      "Step 1683: train loss: 1.4528827667236328\n",
      "Step 1683: val loss: 53.41048049926758\n",
      "Step 1684: train loss: 1.452215552330017\n",
      "Step 1684: val loss: 53.4027099609375\n",
      "Step 1685: train loss: 1.4515488147735596\n",
      "Step 1685: val loss: 53.394920349121094\n",
      "Step 1686: train loss: 1.4508824348449707\n",
      "Step 1686: val loss: 53.3870964050293\n",
      "Step 1687: train loss: 1.4502161741256714\n",
      "Step 1687: val loss: 53.37926483154297\n",
      "Step 1688: train loss: 1.4495512247085571\n",
      "Step 1688: val loss: 53.37140655517578\n",
      "Step 1689: train loss: 1.448885202407837\n",
      "Step 1689: val loss: 53.363529205322266\n",
      "Step 1690: train loss: 1.44822096824646\n",
      "Step 1690: val loss: 53.355613708496094\n",
      "Step 1691: train loss: 1.4475561380386353\n",
      "Step 1691: val loss: 53.34769821166992\n",
      "Step 1692: train loss: 1.4468920230865479\n",
      "Step 1692: val loss: 53.339752197265625\n",
      "Step 1693: train loss: 1.4462288618087769\n",
      "Step 1693: val loss: 53.3317756652832\n",
      "Step 1694: train loss: 1.4455654621124268\n",
      "Step 1694: val loss: 53.323787689208984\n",
      "Step 1695: train loss: 1.4449031352996826\n",
      "Step 1695: val loss: 53.31576919555664\n",
      "Step 1696: train loss: 1.444240927696228\n",
      "Step 1696: val loss: 53.307735443115234\n",
      "Step 1697: train loss: 1.4435789585113525\n",
      "Step 1697: val loss: 53.2996826171875\n",
      "Step 1698: train loss: 1.4429173469543457\n",
      "Step 1698: val loss: 53.29161071777344\n",
      "Step 1699: train loss: 1.4422566890716553\n",
      "Step 1699: val loss: 53.28350830078125\n",
      "Step 1700: train loss: 1.441595435142517\n",
      "Step 1700: val loss: 53.27538299560547\n",
      "Step 1701: train loss: 1.4409358501434326\n",
      "Step 1701: val loss: 53.26723098754883\n",
      "Step 1702: train loss: 1.4402750730514526\n",
      "Step 1702: val loss: 53.25907516479492\n",
      "Step 1703: train loss: 1.4396158456802368\n",
      "Step 1703: val loss: 53.250885009765625\n",
      "Step 1704: train loss: 1.4389569759368896\n",
      "Step 1704: val loss: 53.2426872253418\n",
      "Step 1705: train loss: 1.438297986984253\n",
      "Step 1705: val loss: 53.23444747924805\n",
      "Step 1706: train loss: 1.4376394748687744\n",
      "Step 1706: val loss: 53.2261962890625\n",
      "Step 1707: train loss: 1.4369819164276123\n",
      "Step 1707: val loss: 53.217933654785156\n",
      "Step 1708: train loss: 1.4363248348236084\n",
      "Step 1708: val loss: 53.20963668823242\n",
      "Step 1709: train loss: 1.435667634010315\n",
      "Step 1709: val loss: 53.20133590698242\n",
      "Step 1710: train loss: 1.4350106716156006\n",
      "Step 1710: val loss: 53.1929931640625\n",
      "Step 1711: train loss: 1.434354543685913\n",
      "Step 1711: val loss: 53.18463134765625\n",
      "Step 1712: train loss: 1.433698058128357\n",
      "Step 1712: val loss: 53.17626190185547\n",
      "Step 1713: train loss: 1.4330432415008545\n",
      "Step 1713: val loss: 53.16785430908203\n",
      "Step 1714: train loss: 1.4323877096176147\n",
      "Step 1714: val loss: 53.159446716308594\n",
      "Step 1715: train loss: 1.4317331314086914\n",
      "Step 1715: val loss: 53.150997161865234\n",
      "Step 1716: train loss: 1.4310786724090576\n",
      "Step 1716: val loss: 53.142539978027344\n",
      "Step 1717: train loss: 1.4304245710372925\n",
      "Step 1717: val loss: 53.13406753540039\n",
      "Step 1718: train loss: 1.4297711849212646\n",
      "Step 1718: val loss: 53.12556457519531\n",
      "Step 1719: train loss: 1.4291183948516846\n",
      "Step 1719: val loss: 53.117027282714844\n",
      "Step 1720: train loss: 1.4284658432006836\n",
      "Step 1720: val loss: 53.108489990234375\n",
      "Step 1721: train loss: 1.4278132915496826\n",
      "Step 1721: val loss: 53.09992980957031\n",
      "Step 1722: train loss: 1.4271609783172607\n",
      "Step 1722: val loss: 53.09134292602539\n",
      "Step 1723: train loss: 1.4265093803405762\n",
      "Step 1723: val loss: 53.08274459838867\n",
      "Step 1724: train loss: 1.4258588552474976\n",
      "Step 1724: val loss: 53.0741081237793\n",
      "Step 1725: train loss: 1.4252080917358398\n",
      "Step 1725: val loss: 53.06545639038086\n",
      "Step 1726: train loss: 1.4245576858520508\n",
      "Step 1726: val loss: 53.056785583496094\n",
      "Step 1727: train loss: 1.4239076375961304\n",
      "Step 1727: val loss: 53.04810333251953\n",
      "Step 1728: train loss: 1.4232580661773682\n",
      "Step 1728: val loss: 53.03939437866211\n",
      "Step 1729: train loss: 1.4226088523864746\n",
      "Step 1729: val loss: 53.030662536621094\n",
      "Step 1730: train loss: 1.4219598770141602\n",
      "Step 1730: val loss: 53.02190399169922\n",
      "Step 1731: train loss: 1.421311378479004\n",
      "Step 1731: val loss: 53.01313781738281\n",
      "Step 1732: train loss: 1.420663595199585\n",
      "Step 1732: val loss: 53.00434494018555\n",
      "Step 1733: train loss: 1.4200160503387451\n",
      "Step 1733: val loss: 52.995540618896484\n",
      "Step 1734: train loss: 1.4193689823150635\n",
      "Step 1734: val loss: 52.9866943359375\n",
      "Step 1735: train loss: 1.41872239112854\n",
      "Step 1735: val loss: 52.97785186767578\n",
      "Step 1736: train loss: 1.4180757999420166\n",
      "Step 1736: val loss: 52.96897506713867\n",
      "Step 1737: train loss: 1.4174296855926514\n",
      "Step 1737: val loss: 52.9600944519043\n",
      "Step 1738: train loss: 1.4167840480804443\n",
      "Step 1738: val loss: 52.951171875\n",
      "Step 1739: train loss: 1.4161388874053955\n",
      "Step 1739: val loss: 52.942237854003906\n",
      "Step 1740: train loss: 1.4154939651489258\n",
      "Step 1740: val loss: 52.933284759521484\n",
      "Step 1741: train loss: 1.414849042892456\n",
      "Step 1741: val loss: 52.924320220947266\n",
      "Step 1742: train loss: 1.4142049551010132\n",
      "Step 1742: val loss: 52.915321350097656\n",
      "Step 1743: train loss: 1.4135611057281494\n",
      "Step 1743: val loss: 52.90631103515625\n",
      "Step 1744: train loss: 1.412917971611023\n",
      "Step 1744: val loss: 52.89727783203125\n",
      "Step 1745: train loss: 1.412274718284607\n",
      "Step 1745: val loss: 52.88823318481445\n",
      "Step 1746: train loss: 1.4116325378417969\n",
      "Step 1746: val loss: 52.87913513183594\n",
      "Step 1747: train loss: 1.410990595817566\n",
      "Step 1747: val loss: 52.870059967041016\n",
      "Step 1748: train loss: 1.410348892211914\n",
      "Step 1748: val loss: 52.86094665527344\n",
      "Step 1749: train loss: 1.4097075462341309\n",
      "Step 1749: val loss: 52.851810455322266\n",
      "Step 1750: train loss: 1.4090663194656372\n",
      "Step 1750: val loss: 52.84265899658203\n",
      "Step 1751: train loss: 1.4084259271621704\n",
      "Step 1751: val loss: 52.83347702026367\n",
      "Step 1752: train loss: 1.4077856540679932\n",
      "Step 1752: val loss: 52.82428741455078\n",
      "Step 1753: train loss: 1.4071457386016846\n",
      "Step 1753: val loss: 52.8150749206543\n",
      "Step 1754: train loss: 1.4065064191818237\n",
      "Step 1754: val loss: 52.80584716796875\n",
      "Step 1755: train loss: 1.4058672189712524\n",
      "Step 1755: val loss: 52.79659652709961\n",
      "Step 1756: train loss: 1.4052283763885498\n",
      "Step 1756: val loss: 52.787330627441406\n",
      "Step 1757: train loss: 1.4045898914337158\n",
      "Step 1757: val loss: 52.77804946899414\n",
      "Step 1758: train loss: 1.4039522409439087\n",
      "Step 1758: val loss: 52.768733978271484\n",
      "Step 1759: train loss: 1.4033143520355225\n",
      "Step 1759: val loss: 52.7594108581543\n",
      "Step 1760: train loss: 1.402677297592163\n",
      "Step 1760: val loss: 52.75006103515625\n",
      "Step 1761: train loss: 1.4020402431488037\n",
      "Step 1761: val loss: 52.740699768066406\n",
      "Step 1762: train loss: 1.4014041423797607\n",
      "Step 1762: val loss: 52.73130798339844\n",
      "Step 1763: train loss: 1.4007680416107178\n",
      "Step 1763: val loss: 52.72190856933594\n",
      "Step 1764: train loss: 1.4001317024230957\n",
      "Step 1764: val loss: 52.71249771118164\n",
      "Step 1765: train loss: 1.399497151374817\n",
      "Step 1765: val loss: 52.703033447265625\n",
      "Step 1766: train loss: 1.3988622426986694\n",
      "Step 1766: val loss: 52.693572998046875\n",
      "Step 1767: train loss: 1.3982279300689697\n",
      "Step 1767: val loss: 52.68408966064453\n",
      "Step 1768: train loss: 1.3975937366485596\n",
      "Step 1768: val loss: 52.674583435058594\n",
      "Step 1769: train loss: 1.396959900856018\n",
      "Step 1769: val loss: 52.66505813598633\n",
      "Step 1770: train loss: 1.3963267803192139\n",
      "Step 1770: val loss: 52.65552520751953\n",
      "Step 1771: train loss: 1.395693302154541\n",
      "Step 1771: val loss: 52.64596939086914\n",
      "Step 1772: train loss: 1.3950607776641846\n",
      "Step 1772: val loss: 52.63638687133789\n",
      "Step 1773: train loss: 1.3944283723831177\n",
      "Step 1773: val loss: 52.62678909301758\n",
      "Step 1774: train loss: 1.393796682357788\n",
      "Step 1774: val loss: 52.6171760559082\n",
      "Step 1775: train loss: 1.3931657075881958\n",
      "Step 1775: val loss: 52.607540130615234\n",
      "Step 1776: train loss: 1.3925341367721558\n",
      "Step 1776: val loss: 52.59788513183594\n",
      "Step 1777: train loss: 1.3919041156768799\n",
      "Step 1777: val loss: 52.588218688964844\n",
      "Step 1778: train loss: 1.3912732601165771\n",
      "Step 1778: val loss: 52.578521728515625\n",
      "Step 1779: train loss: 1.3906430006027222\n",
      "Step 1779: val loss: 52.56882095336914\n",
      "Step 1780: train loss: 1.3900132179260254\n",
      "Step 1780: val loss: 52.5590934753418\n",
      "Step 1781: train loss: 1.3893839120864868\n",
      "Step 1781: val loss: 52.549339294433594\n",
      "Step 1782: train loss: 1.388755202293396\n",
      "Step 1782: val loss: 52.539581298828125\n",
      "Step 1783: train loss: 1.3881264925003052\n",
      "Step 1783: val loss: 52.5297966003418\n",
      "Step 1784: train loss: 1.3874982595443726\n",
      "Step 1784: val loss: 52.519996643066406\n",
      "Step 1785: train loss: 1.3868706226348877\n",
      "Step 1785: val loss: 52.51018142700195\n",
      "Step 1786: train loss: 1.3862433433532715\n",
      "Step 1786: val loss: 52.50033187866211\n",
      "Step 1787: train loss: 1.3856160640716553\n",
      "Step 1787: val loss: 52.490482330322266\n",
      "Step 1788: train loss: 1.3849893808364868\n",
      "Step 1788: val loss: 52.480613708496094\n",
      "Step 1789: train loss: 1.3843629360198975\n",
      "Step 1789: val loss: 52.4707145690918\n",
      "Step 1790: train loss: 1.3837370872497559\n",
      "Step 1790: val loss: 52.46080017089844\n",
      "Step 1791: train loss: 1.3831112384796143\n",
      "Step 1791: val loss: 52.45088195800781\n",
      "Step 1792: train loss: 1.3824865818023682\n",
      "Step 1792: val loss: 52.44092559814453\n",
      "Step 1793: train loss: 1.3818612098693848\n",
      "Step 1793: val loss: 52.43096160888672\n",
      "Step 1794: train loss: 1.3812371492385864\n",
      "Step 1794: val loss: 52.42098617553711\n",
      "Step 1795: train loss: 1.380612850189209\n",
      "Step 1795: val loss: 52.41098403930664\n",
      "Step 1796: train loss: 1.3799892663955688\n",
      "Step 1796: val loss: 52.400962829589844\n",
      "Step 1797: train loss: 1.3793656826019287\n",
      "Step 1797: val loss: 52.39092254638672\n",
      "Step 1798: train loss: 1.3787424564361572\n",
      "Step 1798: val loss: 52.380863189697266\n",
      "Step 1799: train loss: 1.3781201839447021\n",
      "Step 1799: val loss: 52.370784759521484\n",
      "Step 1800: train loss: 1.3774980306625366\n",
      "Step 1800: val loss: 52.36069107055664\n",
      "Step 1801: train loss: 1.3768757581710815\n",
      "Step 1801: val loss: 52.3505744934082\n",
      "Step 1802: train loss: 1.376254677772522\n",
      "Step 1802: val loss: 52.34044647216797\n",
      "Step 1803: train loss: 1.3756334781646729\n",
      "Step 1803: val loss: 52.33030319213867\n",
      "Step 1804: train loss: 1.3750126361846924\n",
      "Step 1804: val loss: 52.32012939453125\n",
      "Step 1805: train loss: 1.3743922710418701\n",
      "Step 1805: val loss: 52.30994415283203\n",
      "Step 1806: train loss: 1.373772382736206\n",
      "Step 1806: val loss: 52.29974365234375\n",
      "Step 1807: train loss: 1.3731523752212524\n",
      "Step 1807: val loss: 52.28952407836914\n",
      "Step 1808: train loss: 1.3725335597991943\n",
      "Step 1808: val loss: 52.279266357421875\n",
      "Step 1809: train loss: 1.3719146251678467\n",
      "Step 1809: val loss: 52.26902389526367\n",
      "Step 1810: train loss: 1.3712961673736572\n",
      "Step 1810: val loss: 52.258750915527344\n",
      "Step 1811: train loss: 1.3706780672073364\n",
      "Step 1811: val loss: 52.248470306396484\n",
      "Step 1812: train loss: 1.3700602054595947\n",
      "Step 1812: val loss: 52.23815155029297\n",
      "Step 1813: train loss: 1.3694429397583008\n",
      "Step 1813: val loss: 52.22782516479492\n",
      "Step 1814: train loss: 1.3688256740570068\n",
      "Step 1814: val loss: 52.21746826171875\n",
      "Step 1815: train loss: 1.368208885192871\n",
      "Step 1815: val loss: 52.20712661743164\n",
      "Step 1816: train loss: 1.367592453956604\n",
      "Step 1816: val loss: 52.19672775268555\n",
      "Step 1817: train loss: 1.3669763803482056\n",
      "Step 1817: val loss: 52.186344146728516\n",
      "Step 1818: train loss: 1.366361141204834\n",
      "Step 1818: val loss: 52.1759147644043\n",
      "Step 1819: train loss: 1.365746021270752\n",
      "Step 1819: val loss: 52.16548156738281\n",
      "Step 1820: train loss: 1.3651305437088013\n",
      "Step 1820: val loss: 52.15504455566406\n",
      "Step 1821: train loss: 1.3645164966583252\n",
      "Step 1821: val loss: 52.144569396972656\n",
      "Step 1822: train loss: 1.3639020919799805\n",
      "Step 1822: val loss: 52.13409423828125\n",
      "Step 1823: train loss: 1.3632876873016357\n",
      "Step 1823: val loss: 52.12357711791992\n",
      "Step 1824: train loss: 1.3626744747161865\n",
      "Step 1824: val loss: 52.113067626953125\n",
      "Step 1825: train loss: 1.3620613813400269\n",
      "Step 1825: val loss: 52.102535247802734\n",
      "Step 1826: train loss: 1.3614484071731567\n",
      "Step 1826: val loss: 52.09197998046875\n",
      "Step 1827: train loss: 1.3608357906341553\n",
      "Step 1827: val loss: 52.08141326904297\n",
      "Step 1828: train loss: 1.3602242469787598\n",
      "Step 1828: val loss: 52.07081985473633\n",
      "Step 1829: train loss: 1.3596121072769165\n",
      "Step 1829: val loss: 52.06022262573242\n",
      "Step 1830: train loss: 1.3590012788772583\n",
      "Step 1830: val loss: 52.049598693847656\n",
      "Step 1831: train loss: 1.3583903312683105\n",
      "Step 1831: val loss: 52.03895950317383\n",
      "Step 1832: train loss: 1.3577792644500732\n",
      "Step 1832: val loss: 52.02831268310547\n",
      "Step 1833: train loss: 1.357169270515442\n",
      "Step 1833: val loss: 52.01763916015625\n",
      "Step 1834: train loss: 1.356559157371521\n",
      "Step 1834: val loss: 52.006954193115234\n",
      "Step 1835: train loss: 1.3559496402740479\n",
      "Step 1835: val loss: 51.99624252319336\n",
      "Step 1836: train loss: 1.3553407192230225\n",
      "Step 1836: val loss: 51.98551559448242\n",
      "Step 1837: train loss: 1.354731798171997\n",
      "Step 1837: val loss: 51.97477340698242\n",
      "Step 1838: train loss: 1.3541233539581299\n",
      "Step 1838: val loss: 51.964019775390625\n",
      "Step 1839: train loss: 1.3535149097442627\n",
      "Step 1839: val loss: 51.95325469970703\n",
      "Step 1840: train loss: 1.3529071807861328\n",
      "Step 1840: val loss: 51.94245910644531\n",
      "Step 1841: train loss: 1.352299690246582\n",
      "Step 1841: val loss: 51.9316520690918\n",
      "Step 1842: train loss: 1.3516925573349\n",
      "Step 1842: val loss: 51.92083740234375\n",
      "Step 1843: train loss: 1.3510856628417969\n",
      "Step 1843: val loss: 51.909996032714844\n",
      "Step 1844: train loss: 1.350479245185852\n",
      "Step 1844: val loss: 51.89914321899414\n",
      "Step 1845: train loss: 1.3498733043670654\n",
      "Step 1845: val loss: 51.888267517089844\n",
      "Step 1846: train loss: 1.3492677211761475\n",
      "Step 1846: val loss: 51.87738037109375\n",
      "Step 1847: train loss: 1.3486621379852295\n",
      "Step 1847: val loss: 51.866477966308594\n",
      "Step 1848: train loss: 1.348057746887207\n",
      "Step 1848: val loss: 51.85553741455078\n",
      "Step 1849: train loss: 1.3474524021148682\n",
      "Step 1849: val loss: 51.84461975097656\n",
      "Step 1850: train loss: 1.3468486070632935\n",
      "Step 1850: val loss: 51.83366394042969\n",
      "Step 1851: train loss: 1.3462448120117188\n",
      "Step 1851: val loss: 51.82269287109375\n",
      "Step 1852: train loss: 1.3456408977508545\n",
      "Step 1852: val loss: 51.81170654296875\n",
      "Step 1853: train loss: 1.3450374603271484\n",
      "Step 1853: val loss: 51.80071258544922\n",
      "Step 1854: train loss: 1.3444349765777588\n",
      "Step 1854: val loss: 51.78968811035156\n",
      "Step 1855: train loss: 1.3438321352005005\n",
      "Step 1855: val loss: 51.778656005859375\n",
      "Step 1856: train loss: 1.3432296514511108\n",
      "Step 1856: val loss: 51.767616271972656\n",
      "Step 1857: train loss: 1.3426275253295898\n",
      "Step 1857: val loss: 51.75654983520508\n",
      "Step 1858: train loss: 1.3420259952545166\n",
      "Step 1858: val loss: 51.7454719543457\n",
      "Step 1859: train loss: 1.3414243459701538\n",
      "Step 1859: val loss: 51.734375\n",
      "Step 1860: train loss: 1.3408238887786865\n",
      "Step 1860: val loss: 51.7232666015625\n",
      "Step 1861: train loss: 1.3402231931686401\n",
      "Step 1861: val loss: 51.712135314941406\n",
      "Step 1862: train loss: 1.339623212814331\n",
      "Step 1862: val loss: 51.70097732543945\n",
      "Step 1863: train loss: 1.339023232460022\n",
      "Step 1863: val loss: 51.6898193359375\n",
      "Step 1864: train loss: 1.3384238481521606\n",
      "Step 1864: val loss: 51.67864990234375\n",
      "Step 1865: train loss: 1.3378245830535889\n",
      "Step 1865: val loss: 51.667457580566406\n",
      "Step 1866: train loss: 1.3372256755828857\n",
      "Step 1866: val loss: 51.656246185302734\n",
      "Step 1867: train loss: 1.3366272449493408\n",
      "Step 1867: val loss: 51.645023345947266\n",
      "Step 1868: train loss: 1.336029052734375\n",
      "Step 1868: val loss: 51.63377380371094\n",
      "Step 1869: train loss: 1.335431694984436\n",
      "Step 1869: val loss: 51.62252426147461\n",
      "Step 1870: train loss: 1.3348338603973389\n",
      "Step 1870: val loss: 51.61125946044922\n",
      "Step 1871: train loss: 1.334236979484558\n",
      "Step 1871: val loss: 51.59996032714844\n",
      "Step 1872: train loss: 1.333640456199646\n",
      "Step 1872: val loss: 51.58864974975586\n",
      "Step 1873: train loss: 1.3330440521240234\n",
      "Step 1873: val loss: 51.577335357666016\n",
      "Step 1874: train loss: 1.3324477672576904\n",
      "Step 1874: val loss: 51.56599426269531\n",
      "Step 1875: train loss: 1.3318519592285156\n",
      "Step 1875: val loss: 51.554656982421875\n",
      "Step 1876: train loss: 1.3312562704086304\n",
      "Step 1876: val loss: 51.54328155517578\n",
      "Step 1877: train loss: 1.3306618928909302\n",
      "Step 1877: val loss: 51.53190231323242\n",
      "Step 1878: train loss: 1.3300669193267822\n",
      "Step 1878: val loss: 51.5205078125\n",
      "Step 1879: train loss: 1.3294727802276611\n",
      "Step 1879: val loss: 51.50909423828125\n",
      "Step 1880: train loss: 1.3288787603378296\n",
      "Step 1880: val loss: 51.497650146484375\n",
      "Step 1881: train loss: 1.3282853364944458\n",
      "Step 1881: val loss: 51.4862174987793\n",
      "Step 1882: train loss: 1.327691674232483\n",
      "Step 1882: val loss: 51.474754333496094\n",
      "Step 1883: train loss: 1.327099084854126\n",
      "Step 1883: val loss: 51.46327590942383\n",
      "Step 1884: train loss: 1.3265066146850586\n",
      "Step 1884: val loss: 51.451778411865234\n",
      "Step 1885: train loss: 1.3259141445159912\n",
      "Step 1885: val loss: 51.440277099609375\n",
      "Step 1886: train loss: 1.3253223896026611\n",
      "Step 1886: val loss: 51.428741455078125\n",
      "Step 1887: train loss: 1.3247308731079102\n",
      "Step 1887: val loss: 51.41720199584961\n",
      "Step 1888: train loss: 1.3241395950317383\n",
      "Step 1888: val loss: 51.405662536621094\n",
      "Step 1889: train loss: 1.3235487937927246\n",
      "Step 1889: val loss: 51.39408874511719\n",
      "Step 1890: train loss: 1.322957992553711\n",
      "Step 1890: val loss: 51.38252258300781\n",
      "Step 1891: train loss: 1.3223674297332764\n",
      "Step 1891: val loss: 51.37092971801758\n",
      "Step 1892: train loss: 1.321777582168579\n",
      "Step 1892: val loss: 51.35932159423828\n",
      "Step 1893: train loss: 1.32118821144104\n",
      "Step 1893: val loss: 51.347686767578125\n",
      "Step 1894: train loss: 1.320598840713501\n",
      "Step 1894: val loss: 51.33604049682617\n",
      "Step 1895: train loss: 1.3200095891952515\n",
      "Step 1895: val loss: 51.32439422607422\n",
      "Step 1896: train loss: 1.3194210529327393\n",
      "Step 1896: val loss: 51.31272888183594\n",
      "Step 1897: train loss: 1.3188327550888062\n",
      "Step 1897: val loss: 51.30104064941406\n",
      "Step 1898: train loss: 1.3182449340820312\n",
      "Step 1898: val loss: 51.289344787597656\n",
      "Step 1899: train loss: 1.317657232284546\n",
      "Step 1899: val loss: 51.27762222290039\n",
      "Step 1900: train loss: 1.3170701265335083\n",
      "Step 1900: val loss: 51.26589584350586\n",
      "Step 1901: train loss: 1.316483736038208\n",
      "Step 1901: val loss: 51.254154205322266\n",
      "Step 1902: train loss: 1.31589674949646\n",
      "Step 1902: val loss: 51.24237823486328\n",
      "Step 1903: train loss: 1.3153104782104492\n",
      "Step 1903: val loss: 51.23061752319336\n",
      "Step 1904: train loss: 1.3147246837615967\n",
      "Step 1904: val loss: 51.21882247924805\n",
      "Step 1905: train loss: 1.3141392469406128\n",
      "Step 1905: val loss: 51.20701599121094\n",
      "Step 1906: train loss: 1.313553810119629\n",
      "Step 1906: val loss: 51.19519805908203\n",
      "Step 1907: train loss: 1.3129689693450928\n",
      "Step 1907: val loss: 51.18335723876953\n",
      "Step 1908: train loss: 1.3123849630355835\n",
      "Step 1908: val loss: 51.171504974365234\n",
      "Step 1909: train loss: 1.3118007183074951\n",
      "Step 1909: val loss: 51.159645080566406\n",
      "Step 1910: train loss: 1.3112165927886963\n",
      "Step 1910: val loss: 51.14777374267578\n",
      "Step 1911: train loss: 1.3106329441070557\n",
      "Step 1911: val loss: 51.13588333129883\n",
      "Step 1912: train loss: 1.3100494146347046\n",
      "Step 1912: val loss: 51.12397766113281\n",
      "Step 1913: train loss: 1.3094667196273804\n",
      "Step 1913: val loss: 51.11205291748047\n",
      "Step 1914: train loss: 1.3088842630386353\n",
      "Step 1914: val loss: 51.10012435913086\n",
      "Step 1915: train loss: 1.3083016872406006\n",
      "Step 1915: val loss: 51.088172912597656\n",
      "Step 1916: train loss: 1.3077198266983032\n",
      "Step 1916: val loss: 51.07620620727539\n",
      "Step 1917: train loss: 1.3071380853652954\n",
      "Step 1917: val loss: 51.064239501953125\n",
      "Step 1918: train loss: 1.3065569400787354\n",
      "Step 1918: val loss: 51.052242279052734\n",
      "Step 1919: train loss: 1.3059756755828857\n",
      "Step 1919: val loss: 51.04024124145508\n",
      "Step 1920: train loss: 1.305395483970642\n",
      "Step 1920: val loss: 51.0282096862793\n",
      "Step 1921: train loss: 1.3048149347305298\n",
      "Step 1921: val loss: 51.01618194580078\n",
      "Step 1922: train loss: 1.3042349815368652\n",
      "Step 1922: val loss: 51.00412368774414\n",
      "Step 1923: train loss: 1.3036556243896484\n",
      "Step 1923: val loss: 50.9920654296875\n",
      "Step 1924: train loss: 1.3030766248703003\n",
      "Step 1924: val loss: 50.9799919128418\n",
      "Step 1925: train loss: 1.302497386932373\n",
      "Step 1925: val loss: 50.967891693115234\n",
      "Step 1926: train loss: 1.3019187450408936\n",
      "Step 1926: val loss: 50.955787658691406\n",
      "Step 1927: train loss: 1.3013402223587036\n",
      "Step 1927: val loss: 50.94367218017578\n",
      "Step 1928: train loss: 1.300762414932251\n",
      "Step 1928: val loss: 50.931541442871094\n",
      "Step 1929: train loss: 1.3001848459243774\n",
      "Step 1929: val loss: 50.919395446777344\n",
      "Step 1930: train loss: 1.2996076345443726\n",
      "Step 1930: val loss: 50.90723419189453\n",
      "Step 1931: train loss: 1.2990304231643677\n",
      "Step 1931: val loss: 50.895057678222656\n",
      "Step 1932: train loss: 1.2984540462493896\n",
      "Step 1932: val loss: 50.88286590576172\n",
      "Step 1933: train loss: 1.297877311706543\n",
      "Step 1933: val loss: 50.87066650390625\n",
      "Step 1934: train loss: 1.2973014116287231\n",
      "Step 1934: val loss: 50.85845947265625\n",
      "Step 1935: train loss: 1.2967253923416138\n",
      "Step 1935: val loss: 50.84622573852539\n",
      "Step 1936: train loss: 1.2961500883102417\n",
      "Step 1936: val loss: 50.83397674560547\n",
      "Step 1937: train loss: 1.2955749034881592\n",
      "Step 1937: val loss: 50.821720123291016\n",
      "Step 1938: train loss: 1.2950005531311035\n",
      "Step 1938: val loss: 50.8094367980957\n",
      "Step 1939: train loss: 1.2944262027740479\n",
      "Step 1939: val loss: 50.797157287597656\n",
      "Step 1940: train loss: 1.2938519716262817\n",
      "Step 1940: val loss: 50.784854888916016\n",
      "Step 1941: train loss: 1.2932782173156738\n",
      "Step 1941: val loss: 50.77253341674805\n",
      "Step 1942: train loss: 1.292704701423645\n",
      "Step 1942: val loss: 50.76021957397461\n",
      "Step 1943: train loss: 1.2921314239501953\n",
      "Step 1943: val loss: 50.74787902832031\n",
      "Step 1944: train loss: 1.2915592193603516\n",
      "Step 1944: val loss: 50.73551940917969\n",
      "Step 1945: train loss: 1.2909862995147705\n",
      "Step 1945: val loss: 50.7231559753418\n",
      "Step 1946: train loss: 1.2904144525527954\n",
      "Step 1946: val loss: 50.71076583862305\n",
      "Step 1947: train loss: 1.2898423671722412\n",
      "Step 1947: val loss: 50.69838333129883\n",
      "Step 1948: train loss: 1.2892707586288452\n",
      "Step 1948: val loss: 50.68597412109375\n",
      "Step 1949: train loss: 1.288699746131897\n",
      "Step 1949: val loss: 50.67354965209961\n",
      "Step 1950: train loss: 1.2881286144256592\n",
      "Step 1950: val loss: 50.66111755371094\n",
      "Step 1951: train loss: 1.2875584363937378\n",
      "Step 1951: val loss: 50.648658752441406\n",
      "Step 1952: train loss: 1.2869880199432373\n",
      "Step 1952: val loss: 50.63621139526367\n",
      "Step 1953: train loss: 1.2864181995391846\n",
      "Step 1953: val loss: 50.62373352050781\n",
      "Step 1954: train loss: 1.2858483791351318\n",
      "Step 1954: val loss: 50.611244201660156\n",
      "Step 1955: train loss: 1.2852795124053955\n",
      "Step 1955: val loss: 50.59872817993164\n",
      "Step 1956: train loss: 1.2847102880477905\n",
      "Step 1956: val loss: 50.58623504638672\n",
      "Step 1957: train loss: 1.2841416597366333\n",
      "Step 1957: val loss: 50.573699951171875\n",
      "Step 1958: train loss: 1.283573031425476\n",
      "Step 1958: val loss: 50.56116485595703\n",
      "Step 1959: train loss: 1.2830054759979248\n",
      "Step 1959: val loss: 50.54861068725586\n",
      "Step 1960: train loss: 1.2824375629425049\n",
      "Step 1960: val loss: 50.536041259765625\n",
      "Step 1961: train loss: 1.2818697690963745\n",
      "Step 1961: val loss: 50.52347183227539\n",
      "Step 1962: train loss: 1.2813031673431396\n",
      "Step 1962: val loss: 50.51087951660156\n",
      "Step 1963: train loss: 1.2807363271713257\n",
      "Step 1963: val loss: 50.49827575683594\n",
      "Step 1964: train loss: 1.280170202255249\n",
      "Step 1964: val loss: 50.48564910888672\n",
      "Step 1965: train loss: 1.2796043157577515\n",
      "Step 1965: val loss: 50.4730224609375\n",
      "Step 1966: train loss: 1.2790379524230957\n",
      "Step 1966: val loss: 50.46038055419922\n",
      "Step 1967: train loss: 1.2784727811813354\n",
      "Step 1967: val loss: 50.44771957397461\n",
      "Step 1968: train loss: 1.2779078483581543\n",
      "Step 1968: val loss: 50.435054779052734\n",
      "Step 1969: train loss: 1.2773430347442627\n",
      "Step 1969: val loss: 50.4223747253418\n",
      "Step 1970: train loss: 1.2767784595489502\n",
      "Step 1970: val loss: 50.40967559814453\n",
      "Step 1971: train loss: 1.2762141227722168\n",
      "Step 1971: val loss: 50.396968841552734\n",
      "Step 1972: train loss: 1.2756500244140625\n",
      "Step 1972: val loss: 50.384254455566406\n",
      "Step 1973: train loss: 1.2750866413116455\n",
      "Step 1973: val loss: 50.37152099609375\n",
      "Step 1974: train loss: 1.2745234966278076\n",
      "Step 1974: val loss: 50.35875701904297\n",
      "Step 1975: train loss: 1.2739603519439697\n",
      "Step 1975: val loss: 50.346012115478516\n",
      "Step 1976: train loss: 1.27339768409729\n",
      "Step 1976: val loss: 50.333248138427734\n",
      "Step 1977: train loss: 1.2728354930877686\n",
      "Step 1977: val loss: 50.32045364379883\n",
      "Step 1978: train loss: 1.272273302078247\n",
      "Step 1978: val loss: 50.307647705078125\n",
      "Step 1979: train loss: 1.2717115879058838\n",
      "Step 1979: val loss: 50.29484939575195\n",
      "Step 1980: train loss: 1.2711503505706787\n",
      "Step 1980: val loss: 50.28202438354492\n",
      "Step 1981: train loss: 1.2705894708633423\n",
      "Step 1981: val loss: 50.26918411254883\n",
      "Step 1982: train loss: 1.2700287103652954\n",
      "Step 1982: val loss: 50.25634765625\n",
      "Step 1983: train loss: 1.2694683074951172\n",
      "Step 1983: val loss: 50.24348449707031\n",
      "Step 1984: train loss: 1.2689073085784912\n",
      "Step 1984: val loss: 50.23061752319336\n",
      "Step 1985: train loss: 1.268347978591919\n",
      "Step 1985: val loss: 50.217735290527344\n",
      "Step 1986: train loss: 1.2677885293960571\n",
      "Step 1986: val loss: 50.204830169677734\n",
      "Step 1987: train loss: 1.267229437828064\n",
      "Step 1987: val loss: 50.191917419433594\n",
      "Step 1988: train loss: 1.2666702270507812\n",
      "Step 1988: val loss: 50.17900466918945\n",
      "Step 1989: train loss: 1.2661114931106567\n",
      "Step 1989: val loss: 50.166072845458984\n",
      "Step 1990: train loss: 1.2655537128448486\n",
      "Step 1990: val loss: 50.15312194824219\n",
      "Step 1991: train loss: 1.2649955749511719\n",
      "Step 1991: val loss: 50.14017105102539\n",
      "Step 1992: train loss: 1.2644379138946533\n",
      "Step 1992: val loss: 50.12720489501953\n",
      "Step 1993: train loss: 1.2638804912567139\n",
      "Step 1993: val loss: 50.114219665527344\n",
      "Step 1994: train loss: 1.2633235454559326\n",
      "Step 1994: val loss: 50.10122299194336\n",
      "Step 1995: train loss: 1.2627665996551514\n",
      "Step 1995: val loss: 50.088218688964844\n",
      "Step 1996: train loss: 1.2622102499008179\n",
      "Step 1996: val loss: 50.0752067565918\n",
      "Step 1997: train loss: 1.2616541385650635\n",
      "Step 1997: val loss: 50.062171936035156\n",
      "Step 1998: train loss: 1.2610981464385986\n",
      "Step 1998: val loss: 50.04912567138672\n",
      "Step 1999: train loss: 1.2605431079864502\n",
      "Step 1999: val loss: 50.036067962646484\n",
      "Step 2000: train loss: 1.2599880695343018\n",
      "Step 2000: val loss: 50.02300262451172\n",
      "Step 2001: train loss: 1.2594330310821533\n",
      "Step 2001: val loss: 50.00993347167969\n",
      "Step 2002: train loss: 1.2588783502578735\n",
      "Step 2002: val loss: 49.99685287475586\n",
      "Step 2003: train loss: 1.2583236694335938\n",
      "Step 2003: val loss: 49.983734130859375\n",
      "Step 2004: train loss: 1.2577695846557617\n",
      "Step 2004: val loss: 49.97063446044922\n",
      "Step 2005: train loss: 1.2572158575057983\n",
      "Step 2005: val loss: 49.957496643066406\n",
      "Step 2006: train loss: 1.2566626071929932\n",
      "Step 2006: val loss: 49.944366455078125\n",
      "Step 2007: train loss: 1.2561097145080566\n",
      "Step 2007: val loss: 49.931209564208984\n",
      "Step 2008: train loss: 1.2555568218231201\n",
      "Step 2008: val loss: 49.91804504394531\n",
      "Step 2009: train loss: 1.2550042867660522\n",
      "Step 2009: val loss: 49.904876708984375\n",
      "Step 2010: train loss: 1.2544524669647217\n",
      "Step 2010: val loss: 49.891693115234375\n",
      "Step 2011: train loss: 1.2539005279541016\n",
      "Step 2011: val loss: 49.87848663330078\n",
      "Step 2012: train loss: 1.2533491849899292\n",
      "Step 2012: val loss: 49.86528396606445\n",
      "Step 2013: train loss: 1.2527976036071777\n",
      "Step 2013: val loss: 49.85207748413086\n",
      "Step 2014: train loss: 1.252246618270874\n",
      "Step 2014: val loss: 49.83884048461914\n",
      "Step 2015: train loss: 1.251696228981018\n",
      "Step 2015: val loss: 49.82559585571289\n",
      "Step 2016: train loss: 1.2511457204818726\n",
      "Step 2016: val loss: 49.81234359741211\n",
      "Step 2017: train loss: 1.2505959272384644\n",
      "Step 2017: val loss: 49.799068450927734\n",
      "Step 2018: train loss: 1.2500464916229248\n",
      "Step 2018: val loss: 49.78579330444336\n",
      "Step 2019: train loss: 1.2494969367980957\n",
      "Step 2019: val loss: 49.77251052856445\n",
      "Step 2020: train loss: 1.2489478588104248\n",
      "Step 2020: val loss: 49.75920867919922\n",
      "Step 2021: train loss: 1.248399019241333\n",
      "Step 2021: val loss: 49.74589920043945\n",
      "Step 2022: train loss: 1.2478500604629517\n",
      "Step 2022: val loss: 49.73258590698242\n",
      "Step 2023: train loss: 1.2473022937774658\n",
      "Step 2023: val loss: 49.719242095947266\n",
      "Step 2024: train loss: 1.2467544078826904\n",
      "Step 2024: val loss: 49.70587921142578\n",
      "Step 2025: train loss: 1.2462066411972046\n",
      "Step 2025: val loss: 49.692535400390625\n",
      "Step 2026: train loss: 1.2456592321395874\n",
      "Step 2026: val loss: 49.67916488647461\n",
      "Step 2027: train loss: 1.245112657546997\n",
      "Step 2027: val loss: 49.66578674316406\n",
      "Step 2028: train loss: 1.244565486907959\n",
      "Step 2028: val loss: 49.65239715576172\n",
      "Step 2029: train loss: 1.2440192699432373\n",
      "Step 2029: val loss: 49.63899612426758\n",
      "Step 2030: train loss: 1.243472933769226\n",
      "Step 2030: val loss: 49.625587463378906\n",
      "Step 2031: train loss: 1.2429271936416626\n",
      "Step 2031: val loss: 49.61215591430664\n",
      "Step 2032: train loss: 1.2423814535140991\n",
      "Step 2032: val loss: 49.59872817993164\n",
      "Step 2033: train loss: 1.2418363094329834\n",
      "Step 2033: val loss: 49.585269927978516\n",
      "Step 2034: train loss: 1.2412917613983154\n",
      "Step 2034: val loss: 49.571807861328125\n",
      "Step 2035: train loss: 1.2407468557357788\n",
      "Step 2035: val loss: 49.558353424072266\n",
      "Step 2036: train loss: 1.2402026653289795\n",
      "Step 2036: val loss: 49.54487228393555\n",
      "Step 2037: train loss: 1.2396585941314697\n",
      "Step 2037: val loss: 49.531394958496094\n",
      "Step 2038: train loss: 1.2391146421432495\n",
      "Step 2038: val loss: 49.51787567138672\n",
      "Step 2039: train loss: 1.2385716438293457\n",
      "Step 2039: val loss: 49.504364013671875\n",
      "Step 2040: train loss: 1.2380285263061523\n",
      "Step 2040: val loss: 49.4908447265625\n",
      "Step 2041: train loss: 1.2374857664108276\n",
      "Step 2041: val loss: 49.4773063659668\n",
      "Step 2042: train loss: 1.236943006515503\n",
      "Step 2042: val loss: 49.463768005371094\n",
      "Step 2043: train loss: 1.236401081085205\n",
      "Step 2043: val loss: 49.45020294189453\n",
      "Step 2044: train loss: 1.235858678817749\n",
      "Step 2044: val loss: 49.436649322509766\n",
      "Step 2045: train loss: 1.2353172302246094\n",
      "Step 2045: val loss: 49.42306900024414\n",
      "Step 2046: train loss: 1.2347757816314697\n",
      "Step 2046: val loss: 49.409481048583984\n",
      "Step 2047: train loss: 1.2342348098754883\n",
      "Step 2047: val loss: 49.3958854675293\n",
      "Step 2048: train loss: 1.2336938381195068\n",
      "Step 2048: val loss: 49.38227081298828\n",
      "Step 2049: train loss: 1.233153223991394\n",
      "Step 2049: val loss: 49.368648529052734\n",
      "Step 2050: train loss: 1.2326136827468872\n",
      "Step 2050: val loss: 49.355010986328125\n",
      "Step 2051: train loss: 1.2320735454559326\n",
      "Step 2051: val loss: 49.341373443603516\n",
      "Step 2052: train loss: 1.2315341234207153\n",
      "Step 2052: val loss: 49.32771301269531\n",
      "Step 2053: train loss: 1.2309945821762085\n",
      "Step 2053: val loss: 49.31406021118164\n",
      "Step 2054: train loss: 1.2304556369781494\n",
      "Step 2054: val loss: 49.30038833618164\n",
      "Step 2055: train loss: 1.2299168109893799\n",
      "Step 2055: val loss: 49.28670883178711\n",
      "Step 2056: train loss: 1.2293788194656372\n",
      "Step 2056: val loss: 49.27300262451172\n",
      "Step 2057: train loss: 1.2288405895233154\n",
      "Step 2057: val loss: 49.25929260253906\n",
      "Step 2058: train loss: 1.2283024787902832\n",
      "Step 2058: val loss: 49.24558639526367\n",
      "Step 2059: train loss: 1.2277650833129883\n",
      "Step 2059: val loss: 49.23185729980469\n",
      "Step 2060: train loss: 1.2272279262542725\n",
      "Step 2060: val loss: 49.218135833740234\n",
      "Step 2061: train loss: 1.2266910076141357\n",
      "Step 2061: val loss: 49.204376220703125\n",
      "Step 2062: train loss: 1.2261539697647095\n",
      "Step 2062: val loss: 49.19062423706055\n",
      "Step 2063: train loss: 1.2256176471710205\n",
      "Step 2063: val loss: 49.17686462402344\n",
      "Step 2064: train loss: 1.2250821590423584\n",
      "Step 2064: val loss: 49.16307830810547\n",
      "Step 2065: train loss: 1.224546194076538\n",
      "Step 2065: val loss: 49.14928436279297\n",
      "Step 2066: train loss: 1.2240103483200073\n",
      "Step 2066: val loss: 49.13549041748047\n",
      "Step 2067: train loss: 1.223475456237793\n",
      "Step 2067: val loss: 49.121681213378906\n",
      "Step 2068: train loss: 1.22294020652771\n",
      "Step 2068: val loss: 49.10786437988281\n",
      "Step 2069: train loss: 1.2224059104919434\n",
      "Step 2069: val loss: 49.094032287597656\n",
      "Step 2070: train loss: 1.2218713760375977\n",
      "Step 2070: val loss: 49.0802001953125\n",
      "Step 2071: train loss: 1.2213375568389893\n",
      "Step 2071: val loss: 49.066341400146484\n",
      "Step 2072: train loss: 1.2208036184310913\n",
      "Step 2072: val loss: 49.052486419677734\n",
      "Step 2073: train loss: 1.2202701568603516\n",
      "Step 2073: val loss: 49.03861618041992\n",
      "Step 2074: train loss: 1.2197368144989014\n",
      "Step 2074: val loss: 49.02473449707031\n",
      "Step 2075: train loss: 1.2192041873931885\n",
      "Step 2075: val loss: 49.01084899902344\n",
      "Step 2076: train loss: 1.218671441078186\n",
      "Step 2076: val loss: 48.996952056884766\n",
      "Step 2077: train loss: 1.2181389331817627\n",
      "Step 2077: val loss: 48.9830436706543\n",
      "Step 2078: train loss: 1.2176072597503662\n",
      "Step 2078: val loss: 48.969120025634766\n",
      "Step 2079: train loss: 1.217075228691101\n",
      "Step 2079: val loss: 48.95519256591797\n",
      "Step 2080: train loss: 1.2165437936782837\n",
      "Step 2080: val loss: 48.94123840332031\n",
      "Step 2081: train loss: 1.2160130739212036\n",
      "Step 2081: val loss: 48.92729187011719\n",
      "Step 2082: train loss: 1.2154818773269653\n",
      "Step 2082: val loss: 48.913333892822266\n",
      "Step 2083: train loss: 1.2149512767791748\n",
      "Step 2083: val loss: 48.89936447143555\n",
      "Step 2084: train loss: 1.214421272277832\n",
      "Step 2084: val loss: 48.8853874206543\n",
      "Step 2085: train loss: 1.2138912677764893\n",
      "Step 2085: val loss: 48.87139892578125\n",
      "Step 2086: train loss: 1.2133616209030151\n",
      "Step 2086: val loss: 48.857398986816406\n",
      "Step 2087: train loss: 1.212831735610962\n",
      "Step 2087: val loss: 48.843387603759766\n",
      "Step 2088: train loss: 1.2123026847839355\n",
      "Step 2088: val loss: 48.82938003540039\n",
      "Step 2089: train loss: 1.2117737531661987\n",
      "Step 2089: val loss: 48.815345764160156\n",
      "Step 2090: train loss: 1.2112455368041992\n",
      "Step 2090: val loss: 48.80130386352539\n",
      "Step 2091: train loss: 1.2107168436050415\n",
      "Step 2091: val loss: 48.787269592285156\n",
      "Step 2092: train loss: 1.210188627243042\n",
      "Step 2092: val loss: 48.77320861816406\n",
      "Step 2093: train loss: 1.2096608877182007\n",
      "Step 2093: val loss: 48.75913619995117\n",
      "Step 2094: train loss: 1.2091338634490967\n",
      "Step 2094: val loss: 48.74506378173828\n",
      "Step 2095: train loss: 1.2086069583892822\n",
      "Step 2095: val loss: 48.7309684753418\n",
      "Step 2096: train loss: 1.2080793380737305\n",
      "Step 2096: val loss: 48.71689224243164\n",
      "Step 2097: train loss: 1.2075531482696533\n",
      "Step 2097: val loss: 48.7027702331543\n",
      "Step 2098: train loss: 1.2070268392562866\n",
      "Step 2098: val loss: 48.688655853271484\n",
      "Step 2099: train loss: 1.2065011262893677\n",
      "Step 2099: val loss: 48.67453384399414\n",
      "Step 2100: train loss: 1.2059752941131592\n",
      "Step 2100: val loss: 48.66040802001953\n",
      "Step 2101: train loss: 1.2054497003555298\n",
      "Step 2101: val loss: 48.6462516784668\n",
      "Step 2102: train loss: 1.2049245834350586\n",
      "Step 2102: val loss: 48.632118225097656\n",
      "Step 2103: train loss: 1.2043994665145874\n",
      "Step 2103: val loss: 48.61796188354492\n",
      "Step 2104: train loss: 1.2038748264312744\n",
      "Step 2104: val loss: 48.60379409790039\n",
      "Step 2105: train loss: 1.203350305557251\n",
      "Step 2105: val loss: 48.58960723876953\n",
      "Step 2106: train loss: 1.2028264999389648\n",
      "Step 2106: val loss: 48.575408935546875\n",
      "Step 2107: train loss: 1.2023028135299683\n",
      "Step 2107: val loss: 48.56120681762695\n",
      "Step 2108: train loss: 1.2017794847488403\n",
      "Step 2108: val loss: 48.547000885009766\n",
      "Step 2109: train loss: 1.2012561559677124\n",
      "Step 2109: val loss: 48.532779693603516\n",
      "Step 2110: train loss: 1.2007333040237427\n",
      "Step 2110: val loss: 48.5185546875\n",
      "Step 2111: train loss: 1.2002105712890625\n",
      "Step 2111: val loss: 48.50432205200195\n",
      "Step 2112: train loss: 1.1996880769729614\n",
      "Step 2112: val loss: 48.490081787109375\n",
      "Step 2113: train loss: 1.1991665363311768\n",
      "Step 2113: val loss: 48.4758186340332\n",
      "Step 2114: train loss: 1.1986448764801025\n",
      "Step 2114: val loss: 48.46155548095703\n",
      "Step 2115: train loss: 1.1981230974197388\n",
      "Step 2115: val loss: 48.44728088378906\n",
      "Step 2116: train loss: 1.1976017951965332\n",
      "Step 2116: val loss: 48.433006286621094\n",
      "Step 2117: train loss: 1.1970809698104858\n",
      "Step 2117: val loss: 48.4187126159668\n",
      "Step 2118: train loss: 1.196560263633728\n",
      "Step 2118: val loss: 48.40440368652344\n",
      "Step 2119: train loss: 1.1960399150848389\n",
      "Step 2119: val loss: 48.39010238647461\n",
      "Step 2120: train loss: 1.1955199241638184\n",
      "Step 2120: val loss: 48.375789642333984\n",
      "Step 2121: train loss: 1.1949996948242188\n",
      "Step 2121: val loss: 48.3614616394043\n",
      "Step 2122: train loss: 1.194480299949646\n",
      "Step 2122: val loss: 48.34712600708008\n",
      "Step 2123: train loss: 1.193961262702942\n",
      "Step 2123: val loss: 48.332786560058594\n",
      "Step 2124: train loss: 1.19344162940979\n",
      "Step 2124: val loss: 48.31843566894531\n",
      "Step 2125: train loss: 1.192922830581665\n",
      "Step 2125: val loss: 48.304073333740234\n",
      "Step 2126: train loss: 1.1924049854278564\n",
      "Step 2126: val loss: 48.28969955444336\n",
      "Step 2127: train loss: 1.1918864250183105\n",
      "Step 2127: val loss: 48.275325775146484\n",
      "Step 2128: train loss: 1.1913683414459229\n",
      "Step 2128: val loss: 48.26093292236328\n",
      "Step 2129: train loss: 1.1908504962921143\n",
      "Step 2129: val loss: 48.24654006958008\n",
      "Step 2130: train loss: 1.1903339624404907\n",
      "Step 2130: val loss: 48.232120513916016\n",
      "Step 2131: train loss: 1.1898164749145508\n",
      "Step 2131: val loss: 48.21771240234375\n",
      "Step 2132: train loss: 1.1892999410629272\n",
      "Step 2132: val loss: 48.20329284667969\n",
      "Step 2133: train loss: 1.1887834072113037\n",
      "Step 2133: val loss: 48.18885040283203\n",
      "Step 2134: train loss: 1.1882669925689697\n",
      "Step 2134: val loss: 48.17442321777344\n",
      "Step 2135: train loss: 1.1877509355545044\n",
      "Step 2135: val loss: 48.15996170043945\n",
      "Step 2136: train loss: 1.1872351169586182\n",
      "Step 2136: val loss: 48.145511627197266\n",
      "Step 2137: train loss: 1.1867196559906006\n",
      "Step 2137: val loss: 48.13103485107422\n",
      "Step 2138: train loss: 1.1862045526504517\n",
      "Step 2138: val loss: 48.116573333740234\n",
      "Step 2139: train loss: 1.1856896877288818\n",
      "Step 2139: val loss: 48.102088928222656\n",
      "Step 2140: train loss: 1.1851751804351807\n",
      "Step 2140: val loss: 48.08759307861328\n",
      "Step 2141: train loss: 1.1846612691879272\n",
      "Step 2141: val loss: 48.073089599609375\n",
      "Step 2142: train loss: 1.1841466426849365\n",
      "Step 2142: val loss: 48.05859375\n",
      "Step 2143: train loss: 1.1836326122283936\n",
      "Step 2143: val loss: 48.044063568115234\n",
      "Step 2144: train loss: 1.1831194162368774\n",
      "Step 2144: val loss: 48.029544830322266\n",
      "Step 2145: train loss: 1.1826059818267822\n",
      "Step 2145: val loss: 48.0150146484375\n",
      "Step 2146: train loss: 1.1820932626724243\n",
      "Step 2146: val loss: 48.00046157836914\n",
      "Step 2147: train loss: 1.181580662727356\n",
      "Step 2147: val loss: 47.98591232299805\n",
      "Step 2148: train loss: 1.1810684204101562\n",
      "Step 2148: val loss: 47.971351623535156\n",
      "Step 2149: train loss: 1.180555820465088\n",
      "Step 2149: val loss: 47.956783294677734\n",
      "Step 2150: train loss: 1.1800442934036255\n",
      "Step 2150: val loss: 47.94220733642578\n",
      "Step 2151: train loss: 1.179532766342163\n",
      "Step 2151: val loss: 47.92761993408203\n",
      "Step 2152: train loss: 1.1790212392807007\n",
      "Step 2152: val loss: 47.91303253173828\n",
      "Step 2153: train loss: 1.178510308265686\n",
      "Step 2153: val loss: 47.8984260559082\n",
      "Step 2154: train loss: 1.1779996156692505\n",
      "Step 2154: val loss: 47.883827209472656\n",
      "Step 2155: train loss: 1.177488923072815\n",
      "Step 2155: val loss: 47.869205474853516\n",
      "Step 2156: train loss: 1.1769788265228271\n",
      "Step 2156: val loss: 47.85457992553711\n",
      "Step 2157: train loss: 1.1764692068099976\n",
      "Step 2157: val loss: 47.839942932128906\n",
      "Step 2158: train loss: 1.1759592294692993\n",
      "Step 2158: val loss: 47.82529830932617\n",
      "Step 2159: train loss: 1.1754498481750488\n",
      "Step 2159: val loss: 47.81064224243164\n",
      "Step 2160: train loss: 1.1749409437179565\n",
      "Step 2160: val loss: 47.79598617553711\n",
      "Step 2161: train loss: 1.1744316816329956\n",
      "Step 2161: val loss: 47.781314849853516\n",
      "Step 2162: train loss: 1.173923134803772\n",
      "Step 2162: val loss: 47.76665496826172\n",
      "Step 2163: train loss: 1.1734148263931274\n",
      "Step 2163: val loss: 47.75197219848633\n",
      "Step 2164: train loss: 1.1729063987731934\n",
      "Step 2164: val loss: 47.73727798461914\n",
      "Step 2165: train loss: 1.1723988056182861\n",
      "Step 2165: val loss: 47.72258758544922\n",
      "Step 2166: train loss: 1.1718913316726685\n",
      "Step 2166: val loss: 47.707881927490234\n",
      "Step 2167: train loss: 1.171384334564209\n",
      "Step 2167: val loss: 47.69316482543945\n",
      "Step 2168: train loss: 1.1708767414093018\n",
      "Step 2168: val loss: 47.67844772338867\n",
      "Step 2169: train loss: 1.1703698635101318\n",
      "Step 2169: val loss: 47.663719177246094\n",
      "Step 2170: train loss: 1.1698638200759888\n",
      "Step 2170: val loss: 47.648983001708984\n",
      "Step 2171: train loss: 1.169357419013977\n",
      "Step 2171: val loss: 47.63423156738281\n",
      "Step 2172: train loss: 1.168851613998413\n",
      "Step 2172: val loss: 47.61948013305664\n",
      "Step 2173: train loss: 1.1683461666107178\n",
      "Step 2173: val loss: 47.60471725463867\n",
      "Step 2174: train loss: 1.1678407192230225\n",
      "Step 2174: val loss: 47.58994674682617\n",
      "Step 2175: train loss: 1.1673352718353271\n",
      "Step 2175: val loss: 47.575172424316406\n",
      "Step 2176: train loss: 1.1668307781219482\n",
      "Step 2176: val loss: 47.56038284301758\n",
      "Step 2177: train loss: 1.1663259267807007\n",
      "Step 2177: val loss: 47.54560089111328\n",
      "Step 2178: train loss: 1.1658217906951904\n",
      "Step 2178: val loss: 47.53078842163086\n",
      "Step 2179: train loss: 1.1653176546096802\n",
      "Step 2179: val loss: 47.515987396240234\n",
      "Step 2180: train loss: 1.1648138761520386\n",
      "Step 2180: val loss: 47.50117492675781\n",
      "Step 2181: train loss: 1.1643105745315552\n",
      "Step 2181: val loss: 47.486351013183594\n",
      "Step 2182: train loss: 1.1638071537017822\n",
      "Step 2182: val loss: 47.471519470214844\n",
      "Step 2183: train loss: 1.1633039712905884\n",
      "Step 2183: val loss: 47.456695556640625\n",
      "Step 2184: train loss: 1.1628012657165527\n",
      "Step 2184: val loss: 47.44184494018555\n",
      "Step 2185: train loss: 1.162298560142517\n",
      "Step 2185: val loss: 47.4269905090332\n",
      "Step 2186: train loss: 1.1617968082427979\n",
      "Step 2186: val loss: 47.4121208190918\n",
      "Step 2187: train loss: 1.161294937133789\n",
      "Step 2187: val loss: 47.39725875854492\n",
      "Step 2188: train loss: 1.1607930660247803\n",
      "Step 2188: val loss: 47.38237380981445\n",
      "Step 2189: train loss: 1.1602919101715088\n",
      "Step 2189: val loss: 47.367496490478516\n",
      "Step 2190: train loss: 1.1597906351089478\n",
      "Step 2190: val loss: 47.352603912353516\n",
      "Step 2191: train loss: 1.1592899560928345\n",
      "Step 2191: val loss: 47.33770751953125\n",
      "Step 2192: train loss: 1.158789038658142\n",
      "Step 2192: val loss: 47.32280349731445\n",
      "Step 2193: train loss: 1.1582889556884766\n",
      "Step 2193: val loss: 47.30789566040039\n",
      "Step 2194: train loss: 1.1577887535095215\n",
      "Step 2194: val loss: 47.292964935302734\n",
      "Step 2195: train loss: 1.157288908958435\n",
      "Step 2195: val loss: 47.278038024902344\n",
      "Step 2196: train loss: 1.156789779663086\n",
      "Step 2196: val loss: 47.26310729980469\n",
      "Step 2197: train loss: 1.1562904119491577\n",
      "Step 2197: val loss: 47.2481575012207\n",
      "Step 2198: train loss: 1.155791163444519\n",
      "Step 2198: val loss: 47.233211517333984\n",
      "Step 2199: train loss: 1.1552925109863281\n",
      "Step 2199: val loss: 47.21825408935547\n",
      "Step 2200: train loss: 1.1547937393188477\n",
      "Step 2200: val loss: 47.20328903198242\n",
      "Step 2201: train loss: 1.154295802116394\n",
      "Step 2201: val loss: 47.188316345214844\n",
      "Step 2202: train loss: 1.1537978649139404\n",
      "Step 2202: val loss: 47.17333984375\n",
      "Step 2203: train loss: 1.1532998085021973\n",
      "Step 2203: val loss: 47.15835189819336\n",
      "Step 2204: train loss: 1.1528024673461914\n",
      "Step 2204: val loss: 47.143367767333984\n",
      "Step 2205: train loss: 1.152305006980896\n",
      "Step 2205: val loss: 47.12837219238281\n",
      "Step 2206: train loss: 1.1518080234527588\n",
      "Step 2206: val loss: 47.11335754394531\n",
      "Step 2207: train loss: 1.1513118743896484\n",
      "Step 2207: val loss: 47.09834289550781\n",
      "Step 2208: train loss: 1.150815486907959\n",
      "Step 2208: val loss: 47.08332061767578\n",
      "Step 2209: train loss: 1.15031898021698\n",
      "Step 2209: val loss: 47.06830596923828\n",
      "Step 2210: train loss: 1.1498230695724487\n",
      "Step 2210: val loss: 47.05326461791992\n",
      "Step 2211: train loss: 1.1493273973464966\n",
      "Step 2211: val loss: 47.03822708129883\n",
      "Step 2212: train loss: 1.1488323211669922\n",
      "Step 2212: val loss: 47.02316665649414\n",
      "Step 2213: train loss: 1.1483372449874878\n",
      "Step 2213: val loss: 47.00811767578125\n",
      "Step 2214: train loss: 1.1478424072265625\n",
      "Step 2214: val loss: 46.993045806884766\n",
      "Step 2215: train loss: 1.147347331047058\n",
      "Step 2215: val loss: 46.97798538208008\n",
      "Step 2216: train loss: 1.1468532085418701\n",
      "Step 2216: val loss: 46.96290588378906\n",
      "Step 2217: train loss: 1.1463594436645508\n",
      "Step 2217: val loss: 46.94782257080078\n",
      "Step 2218: train loss: 1.1458653211593628\n",
      "Step 2218: val loss: 46.932743072509766\n",
      "Step 2219: train loss: 1.1453717947006226\n",
      "Step 2219: val loss: 46.917625427246094\n",
      "Step 2220: train loss: 1.1448783874511719\n",
      "Step 2220: val loss: 46.902530670166016\n",
      "Step 2221: train loss: 1.1443853378295898\n",
      "Step 2221: val loss: 46.88742446899414\n",
      "Step 2222: train loss: 1.1438926458358765\n",
      "Step 2222: val loss: 46.8723030090332\n",
      "Step 2223: train loss: 1.143399953842163\n",
      "Step 2223: val loss: 46.857181549072266\n",
      "Step 2224: train loss: 1.1429076194763184\n",
      "Step 2224: val loss: 46.8420524597168\n",
      "Step 2225: train loss: 1.1424157619476318\n",
      "Step 2225: val loss: 46.8269157409668\n",
      "Step 2226: train loss: 1.1419241428375244\n",
      "Step 2226: val loss: 46.811771392822266\n",
      "Step 2227: train loss: 1.1414326429367065\n",
      "Step 2227: val loss: 46.79662322998047\n",
      "Step 2228: train loss: 1.1409413814544678\n",
      "Step 2228: val loss: 46.781455993652344\n",
      "Step 2229: train loss: 1.14044988155365\n",
      "Step 2229: val loss: 46.76629638671875\n",
      "Step 2230: train loss: 1.1399600505828857\n",
      "Step 2230: val loss: 46.751121520996094\n",
      "Step 2231: train loss: 1.139469027519226\n",
      "Step 2231: val loss: 46.73594665527344\n",
      "Step 2232: train loss: 1.1389788389205933\n",
      "Step 2232: val loss: 46.720767974853516\n",
      "Step 2233: train loss: 1.138488531112671\n",
      "Step 2233: val loss: 46.70557403564453\n",
      "Step 2234: train loss: 1.1379992961883545\n",
      "Step 2234: val loss: 46.69038391113281\n",
      "Step 2235: train loss: 1.137510061264038\n",
      "Step 2235: val loss: 46.67517852783203\n",
      "Step 2236: train loss: 1.1370207071304321\n",
      "Step 2236: val loss: 46.65997314453125\n",
      "Step 2237: train loss: 1.1365312337875366\n",
      "Step 2237: val loss: 46.64475631713867\n",
      "Step 2238: train loss: 1.1360431909561157\n",
      "Step 2238: val loss: 46.6295280456543\n",
      "Step 2239: train loss: 1.135554552078247\n",
      "Step 2239: val loss: 46.61429977416992\n",
      "Step 2240: train loss: 1.135066032409668\n",
      "Step 2240: val loss: 46.59907150268555\n",
      "Step 2241: train loss: 1.1345783472061157\n",
      "Step 2241: val loss: 46.58382797241211\n",
      "Step 2242: train loss: 1.1340906620025635\n",
      "Step 2242: val loss: 46.568572998046875\n",
      "Step 2243: train loss: 1.133603572845459\n",
      "Step 2243: val loss: 46.5533332824707\n",
      "Step 2244: train loss: 1.1331161260604858\n",
      "Step 2244: val loss: 46.53806686401367\n",
      "Step 2245: train loss: 1.132629156112671\n",
      "Step 2245: val loss: 46.522796630859375\n",
      "Step 2246: train loss: 1.132142424583435\n",
      "Step 2246: val loss: 46.50751876831055\n",
      "Step 2247: train loss: 1.1316559314727783\n",
      "Step 2247: val loss: 46.49224853515625\n",
      "Step 2248: train loss: 1.1311701536178589\n",
      "Step 2248: val loss: 46.47694778442383\n",
      "Step 2249: train loss: 1.1306840181350708\n",
      "Step 2249: val loss: 46.461666107177734\n",
      "Step 2250: train loss: 1.1301987171173096\n",
      "Step 2250: val loss: 46.44635772705078\n",
      "Step 2251: train loss: 1.1297131776809692\n",
      "Step 2251: val loss: 46.431060791015625\n",
      "Step 2252: train loss: 1.1292279958724976\n",
      "Step 2252: val loss: 46.415748596191406\n",
      "Step 2253: train loss: 1.1287426948547363\n",
      "Step 2253: val loss: 46.40043640136719\n",
      "Step 2254: train loss: 1.128258466720581\n",
      "Step 2254: val loss: 46.38510513305664\n",
      "Step 2255: train loss: 1.1277740001678467\n",
      "Step 2255: val loss: 46.369781494140625\n",
      "Step 2256: train loss: 1.1272902488708496\n",
      "Step 2256: val loss: 46.35443878173828\n",
      "Step 2257: train loss: 1.1268064975738525\n",
      "Step 2257: val loss: 46.3390998840332\n",
      "Step 2258: train loss: 1.1263223886489868\n",
      "Step 2258: val loss: 46.32374954223633\n",
      "Step 2259: train loss: 1.1258389949798584\n",
      "Step 2259: val loss: 46.30839920043945\n",
      "Step 2260: train loss: 1.125355839729309\n",
      "Step 2260: val loss: 46.29304504394531\n",
      "Step 2261: train loss: 1.1248725652694702\n",
      "Step 2261: val loss: 46.27767562866211\n",
      "Step 2262: train loss: 1.1243903636932373\n",
      "Step 2262: val loss: 46.26230239868164\n",
      "Step 2263: train loss: 1.1239081621170044\n",
      "Step 2263: val loss: 46.24691390991211\n",
      "Step 2264: train loss: 1.1234259605407715\n",
      "Step 2264: val loss: 46.23154067993164\n",
      "Step 2265: train loss: 1.1229438781738281\n",
      "Step 2265: val loss: 46.21614456176758\n",
      "Step 2266: train loss: 1.122462272644043\n",
      "Step 2266: val loss: 46.20075225830078\n",
      "Step 2267: train loss: 1.1219810247421265\n",
      "Step 2267: val loss: 46.18534469604492\n",
      "Step 2268: train loss: 1.121500015258789\n",
      "Step 2268: val loss: 46.16994857788086\n",
      "Step 2269: train loss: 1.1210190057754517\n",
      "Step 2269: val loss: 46.1545295715332\n",
      "Step 2270: train loss: 1.1205387115478516\n",
      "Step 2270: val loss: 46.13911437988281\n",
      "Step 2271: train loss: 1.1200579404830933\n",
      "Step 2271: val loss: 46.123687744140625\n",
      "Step 2272: train loss: 1.1195780038833618\n",
      "Step 2272: val loss: 46.10825729370117\n",
      "Step 2273: train loss: 1.1190979480743408\n",
      "Step 2273: val loss: 46.09282684326172\n",
      "Step 2274: train loss: 1.1186182498931885\n",
      "Step 2274: val loss: 46.077388763427734\n",
      "Step 2275: train loss: 1.1181389093399048\n",
      "Step 2275: val loss: 46.06193923950195\n",
      "Step 2276: train loss: 1.1176598072052002\n",
      "Step 2276: val loss: 46.04648208618164\n",
      "Step 2277: train loss: 1.1171809434890747\n",
      "Step 2277: val loss: 46.031036376953125\n",
      "Step 2278: train loss: 1.1167020797729492\n",
      "Step 2278: val loss: 46.015567779541016\n",
      "Step 2279: train loss: 1.1162235736846924\n",
      "Step 2279: val loss: 46.00009536743164\n",
      "Step 2280: train loss: 1.1157457828521729\n",
      "Step 2280: val loss: 45.9846076965332\n",
      "Step 2281: train loss: 1.1152677536010742\n",
      "Step 2281: val loss: 45.96913528442383\n",
      "Step 2282: train loss: 1.1147898435592651\n",
      "Step 2282: val loss: 45.95365524291992\n",
      "Step 2283: train loss: 1.1143128871917725\n",
      "Step 2283: val loss: 45.93815612792969\n",
      "Step 2284: train loss: 1.113835334777832\n",
      "Step 2284: val loss: 45.92265701293945\n",
      "Step 2285: train loss: 1.1133586168289185\n",
      "Step 2285: val loss: 45.90715789794922\n",
      "Step 2286: train loss: 1.1128818988800049\n",
      "Step 2286: val loss: 45.89165115356445\n",
      "Step 2287: train loss: 1.11240553855896\n",
      "Step 2287: val loss: 45.87613296508789\n",
      "Step 2288: train loss: 1.1119296550750732\n",
      "Step 2288: val loss: 45.86061096191406\n",
      "Step 2289: train loss: 1.111453652381897\n",
      "Step 2289: val loss: 45.8450813293457\n",
      "Step 2290: train loss: 1.110978126525879\n",
      "Step 2290: val loss: 45.82954025268555\n",
      "Step 2291: train loss: 1.1105022430419922\n",
      "Step 2291: val loss: 45.814022064208984\n",
      "Step 2292: train loss: 1.110027551651001\n",
      "Step 2292: val loss: 45.7984733581543\n",
      "Step 2293: train loss: 1.1095525026321411\n",
      "Step 2293: val loss: 45.782920837402344\n",
      "Step 2294: train loss: 1.1090774536132812\n",
      "Step 2294: val loss: 45.76736831665039\n",
      "Step 2295: train loss: 1.1086032390594482\n",
      "Step 2295: val loss: 45.751808166503906\n",
      "Step 2296: train loss: 1.1081292629241943\n",
      "Step 2296: val loss: 45.73625183105469\n",
      "Step 2297: train loss: 1.1076551675796509\n",
      "Step 2297: val loss: 45.720672607421875\n",
      "Step 2298: train loss: 1.107181429862976\n",
      "Step 2298: val loss: 45.70510482788086\n",
      "Step 2299: train loss: 1.1067078113555908\n",
      "Step 2299: val loss: 45.68951416015625\n",
      "Step 2300: train loss: 1.1062345504760742\n",
      "Step 2300: val loss: 45.67393493652344\n",
      "Step 2301: train loss: 1.1057612895965576\n",
      "Step 2301: val loss: 45.65835189819336\n",
      "Step 2302: train loss: 1.1052883863449097\n",
      "Step 2302: val loss: 45.64275360107422\n",
      "Step 2303: train loss: 1.1048164367675781\n",
      "Step 2303: val loss: 45.62715530395508\n",
      "Step 2304: train loss: 1.104344129562378\n",
      "Step 2304: val loss: 45.611541748046875\n",
      "Step 2305: train loss: 1.1038718223571777\n",
      "Step 2305: val loss: 45.595943450927734\n",
      "Step 2306: train loss: 1.1033999919891357\n",
      "Step 2306: val loss: 45.58032989501953\n",
      "Step 2307: train loss: 1.1029287576675415\n",
      "Step 2307: val loss: 45.564701080322266\n",
      "Step 2308: train loss: 1.1024572849273682\n",
      "Step 2308: val loss: 45.549076080322266\n",
      "Step 2309: train loss: 1.101986289024353\n",
      "Step 2309: val loss: 45.533443450927734\n",
      "Step 2310: train loss: 1.1015158891677856\n",
      "Step 2310: val loss: 45.5178108215332\n",
      "Step 2311: train loss: 1.1010451316833496\n",
      "Step 2311: val loss: 45.50216293334961\n",
      "Step 2312: train loss: 1.1005746126174927\n",
      "Step 2312: val loss: 45.48652267456055\n",
      "Step 2313: train loss: 1.100104570388794\n",
      "Step 2313: val loss: 45.47086715698242\n",
      "Step 2314: train loss: 1.0996345281600952\n",
      "Step 2314: val loss: 45.455223083496094\n",
      "Step 2315: train loss: 1.0991647243499756\n",
      "Step 2315: val loss: 45.43954849243164\n",
      "Step 2316: train loss: 1.0986957550048828\n",
      "Step 2316: val loss: 45.423892974853516\n",
      "Step 2317: train loss: 1.098226547241211\n",
      "Step 2317: val loss: 45.4082145690918\n",
      "Step 2318: train loss: 1.097757339477539\n",
      "Step 2318: val loss: 45.392547607421875\n",
      "Step 2319: train loss: 1.0972890853881836\n",
      "Step 2319: val loss: 45.37686538696289\n",
      "Step 2320: train loss: 1.0968204736709595\n",
      "Step 2320: val loss: 45.36117935180664\n",
      "Step 2321: train loss: 1.096352219581604\n",
      "Step 2321: val loss: 45.34548568725586\n",
      "Step 2322: train loss: 1.0958844423294067\n",
      "Step 2322: val loss: 45.329795837402344\n",
      "Step 2323: train loss: 1.095416784286499\n",
      "Step 2323: val loss: 45.314090728759766\n",
      "Step 2324: train loss: 1.0949496030807495\n",
      "Step 2324: val loss: 45.298377990722656\n",
      "Step 2325: train loss: 1.094482183456421\n",
      "Step 2325: val loss: 45.28267288208008\n",
      "Step 2326: train loss: 1.0940152406692505\n",
      "Step 2326: val loss: 45.26696014404297\n",
      "Step 2327: train loss: 1.0935484170913696\n",
      "Step 2327: val loss: 45.251243591308594\n",
      "Step 2328: train loss: 1.093082308769226\n",
      "Step 2328: val loss: 45.235511779785156\n",
      "Step 2329: train loss: 1.0926158428192139\n",
      "Step 2329: val loss: 45.21978759765625\n",
      "Step 2330: train loss: 1.0921494960784912\n",
      "Step 2330: val loss: 45.20405960083008\n",
      "Step 2331: train loss: 1.091684341430664\n",
      "Step 2331: val loss: 45.18831253051758\n",
      "Step 2332: train loss: 1.0912182331085205\n",
      "Step 2332: val loss: 45.17257308959961\n",
      "Step 2333: train loss: 1.0907530784606934\n",
      "Step 2333: val loss: 45.156822204589844\n",
      "Step 2334: train loss: 1.0902884006500244\n",
      "Step 2334: val loss: 45.14106750488281\n",
      "Step 2335: train loss: 1.0898230075836182\n",
      "Step 2335: val loss: 45.12532424926758\n",
      "Step 2336: train loss: 1.0893588066101074\n",
      "Step 2336: val loss: 45.10954666137695\n",
      "Step 2337: train loss: 1.0888943672180176\n",
      "Step 2337: val loss: 45.09377670288086\n",
      "Step 2338: train loss: 1.0884299278259277\n",
      "Step 2338: val loss: 45.0780143737793\n",
      "Step 2339: train loss: 1.0879663228988647\n",
      "Step 2339: val loss: 45.06222915649414\n",
      "Step 2340: train loss: 1.0875025987625122\n",
      "Step 2340: val loss: 45.04644775390625\n",
      "Step 2341: train loss: 1.0870392322540283\n",
      "Step 2341: val loss: 45.030662536621094\n",
      "Step 2342: train loss: 1.0865758657455444\n",
      "Step 2342: val loss: 45.01487350463867\n",
      "Step 2343: train loss: 1.0861130952835083\n",
      "Step 2343: val loss: 44.999088287353516\n",
      "Step 2344: train loss: 1.0856505632400513\n",
      "Step 2344: val loss: 44.983280181884766\n",
      "Step 2345: train loss: 1.0851877927780151\n",
      "Step 2345: val loss: 44.96748352050781\n",
      "Step 2346: train loss: 1.084726095199585\n",
      "Step 2346: val loss: 44.95166778564453\n",
      "Step 2347: train loss: 1.084263801574707\n",
      "Step 2347: val loss: 44.93586730957031\n",
      "Step 2348: train loss: 1.0838017463684082\n",
      "Step 2348: val loss: 44.92005920410156\n",
      "Step 2349: train loss: 1.0833405256271362\n",
      "Step 2349: val loss: 44.904232025146484\n",
      "Step 2350: train loss: 1.0828790664672852\n",
      "Step 2350: val loss: 44.888404846191406\n",
      "Step 2351: train loss: 1.082418441772461\n",
      "Step 2351: val loss: 44.8725700378418\n",
      "Step 2352: train loss: 1.0819575786590576\n",
      "Step 2352: val loss: 44.85673904418945\n",
      "Step 2353: train loss: 1.081497073173523\n",
      "Step 2353: val loss: 44.84089660644531\n",
      "Step 2354: train loss: 1.0810364484786987\n",
      "Step 2354: val loss: 44.82505798339844\n",
      "Step 2355: train loss: 1.0805764198303223\n",
      "Step 2355: val loss: 44.8092155456543\n",
      "Step 2356: train loss: 1.080116629600525\n",
      "Step 2356: val loss: 44.793357849121094\n",
      "Step 2357: train loss: 1.079656958580017\n",
      "Step 2357: val loss: 44.77750015258789\n",
      "Step 2358: train loss: 1.079197883605957\n",
      "Step 2358: val loss: 44.76163101196289\n",
      "Step 2359: train loss: 1.078739047050476\n",
      "Step 2359: val loss: 44.74576950073242\n",
      "Step 2360: train loss: 1.0782798528671265\n",
      "Step 2360: val loss: 44.72990798950195\n",
      "Step 2361: train loss: 1.0778214931488037\n",
      "Step 2361: val loss: 44.71402359008789\n",
      "Step 2362: train loss: 1.0773625373840332\n",
      "Step 2362: val loss: 44.69816970825195\n",
      "Step 2363: train loss: 1.0769044160842896\n",
      "Step 2363: val loss: 44.682281494140625\n",
      "Step 2364: train loss: 1.0764464139938354\n",
      "Step 2364: val loss: 44.6663818359375\n",
      "Step 2365: train loss: 1.075988531112671\n",
      "Step 2365: val loss: 44.650508880615234\n",
      "Step 2366: train loss: 1.075531005859375\n",
      "Step 2366: val loss: 44.63461685180664\n",
      "Step 2367: train loss: 1.0750739574432373\n",
      "Step 2367: val loss: 44.61871337890625\n",
      "Step 2368: train loss: 1.0746166706085205\n",
      "Step 2368: val loss: 44.60280990600586\n",
      "Step 2369: train loss: 1.0741602182388306\n",
      "Step 2369: val loss: 44.58690643310547\n",
      "Step 2370: train loss: 1.0737030506134033\n",
      "Step 2370: val loss: 44.57099914550781\n",
      "Step 2371: train loss: 1.0732471942901611\n",
      "Step 2371: val loss: 44.555076599121094\n",
      "Step 2372: train loss: 1.0727910995483398\n",
      "Step 2372: val loss: 44.5391731262207\n",
      "Step 2373: train loss: 1.0723352432250977\n",
      "Step 2373: val loss: 44.52324676513672\n",
      "Step 2374: train loss: 1.0718798637390137\n",
      "Step 2374: val loss: 44.5073127746582\n",
      "Step 2375: train loss: 1.0714237689971924\n",
      "Step 2375: val loss: 44.49139404296875\n",
      "Step 2376: train loss: 1.0709686279296875\n",
      "Step 2376: val loss: 44.47545623779297\n",
      "Step 2377: train loss: 1.07051420211792\n",
      "Step 2377: val loss: 44.45951461791992\n",
      "Step 2378: train loss: 1.070058822631836\n",
      "Step 2378: val loss: 44.443580627441406\n",
      "Step 2379: train loss: 1.0696049928665161\n",
      "Step 2379: val loss: 44.42763900756836\n",
      "Step 2380: train loss: 1.069150447845459\n",
      "Step 2380: val loss: 44.41168212890625\n",
      "Step 2381: train loss: 1.0686962604522705\n",
      "Step 2381: val loss: 44.395729064941406\n",
      "Step 2382: train loss: 1.0682423114776611\n",
      "Step 2382: val loss: 44.37977600097656\n",
      "Step 2383: train loss: 1.067789077758789\n",
      "Step 2383: val loss: 44.36380386352539\n",
      "Step 2384: train loss: 1.0673353672027588\n",
      "Step 2384: val loss: 44.347843170166016\n",
      "Step 2385: train loss: 1.0668823719024658\n",
      "Step 2385: val loss: 44.33188247680664\n",
      "Step 2386: train loss: 1.066429853439331\n",
      "Step 2386: val loss: 44.31589889526367\n",
      "Step 2387: train loss: 1.0659769773483276\n",
      "Step 2387: val loss: 44.2999267578125\n",
      "Step 2388: train loss: 1.0655245780944824\n",
      "Step 2388: val loss: 44.28395080566406\n",
      "Step 2389: train loss: 1.0650726556777954\n",
      "Step 2389: val loss: 44.267967224121094\n",
      "Step 2390: train loss: 1.064620018005371\n",
      "Step 2390: val loss: 44.251976013183594\n",
      "Step 2391: train loss: 1.0641686916351318\n",
      "Step 2391: val loss: 44.2359733581543\n",
      "Step 2392: train loss: 1.0637171268463135\n",
      "Step 2392: val loss: 44.21999740600586\n",
      "Step 2393: train loss: 1.0632659196853638\n",
      "Step 2393: val loss: 44.203983306884766\n",
      "Step 2394: train loss: 1.062814712524414\n",
      "Step 2394: val loss: 44.1879768371582\n",
      "Step 2395: train loss: 1.0623639822006226\n",
      "Step 2395: val loss: 44.1719856262207\n",
      "Step 2396: train loss: 1.0619133710861206\n",
      "Step 2396: val loss: 44.155967712402344\n",
      "Step 2397: train loss: 1.0614631175994873\n",
      "Step 2397: val loss: 44.139957427978516\n",
      "Step 2398: train loss: 1.0610129833221436\n",
      "Step 2398: val loss: 44.12395095825195\n",
      "Step 2399: train loss: 1.0605628490447998\n",
      "Step 2399: val loss: 44.107933044433594\n",
      "Step 2400: train loss: 1.0601129531860352\n",
      "Step 2400: val loss: 44.09191131591797\n",
      "Step 2401: train loss: 1.059664011001587\n",
      "Step 2401: val loss: 44.07587432861328\n",
      "Step 2402: train loss: 1.0592149496078491\n",
      "Step 2402: val loss: 44.05984115600586\n",
      "Step 2403: train loss: 1.0587657690048218\n",
      "Step 2403: val loss: 44.04380416870117\n",
      "Step 2404: train loss: 1.058316707611084\n",
      "Step 2404: val loss: 44.027774810791016\n",
      "Step 2405: train loss: 1.057868242263794\n",
      "Step 2405: val loss: 44.01172637939453\n",
      "Step 2406: train loss: 1.057420015335083\n",
      "Step 2406: val loss: 43.99569320678711\n",
      "Step 2407: train loss: 1.056971788406372\n",
      "Step 2407: val loss: 43.979637145996094\n",
      "Step 2408: train loss: 1.0565236806869507\n",
      "Step 2408: val loss: 43.96358108520508\n",
      "Step 2409: train loss: 1.0560762882232666\n",
      "Step 2409: val loss: 43.947532653808594\n",
      "Step 2410: train loss: 1.0556291341781616\n",
      "Step 2410: val loss: 43.93147277832031\n",
      "Step 2411: train loss: 1.0551813840866089\n",
      "Step 2411: val loss: 43.915409088134766\n",
      "Step 2412: train loss: 1.0547343492507935\n",
      "Step 2412: val loss: 43.89935302734375\n",
      "Step 2413: train loss: 1.0542879104614258\n",
      "Step 2413: val loss: 43.88328170776367\n",
      "Step 2414: train loss: 1.0538413524627686\n",
      "Step 2414: val loss: 43.867218017578125\n",
      "Step 2415: train loss: 1.0533950328826904\n",
      "Step 2415: val loss: 43.851131439208984\n",
      "Step 2416: train loss: 1.0529491901397705\n",
      "Step 2416: val loss: 43.83506393432617\n",
      "Step 2417: train loss: 1.0525028705596924\n",
      "Step 2417: val loss: 43.81898880004883\n",
      "Step 2418: train loss: 1.0520575046539307\n",
      "Step 2418: val loss: 43.802894592285156\n",
      "Step 2419: train loss: 1.0516122579574585\n",
      "Step 2419: val loss: 43.78679275512695\n",
      "Step 2420: train loss: 1.0511668920516968\n",
      "Step 2420: val loss: 43.770713806152344\n",
      "Step 2421: train loss: 1.0507218837738037\n",
      "Step 2421: val loss: 43.75461196899414\n",
      "Step 2422: train loss: 1.0502771139144897\n",
      "Step 2422: val loss: 43.738529205322266\n",
      "Step 2423: train loss: 1.0498325824737549\n",
      "Step 2423: val loss: 43.7224235534668\n",
      "Step 2424: train loss: 1.0493885278701782\n",
      "Step 2424: val loss: 43.70631790161133\n",
      "Step 2425: train loss: 1.048944354057312\n",
      "Step 2425: val loss: 43.690208435058594\n",
      "Step 2426: train loss: 1.0485005378723145\n",
      "Step 2426: val loss: 43.67409896850586\n",
      "Step 2427: train loss: 1.0480570793151855\n",
      "Step 2427: val loss: 43.65797805786133\n",
      "Step 2428: train loss: 1.0476133823394775\n",
      "Step 2428: val loss: 43.641876220703125\n",
      "Step 2429: train loss: 1.0471702814102173\n",
      "Step 2429: val loss: 43.62575912475586\n",
      "Step 2430: train loss: 1.0467272996902466\n",
      "Step 2430: val loss: 43.60962677001953\n",
      "Step 2431: train loss: 1.0462851524353027\n",
      "Step 2431: val loss: 43.5934944152832\n",
      "Step 2432: train loss: 1.045842170715332\n",
      "Step 2432: val loss: 43.57736587524414\n",
      "Step 2433: train loss: 1.0453999042510986\n",
      "Step 2433: val loss: 43.56123733520508\n",
      "Step 2434: train loss: 1.0449578762054443\n",
      "Step 2434: val loss: 43.54510498046875\n",
      "Step 2435: train loss: 1.0445163249969482\n",
      "Step 2435: val loss: 43.528961181640625\n",
      "Step 2436: train loss: 1.0440747737884521\n",
      "Step 2436: val loss: 43.512821197509766\n",
      "Step 2437: train loss: 1.043633222579956\n",
      "Step 2437: val loss: 43.49667739868164\n",
      "Step 2438: train loss: 1.04319167137146\n",
      "Step 2438: val loss: 43.48053741455078\n",
      "Step 2439: train loss: 1.0427510738372803\n",
      "Step 2439: val loss: 43.464385986328125\n",
      "Step 2440: train loss: 1.0423104763031006\n",
      "Step 2440: val loss: 43.448219299316406\n",
      "Step 2441: train loss: 1.0418696403503418\n",
      "Step 2441: val loss: 43.432071685791016\n",
      "Step 2442: train loss: 1.0414295196533203\n",
      "Step 2442: val loss: 43.41591262817383\n",
      "Step 2443: train loss: 1.0409895181655884\n",
      "Step 2443: val loss: 43.39974594116211\n",
      "Step 2444: train loss: 1.040549635887146\n",
      "Step 2444: val loss: 43.38358688354492\n",
      "Step 2445: train loss: 1.0401099920272827\n",
      "Step 2445: val loss: 43.3674201965332\n",
      "Step 2446: train loss: 1.039670705795288\n",
      "Step 2446: val loss: 43.35124969482422\n",
      "Step 2447: train loss: 1.039231538772583\n",
      "Step 2447: val loss: 43.3350715637207\n",
      "Step 2448: train loss: 1.038792371749878\n",
      "Step 2448: val loss: 43.31890106201172\n",
      "Step 2449: train loss: 1.0383539199829102\n",
      "Step 2449: val loss: 43.302711486816406\n",
      "Step 2450: train loss: 1.0379154682159424\n",
      "Step 2450: val loss: 43.28652572631836\n",
      "Step 2451: train loss: 1.0374774932861328\n",
      "Step 2451: val loss: 43.27035140991211\n",
      "Step 2452: train loss: 1.037038803100586\n",
      "Step 2452: val loss: 43.25416946411133\n",
      "Step 2453: train loss: 1.0366010665893555\n",
      "Step 2453: val loss: 43.237979888916016\n",
      "Step 2454: train loss: 1.0361638069152832\n",
      "Step 2454: val loss: 43.22178268432617\n",
      "Step 2455: train loss: 1.0357258319854736\n",
      "Step 2455: val loss: 43.205596923828125\n",
      "Step 2456: train loss: 1.0352890491485596\n",
      "Step 2456: val loss: 43.189388275146484\n",
      "Step 2457: train loss: 1.0348522663116455\n",
      "Step 2457: val loss: 43.173187255859375\n",
      "Step 2458: train loss: 1.034415364265442\n",
      "Step 2458: val loss: 43.1569938659668\n",
      "Step 2459: train loss: 1.0339785814285278\n",
      "Step 2459: val loss: 43.14078140258789\n",
      "Step 2460: train loss: 1.0335423946380615\n",
      "Step 2460: val loss: 43.124568939208984\n",
      "Step 2461: train loss: 1.0331063270568848\n",
      "Step 2461: val loss: 43.10835266113281\n",
      "Step 2462: train loss: 1.032670497894287\n",
      "Step 2462: val loss: 43.09214782714844\n",
      "Step 2463: train loss: 1.032235026359558\n",
      "Step 2463: val loss: 43.07592010498047\n",
      "Step 2464: train loss: 1.0317991971969604\n",
      "Step 2464: val loss: 43.05971145629883\n",
      "Step 2465: train loss: 1.0313640832901\n",
      "Step 2465: val loss: 43.043487548828125\n",
      "Step 2466: train loss: 1.030929446220398\n",
      "Step 2466: val loss: 43.02725601196289\n",
      "Step 2467: train loss: 1.0304943323135376\n",
      "Step 2467: val loss: 43.01102828979492\n",
      "Step 2468: train loss: 1.0300599336624146\n",
      "Step 2468: val loss: 42.99479675292969\n",
      "Step 2469: train loss: 1.0296252965927124\n",
      "Step 2469: val loss: 42.978580474853516\n",
      "Step 2470: train loss: 1.029191255569458\n",
      "Step 2470: val loss: 42.96234130859375\n",
      "Step 2471: train loss: 1.0287574529647827\n",
      "Step 2471: val loss: 42.946102142333984\n",
      "Step 2472: train loss: 1.0283236503601074\n",
      "Step 2472: val loss: 42.92985534667969\n",
      "Step 2473: train loss: 1.0278900861740112\n",
      "Step 2473: val loss: 42.91361999511719\n",
      "Step 2474: train loss: 1.027457356452942\n",
      "Step 2474: val loss: 42.897369384765625\n",
      "Step 2475: train loss: 1.0270240306854248\n",
      "Step 2475: val loss: 42.88113021850586\n",
      "Step 2476: train loss: 1.0265913009643555\n",
      "Step 2476: val loss: 42.86487579345703\n",
      "Step 2477: train loss: 1.0261584520339966\n",
      "Step 2477: val loss: 42.8486328125\n",
      "Step 2478: train loss: 1.025726318359375\n",
      "Step 2478: val loss: 42.832359313964844\n",
      "Step 2479: train loss: 1.0252940654754639\n",
      "Step 2479: val loss: 42.816104888916016\n",
      "Step 2480: train loss: 1.0248620510101318\n",
      "Step 2480: val loss: 42.79985427856445\n",
      "Step 2481: train loss: 1.0244303941726685\n",
      "Step 2481: val loss: 42.783592224121094\n",
      "Step 2482: train loss: 1.023998737335205\n",
      "Step 2482: val loss: 42.76731872558594\n",
      "Step 2483: train loss: 1.0235671997070312\n",
      "Step 2483: val loss: 42.751060485839844\n",
      "Step 2484: train loss: 1.0231363773345947\n",
      "Step 2484: val loss: 42.734798431396484\n",
      "Step 2485: train loss: 1.0227055549621582\n",
      "Step 2485: val loss: 42.718528747558594\n",
      "Step 2486: train loss: 1.0222746133804321\n",
      "Step 2486: val loss: 42.70225143432617\n",
      "Step 2487: train loss: 1.0218442678451538\n",
      "Step 2487: val loss: 42.68596649169922\n",
      "Step 2488: train loss: 1.0214139223098755\n",
      "Step 2488: val loss: 42.66969680786133\n",
      "Step 2489: train loss: 1.0209839344024658\n",
      "Step 2489: val loss: 42.653419494628906\n",
      "Step 2490: train loss: 1.0205541849136353\n",
      "Step 2490: val loss: 42.63713836669922\n",
      "Step 2491: train loss: 1.020124912261963\n",
      "Step 2491: val loss: 42.620853424072266\n",
      "Step 2492: train loss: 1.0196952819824219\n",
      "Step 2492: val loss: 42.60456085205078\n",
      "Step 2493: train loss: 1.0192660093307495\n",
      "Step 2493: val loss: 42.58827590942383\n",
      "Step 2494: train loss: 1.0188372135162354\n",
      "Step 2494: val loss: 42.571983337402344\n",
      "Step 2495: train loss: 1.018408179283142\n",
      "Step 2495: val loss: 42.55569839477539\n",
      "Step 2496: train loss: 1.017979621887207\n",
      "Step 2496: val loss: 42.539398193359375\n",
      "Step 2497: train loss: 1.017551302909851\n",
      "Step 2497: val loss: 42.523101806640625\n",
      "Step 2498: train loss: 1.0171232223510742\n",
      "Step 2498: val loss: 42.506805419921875\n",
      "Step 2499: train loss: 1.0166953802108765\n",
      "Step 2499: val loss: 42.490509033203125\n",
      "Step 2500: train loss: 1.016268014907837\n",
      "Step 2500: val loss: 42.474205017089844\n",
      "Step 2501: train loss: 1.0158404111862183\n",
      "Step 2501: val loss: 42.4578971862793\n",
      "Step 2502: train loss: 1.0154130458831787\n",
      "Step 2502: val loss: 42.44158935546875\n",
      "Step 2503: train loss: 1.0149861574172974\n",
      "Step 2503: val loss: 42.4252815246582\n",
      "Step 2504: train loss: 1.0145593881607056\n",
      "Step 2504: val loss: 42.408973693847656\n",
      "Step 2505: train loss: 1.0141327381134033\n",
      "Step 2505: val loss: 42.39265060424805\n",
      "Step 2506: train loss: 1.0137064456939697\n",
      "Step 2506: val loss: 42.37633514404297\n",
      "Step 2507: train loss: 1.013279914855957\n",
      "Step 2507: val loss: 42.36001968383789\n",
      "Step 2508: train loss: 1.0128538608551025\n",
      "Step 2508: val loss: 42.34369659423828\n",
      "Step 2509: train loss: 1.0124285221099854\n",
      "Step 2509: val loss: 42.3273811340332\n",
      "Step 2510: train loss: 1.01200270652771\n",
      "Step 2510: val loss: 42.31106185913086\n",
      "Step 2511: train loss: 1.0115773677825928\n",
      "Step 2511: val loss: 42.294734954833984\n",
      "Step 2512: train loss: 1.0111523866653442\n",
      "Step 2512: val loss: 42.27839660644531\n",
      "Step 2513: train loss: 1.0107274055480957\n",
      "Step 2513: val loss: 42.26207733154297\n",
      "Step 2514: train loss: 1.0103027820587158\n",
      "Step 2514: val loss: 42.24574279785156\n",
      "Step 2515: train loss: 1.0098779201507568\n",
      "Step 2515: val loss: 42.22941970825195\n",
      "Step 2516: train loss: 1.0094538927078247\n",
      "Step 2516: val loss: 42.21308135986328\n",
      "Step 2517: train loss: 1.009029507637024\n",
      "Step 2517: val loss: 42.19675064086914\n",
      "Step 2518: train loss: 1.00860595703125\n",
      "Step 2518: val loss: 42.18040466308594\n",
      "Step 2519: train loss: 1.0081822872161865\n",
      "Step 2519: val loss: 42.16407012939453\n",
      "Step 2520: train loss: 1.0077584981918335\n",
      "Step 2520: val loss: 42.14773941040039\n",
      "Step 2521: train loss: 1.0073354244232178\n",
      "Step 2521: val loss: 42.13138961791992\n",
      "Step 2522: train loss: 1.0069127082824707\n",
      "Step 2522: val loss: 42.115047454833984\n",
      "Step 2523: train loss: 1.0064895153045654\n",
      "Step 2523: val loss: 42.098697662353516\n",
      "Step 2524: train loss: 1.006066918373108\n",
      "Step 2524: val loss: 42.08235549926758\n",
      "Step 2525: train loss: 1.005644679069519\n",
      "Step 2525: val loss: 42.06599426269531\n",
      "Step 2526: train loss: 1.0052227973937988\n",
      "Step 2526: val loss: 42.04963684082031\n",
      "Step 2527: train loss: 1.00480055809021\n",
      "Step 2527: val loss: 42.03329849243164\n",
      "Step 2528: train loss: 1.0043789148330688\n",
      "Step 2528: val loss: 42.016937255859375\n",
      "Step 2529: train loss: 1.0039573907852173\n",
      "Step 2529: val loss: 42.000579833984375\n",
      "Step 2530: train loss: 1.003535509109497\n",
      "Step 2530: val loss: 41.98421859741211\n",
      "Step 2531: train loss: 1.0031148195266724\n",
      "Step 2531: val loss: 41.96786117553711\n",
      "Step 2532: train loss: 1.002693772315979\n",
      "Step 2532: val loss: 41.951499938964844\n",
      "Step 2533: train loss: 1.0022730827331543\n",
      "Step 2533: val loss: 41.93513107299805\n",
      "Step 2534: train loss: 1.0018528699874878\n",
      "Step 2534: val loss: 41.91874694824219\n",
      "Step 2535: train loss: 1.001432180404663\n",
      "Step 2535: val loss: 41.90239334106445\n",
      "Step 2536: train loss: 1.0010122060775757\n",
      "Step 2536: val loss: 41.88602066040039\n",
      "Step 2537: train loss: 1.000592589378357\n",
      "Step 2537: val loss: 41.86964797973633\n",
      "Step 2538: train loss: 1.0001726150512695\n",
      "Step 2538: val loss: 41.85328674316406\n",
      "Step 2539: train loss: 0.9997531771659851\n",
      "Step 2539: val loss: 41.8369026184082\n",
      "Step 2540: train loss: 0.9993338584899902\n",
      "Step 2540: val loss: 41.82052230834961\n",
      "Step 2541: train loss: 0.9989145994186401\n",
      "Step 2541: val loss: 41.804161071777344\n",
      "Step 2542: train loss: 0.9984963536262512\n",
      "Step 2542: val loss: 41.78776931762695\n",
      "Step 2543: train loss: 0.9980775117874146\n",
      "Step 2543: val loss: 41.77138137817383\n",
      "Step 2544: train loss: 0.9976590871810913\n",
      "Step 2544: val loss: 41.755008697509766\n",
      "Step 2545: train loss: 0.9972405433654785\n",
      "Step 2545: val loss: 41.73862838745117\n",
      "Step 2546: train loss: 0.9968229532241821\n",
      "Step 2546: val loss: 41.722232818603516\n",
      "Step 2547: train loss: 0.9964050054550171\n",
      "Step 2547: val loss: 41.70585250854492\n",
      "Step 2548: train loss: 0.9959877133369446\n",
      "Step 2548: val loss: 41.68946075439453\n",
      "Step 2549: train loss: 0.9955700039863586\n",
      "Step 2549: val loss: 41.673072814941406\n",
      "Step 2550: train loss: 0.9951528310775757\n",
      "Step 2550: val loss: 41.65668487548828\n",
      "Step 2551: train loss: 0.9947358965873718\n",
      "Step 2551: val loss: 41.64029312133789\n",
      "Step 2552: train loss: 0.994318962097168\n",
      "Step 2552: val loss: 41.623897552490234\n",
      "Step 2553: train loss: 0.9939022064208984\n",
      "Step 2553: val loss: 41.607505798339844\n",
      "Step 2554: train loss: 0.993486225605011\n",
      "Step 2554: val loss: 41.591102600097656\n",
      "Step 2555: train loss: 0.9930702447891235\n",
      "Step 2555: val loss: 41.57470703125\n",
      "Step 2556: train loss: 0.9926538467407227\n",
      "Step 2556: val loss: 41.55831527709961\n",
      "Step 2557: train loss: 0.9922382235527039\n",
      "Step 2557: val loss: 41.54191589355469\n",
      "Step 2558: train loss: 0.9918224215507507\n",
      "Step 2558: val loss: 41.525516510009766\n",
      "Step 2559: train loss: 0.991407036781311\n",
      "Step 2559: val loss: 41.50910568237305\n",
      "Step 2560: train loss: 0.9909920692443848\n",
      "Step 2560: val loss: 41.49270248413086\n",
      "Step 2561: train loss: 0.9905768632888794\n",
      "Step 2561: val loss: 41.47630310058594\n",
      "Step 2562: train loss: 0.9901620745658875\n",
      "Step 2562: val loss: 41.45989990234375\n",
      "Step 2563: train loss: 0.9897474050521851\n",
      "Step 2563: val loss: 41.4434928894043\n",
      "Step 2564: train loss: 0.9893333315849304\n",
      "Step 2564: val loss: 41.427085876464844\n",
      "Step 2565: train loss: 0.9889194369316101\n",
      "Step 2565: val loss: 41.410675048828125\n",
      "Step 2566: train loss: 0.988505482673645\n",
      "Step 2566: val loss: 41.39426040649414\n",
      "Step 2567: train loss: 0.988091766834259\n",
      "Step 2567: val loss: 41.377845764160156\n",
      "Step 2568: train loss: 0.9876782298088074\n",
      "Step 2568: val loss: 41.36144256591797\n",
      "Step 2569: train loss: 0.9872648119926453\n",
      "Step 2569: val loss: 41.34502410888672\n",
      "Step 2570: train loss: 0.9868520498275757\n",
      "Step 2570: val loss: 41.32860565185547\n",
      "Step 2571: train loss: 0.9864384531974792\n",
      "Step 2571: val loss: 41.31219482421875\n",
      "Step 2572: train loss: 0.9860262870788574\n",
      "Step 2572: val loss: 41.2957649230957\n",
      "Step 2573: train loss: 0.9856136441230774\n",
      "Step 2573: val loss: 41.27935791015625\n",
      "Step 2574: train loss: 0.9852014780044556\n",
      "Step 2574: val loss: 41.2629280090332\n",
      "Step 2575: train loss: 0.9847894906997681\n",
      "Step 2575: val loss: 41.24650955200195\n",
      "Step 2576: train loss: 0.9843776822090149\n",
      "Step 2576: val loss: 41.23008728027344\n",
      "Step 2577: train loss: 0.9839655756950378\n",
      "Step 2577: val loss: 41.21367263793945\n",
      "Step 2578: train loss: 0.9835543632507324\n",
      "Step 2578: val loss: 41.19725036621094\n",
      "Step 2579: train loss: 0.9831429719924927\n",
      "Step 2579: val loss: 41.18082046508789\n",
      "Step 2580: train loss: 0.9827322959899902\n",
      "Step 2580: val loss: 41.16438293457031\n",
      "Step 2581: train loss: 0.9823210835456848\n",
      "Step 2581: val loss: 41.14795684814453\n",
      "Step 2582: train loss: 0.9819107055664062\n",
      "Step 2582: val loss: 41.13152313232422\n",
      "Step 2583: train loss: 0.9815001487731934\n",
      "Step 2583: val loss: 41.1151008605957\n",
      "Step 2584: train loss: 0.9810897707939148\n",
      "Step 2584: val loss: 41.098670959472656\n",
      "Step 2585: train loss: 0.9806795120239258\n",
      "Step 2585: val loss: 41.082237243652344\n",
      "Step 2586: train loss: 0.9802699089050293\n",
      "Step 2586: val loss: 41.0658073425293\n",
      "Step 2587: train loss: 0.9798601269721985\n",
      "Step 2587: val loss: 41.04936981201172\n",
      "Step 2588: train loss: 0.9794507026672363\n",
      "Step 2588: val loss: 41.03294372558594\n",
      "Step 2589: train loss: 0.9790416955947876\n",
      "Step 2589: val loss: 41.01649856567383\n",
      "Step 2590: train loss: 0.9786325693130493\n",
      "Step 2590: val loss: 41.000057220458984\n",
      "Step 2591: train loss: 0.978223979473114\n",
      "Step 2591: val loss: 40.98362350463867\n",
      "Step 2592: train loss: 0.9778151512145996\n",
      "Step 2592: val loss: 40.96718215942383\n",
      "Step 2593: train loss: 0.9774066805839539\n",
      "Step 2593: val loss: 40.95075607299805\n",
      "Step 2594: train loss: 0.9769984483718872\n",
      "Step 2594: val loss: 40.934303283691406\n",
      "Step 2595: train loss: 0.9765902757644653\n",
      "Step 2595: val loss: 40.91786193847656\n",
      "Step 2596: train loss: 0.976182758808136\n",
      "Step 2596: val loss: 40.90142059326172\n",
      "Step 2597: train loss: 0.975774884223938\n",
      "Step 2597: val loss: 40.88498306274414\n",
      "Step 2598: train loss: 0.9753676652908325\n",
      "Step 2598: val loss: 40.86853790283203\n",
      "Step 2599: train loss: 0.974960446357727\n",
      "Step 2599: val loss: 40.85207748413086\n",
      "Step 2600: train loss: 0.9745532274246216\n",
      "Step 2600: val loss: 40.83564376831055\n",
      "Step 2601: train loss: 0.9741465449333191\n",
      "Step 2601: val loss: 40.8192024230957\n",
      "Step 2602: train loss: 0.9737393260002136\n",
      "Step 2602: val loss: 40.802757263183594\n",
      "Step 2603: train loss: 0.9733327627182007\n",
      "Step 2603: val loss: 40.78631591796875\n",
      "Step 2604: train loss: 0.9729267358779907\n",
      "Step 2604: val loss: 40.76987075805664\n",
      "Step 2605: train loss: 0.972521185874939\n",
      "Step 2605: val loss: 40.7534065246582\n",
      "Step 2606: train loss: 0.9721152186393738\n",
      "Step 2606: val loss: 40.73695373535156\n",
      "Step 2607: train loss: 0.9717094302177429\n",
      "Step 2607: val loss: 40.72050857543945\n",
      "Step 2608: train loss: 0.9713042974472046\n",
      "Step 2608: val loss: 40.70405578613281\n",
      "Step 2609: train loss: 0.970899224281311\n",
      "Step 2609: val loss: 40.68760681152344\n",
      "Step 2610: train loss: 0.9704940915107727\n",
      "Step 2610: val loss: 40.67115020751953\n",
      "Step 2611: train loss: 0.9700886607170105\n",
      "Step 2611: val loss: 40.654701232910156\n",
      "Step 2612: train loss: 0.969683825969696\n",
      "Step 2612: val loss: 40.63825225830078\n",
      "Step 2613: train loss: 0.9692798852920532\n",
      "Step 2613: val loss: 40.62178421020508\n",
      "Step 2614: train loss: 0.9688755869865417\n",
      "Step 2614: val loss: 40.6053352355957\n",
      "Step 2615: train loss: 0.9684715270996094\n",
      "Step 2615: val loss: 40.588871002197266\n",
      "Step 2616: train loss: 0.9680677652359009\n",
      "Step 2616: val loss: 40.57241439819336\n",
      "Step 2617: train loss: 0.9676640629768372\n",
      "Step 2617: val loss: 40.55596160888672\n",
      "Step 2618: train loss: 0.967260479927063\n",
      "Step 2618: val loss: 40.53950119018555\n",
      "Step 2619: train loss: 0.9668575525283813\n",
      "Step 2619: val loss: 40.523048400878906\n",
      "Step 2620: train loss: 0.9664541482925415\n",
      "Step 2620: val loss: 40.506587982177734\n",
      "Step 2621: train loss: 0.966051459312439\n",
      "Step 2621: val loss: 40.49012756347656\n",
      "Step 2622: train loss: 0.9656485319137573\n",
      "Step 2622: val loss: 40.473663330078125\n",
      "Step 2623: train loss: 0.965246319770813\n",
      "Step 2623: val loss: 40.45720672607422\n",
      "Step 2624: train loss: 0.9648441076278687\n",
      "Step 2624: val loss: 40.44075393676758\n",
      "Step 2625: train loss: 0.9644417762756348\n",
      "Step 2625: val loss: 40.42428970336914\n",
      "Step 2626: train loss: 0.9640399813652039\n",
      "Step 2626: val loss: 40.40782165527344\n",
      "Step 2627: train loss: 0.9636383056640625\n",
      "Step 2627: val loss: 40.391353607177734\n",
      "Step 2628: train loss: 0.9632364511489868\n",
      "Step 2628: val loss: 40.37490463256836\n",
      "Step 2629: train loss: 0.9628354907035828\n",
      "Step 2629: val loss: 40.358428955078125\n",
      "Step 2630: train loss: 0.9624345898628235\n",
      "Step 2630: val loss: 40.34196853637695\n",
      "Step 2631: train loss: 0.9620332717895508\n",
      "Step 2631: val loss: 40.32551193237305\n",
      "Step 2632: train loss: 0.9616327285766602\n",
      "Step 2632: val loss: 40.309043884277344\n",
      "Step 2633: train loss: 0.9612321853637695\n",
      "Step 2633: val loss: 40.29257583618164\n",
      "Step 2634: train loss: 0.9608319401741028\n",
      "Step 2634: val loss: 40.27610778808594\n",
      "Step 2635: train loss: 0.9604319334030151\n",
      "Step 2635: val loss: 40.259639739990234\n",
      "Step 2636: train loss: 0.9600321650505066\n",
      "Step 2636: val loss: 40.24317169189453\n",
      "Step 2637: train loss: 0.959632396697998\n",
      "Step 2637: val loss: 40.22671127319336\n",
      "Step 2638: train loss: 0.9592329263687134\n",
      "Step 2638: val loss: 40.210243225097656\n",
      "Step 2639: train loss: 0.9588335752487183\n",
      "Step 2639: val loss: 40.19377517700195\n",
      "Step 2640: train loss: 0.9584341049194336\n",
      "Step 2640: val loss: 40.177303314208984\n",
      "Step 2641: train loss: 0.9580349922180176\n",
      "Step 2641: val loss: 40.16084289550781\n",
      "Step 2642: train loss: 0.9576364755630493\n",
      "Step 2642: val loss: 40.144371032714844\n",
      "Step 2643: train loss: 0.9572380185127258\n",
      "Step 2643: val loss: 40.127899169921875\n",
      "Step 2644: train loss: 0.9568397402763367\n",
      "Step 2644: val loss: 40.11143112182617\n",
      "Step 2645: train loss: 0.9564414024353027\n",
      "Step 2645: val loss: 40.09495544433594\n",
      "Step 2646: train loss: 0.9560436010360718\n",
      "Step 2646: val loss: 40.0784797668457\n",
      "Step 2647: train loss: 0.9556456804275513\n",
      "Step 2647: val loss: 40.06201171875\n",
      "Step 2648: train loss: 0.9552480578422546\n",
      "Step 2648: val loss: 40.04555130004883\n",
      "Step 2649: train loss: 0.954850971698761\n",
      "Step 2649: val loss: 40.02907180786133\n",
      "Step 2650: train loss: 0.9544537663459778\n",
      "Step 2650: val loss: 40.012596130371094\n",
      "Step 2651: train loss: 0.9540569186210632\n",
      "Step 2651: val loss: 39.99612808227539\n",
      "Step 2652: train loss: 0.9536595344543457\n",
      "Step 2652: val loss: 39.97965621948242\n",
      "Step 2653: train loss: 0.9532632827758789\n",
      "Step 2653: val loss: 39.96318817138672\n",
      "Step 2654: train loss: 0.9528671503067017\n",
      "Step 2654: val loss: 39.94670867919922\n",
      "Step 2655: train loss: 0.9524706602096558\n",
      "Step 2655: val loss: 39.930240631103516\n",
      "Step 2656: train loss: 0.952074408531189\n",
      "Step 2656: val loss: 39.91377639770508\n",
      "Step 2657: train loss: 0.9516787528991699\n",
      "Step 2657: val loss: 39.89730453491211\n",
      "Step 2658: train loss: 0.9512831568717957\n",
      "Step 2658: val loss: 39.880821228027344\n",
      "Step 2659: train loss: 0.9508875608444214\n",
      "Step 2659: val loss: 39.864356994628906\n",
      "Step 2660: train loss: 0.950492262840271\n",
      "Step 2660: val loss: 39.847877502441406\n",
      "Step 2661: train loss: 0.9500975608825684\n",
      "Step 2661: val loss: 39.83140563964844\n",
      "Step 2662: train loss: 0.9497025609016418\n",
      "Step 2662: val loss: 39.81493377685547\n",
      "Step 2663: train loss: 0.9493077397346497\n",
      "Step 2663: val loss: 39.7984619140625\n",
      "Step 2664: train loss: 0.9489130973815918\n",
      "Step 2664: val loss: 39.78199005126953\n",
      "Step 2665: train loss: 0.9485187530517578\n",
      "Step 2665: val loss: 39.7655143737793\n",
      "Step 2666: train loss: 0.948124885559082\n",
      "Step 2666: val loss: 39.74903869628906\n",
      "Step 2667: train loss: 0.9477310180664062\n",
      "Step 2667: val loss: 39.732566833496094\n",
      "Step 2668: train loss: 0.9473369717597961\n",
      "Step 2668: val loss: 39.71609115600586\n",
      "Step 2669: train loss: 0.9469439387321472\n",
      "Step 2669: val loss: 39.69961166381836\n",
      "Step 2670: train loss: 0.9465500712394714\n",
      "Step 2670: val loss: 39.68314743041992\n",
      "Step 2671: train loss: 0.9461572766304016\n",
      "Step 2671: val loss: 39.66667175292969\n",
      "Step 2672: train loss: 0.9457640647888184\n",
      "Step 2672: val loss: 39.65019989013672\n",
      "Step 2673: train loss: 0.9453717470169067\n",
      "Step 2673: val loss: 39.633731842041016\n",
      "Step 2674: train loss: 0.9449790716171265\n",
      "Step 2674: val loss: 39.617252349853516\n",
      "Step 2675: train loss: 0.9445866346359253\n",
      "Step 2675: val loss: 39.600772857666016\n",
      "Step 2676: train loss: 0.9441943168640137\n",
      "Step 2676: val loss: 39.58430862426758\n",
      "Step 2677: train loss: 0.9438024759292603\n",
      "Step 2677: val loss: 39.56783676147461\n",
      "Step 2678: train loss: 0.9434108734130859\n",
      "Step 2678: val loss: 39.551353454589844\n",
      "Step 2679: train loss: 0.9430190920829773\n",
      "Step 2679: val loss: 39.53488540649414\n",
      "Step 2680: train loss: 0.9426279067993164\n",
      "Step 2680: val loss: 39.518402099609375\n",
      "Step 2681: train loss: 0.942236602306366\n",
      "Step 2681: val loss: 39.50193786621094\n",
      "Step 2682: train loss: 0.9418449401855469\n",
      "Step 2682: val loss: 39.485469818115234\n",
      "Step 2683: train loss: 0.9414544105529785\n",
      "Step 2683: val loss: 39.4689826965332\n",
      "Step 2684: train loss: 0.9410638809204102\n",
      "Step 2684: val loss: 39.45252227783203\n",
      "Step 2685: train loss: 0.9406734704971313\n",
      "Step 2685: val loss: 39.43604278564453\n",
      "Step 2686: train loss: 0.9402831196784973\n",
      "Step 2686: val loss: 39.41957092285156\n",
      "Step 2687: train loss: 0.9398928880691528\n",
      "Step 2687: val loss: 39.403106689453125\n",
      "Step 2688: train loss: 0.9395033717155457\n",
      "Step 2688: val loss: 39.38662338256836\n",
      "Step 2689: train loss: 0.939113438129425\n",
      "Step 2689: val loss: 39.370155334472656\n",
      "Step 2690: train loss: 0.9387239217758179\n",
      "Step 2690: val loss: 39.35368728637695\n",
      "Step 2691: train loss: 0.938334584236145\n",
      "Step 2691: val loss: 39.33721923828125\n",
      "Step 2692: train loss: 0.9379453659057617\n",
      "Step 2692: val loss: 39.32074737548828\n",
      "Step 2693: train loss: 0.9375564455986023\n",
      "Step 2693: val loss: 39.30426788330078\n",
      "Step 2694: train loss: 0.9371678233146667\n",
      "Step 2694: val loss: 39.28779220581055\n",
      "Step 2695: train loss: 0.9367790222167969\n",
      "Step 2695: val loss: 39.27132797241211\n",
      "Step 2696: train loss: 0.9363905787467957\n",
      "Step 2696: val loss: 39.254859924316406\n",
      "Step 2697: train loss: 0.9360023736953735\n",
      "Step 2697: val loss: 39.23839569091797\n",
      "Step 2698: train loss: 0.9356142282485962\n",
      "Step 2698: val loss: 39.221920013427734\n",
      "Step 2699: train loss: 0.9352267384529114\n",
      "Step 2699: val loss: 39.2054557800293\n",
      "Step 2700: train loss: 0.934839129447937\n",
      "Step 2700: val loss: 39.18897247314453\n",
      "Step 2701: train loss: 0.9344515800476074\n",
      "Step 2701: val loss: 39.17251968383789\n",
      "Step 2702: train loss: 0.9340645670890808\n",
      "Step 2702: val loss: 39.156036376953125\n",
      "Step 2703: train loss: 0.9336770176887512\n",
      "Step 2703: val loss: 39.13957977294922\n",
      "Step 2704: train loss: 0.9332900047302246\n",
      "Step 2704: val loss: 39.12310791015625\n",
      "Step 2705: train loss: 0.9329036474227905\n",
      "Step 2705: val loss: 39.106632232666016\n",
      "Step 2706: train loss: 0.9325169324874878\n",
      "Step 2706: val loss: 39.09016799926758\n",
      "Step 2707: train loss: 0.9321308135986328\n",
      "Step 2707: val loss: 39.07369613647461\n",
      "Step 2708: train loss: 0.9317446947097778\n",
      "Step 2708: val loss: 39.0572395324707\n",
      "Step 2709: train loss: 0.9313583374023438\n",
      "Step 2709: val loss: 39.04076385498047\n",
      "Step 2710: train loss: 0.93097323179245\n",
      "Step 2710: val loss: 39.0242919921875\n",
      "Step 2711: train loss: 0.9305874109268188\n",
      "Step 2711: val loss: 39.0078239440918\n",
      "Step 2712: train loss: 0.9302020072937012\n",
      "Step 2712: val loss: 38.99137496948242\n",
      "Step 2713: train loss: 0.9298169016838074\n",
      "Step 2713: val loss: 38.97489929199219\n",
      "Step 2714: train loss: 0.9294317364692688\n",
      "Step 2714: val loss: 38.958438873291016\n",
      "Step 2715: train loss: 0.9290469288825989\n",
      "Step 2715: val loss: 38.94196701049805\n",
      "Step 2716: train loss: 0.9286624193191528\n",
      "Step 2716: val loss: 38.92551040649414\n",
      "Step 2717: train loss: 0.9282777905464172\n",
      "Step 2717: val loss: 38.90904235839844\n",
      "Step 2718: train loss: 0.9278936386108398\n",
      "Step 2718: val loss: 38.89258575439453\n",
      "Step 2719: train loss: 0.927509605884552\n",
      "Step 2719: val loss: 38.876121520996094\n",
      "Step 2720: train loss: 0.9271254539489746\n",
      "Step 2720: val loss: 38.85966491699219\n",
      "Step 2721: train loss: 0.926741898059845\n",
      "Step 2721: val loss: 38.84320831298828\n",
      "Step 2722: train loss: 0.9263584017753601\n",
      "Step 2722: val loss: 38.826744079589844\n",
      "Step 2723: train loss: 0.9259751439094543\n",
      "Step 2723: val loss: 38.81028366088867\n",
      "Step 2724: train loss: 0.925591766834259\n",
      "Step 2724: val loss: 38.793827056884766\n",
      "Step 2725: train loss: 0.9252086877822876\n",
      "Step 2725: val loss: 38.777366638183594\n",
      "Step 2726: train loss: 0.9248261451721191\n",
      "Step 2726: val loss: 38.76090621948242\n",
      "Step 2727: train loss: 0.9244437217712402\n",
      "Step 2727: val loss: 38.74443817138672\n",
      "Step 2728: train loss: 0.9240614175796509\n",
      "Step 2728: val loss: 38.727989196777344\n",
      "Step 2729: train loss: 0.9236787557601929\n",
      "Step 2729: val loss: 38.711544036865234\n",
      "Step 2730: train loss: 0.9232972264289856\n",
      "Step 2730: val loss: 38.6950798034668\n",
      "Step 2731: train loss: 0.922914981842041\n",
      "Step 2731: val loss: 38.67862319946289\n",
      "Step 2732: train loss: 0.922533392906189\n",
      "Step 2732: val loss: 38.66217041015625\n",
      "Step 2733: train loss: 0.9221519231796265\n",
      "Step 2733: val loss: 38.64570617675781\n",
      "Step 2734: train loss: 0.9217703938484192\n",
      "Step 2734: val loss: 38.62926483154297\n",
      "Step 2735: train loss: 0.9213892221450806\n",
      "Step 2735: val loss: 38.61280822753906\n",
      "Step 2736: train loss: 0.9210087656974792\n",
      "Step 2736: val loss: 38.59634780883789\n",
      "Step 2737: train loss: 0.920627772808075\n",
      "Step 2737: val loss: 38.57990646362305\n",
      "Step 2738: train loss: 0.9202471971511841\n",
      "Step 2738: val loss: 38.563446044921875\n",
      "Step 2739: train loss: 0.9198670387268066\n",
      "Step 2739: val loss: 38.5469970703125\n",
      "Step 2740: train loss: 0.9194867014884949\n",
      "Step 2740: val loss: 38.530555725097656\n",
      "Step 2741: train loss: 0.9191068410873413\n",
      "Step 2741: val loss: 38.514095306396484\n",
      "Step 2742: train loss: 0.918726921081543\n",
      "Step 2742: val loss: 38.49764633178711\n",
      "Step 2743: train loss: 0.918347179889679\n",
      "Step 2743: val loss: 38.481197357177734\n",
      "Step 2744: train loss: 0.917967677116394\n",
      "Step 2744: val loss: 38.46474838256836\n",
      "Step 2745: train loss: 0.9175884127616882\n",
      "Step 2745: val loss: 38.448299407958984\n",
      "Step 2746: train loss: 0.9172093272209167\n",
      "Step 2746: val loss: 38.43185043334961\n",
      "Step 2747: train loss: 0.9168303608894348\n",
      "Step 2747: val loss: 38.41542053222656\n",
      "Step 2748: train loss: 0.9164519309997559\n",
      "Step 2748: val loss: 38.398963928222656\n",
      "Step 2749: train loss: 0.9160731434822083\n",
      "Step 2749: val loss: 38.38251876831055\n",
      "Step 2750: train loss: 0.9156948328018188\n",
      "Step 2750: val loss: 38.366085052490234\n",
      "Step 2751: train loss: 0.9153167009353638\n",
      "Step 2751: val loss: 38.349639892578125\n",
      "Step 2752: train loss: 0.9149381518363953\n",
      "Step 2752: val loss: 38.33320617675781\n",
      "Step 2753: train loss: 0.9145609140396118\n",
      "Step 2753: val loss: 38.31675720214844\n",
      "Step 2754: train loss: 0.9141831398010254\n",
      "Step 2754: val loss: 38.300323486328125\n",
      "Step 2755: train loss: 0.9138056635856628\n",
      "Step 2755: val loss: 38.283878326416016\n",
      "Step 2756: train loss: 0.9134281873703003\n",
      "Step 2756: val loss: 38.2674446105957\n",
      "Step 2757: train loss: 0.913051426410675\n",
      "Step 2757: val loss: 38.250999450683594\n",
      "Step 2758: train loss: 0.9126741290092468\n",
      "Step 2758: val loss: 38.23456954956055\n",
      "Step 2759: train loss: 0.9122975468635559\n",
      "Step 2759: val loss: 38.2181396484375\n",
      "Step 2760: train loss: 0.9119210243225098\n",
      "Step 2760: val loss: 38.20170211791992\n",
      "Step 2761: train loss: 0.911544680595398\n",
      "Step 2761: val loss: 38.18525695800781\n",
      "Step 2762: train loss: 0.9111684560775757\n",
      "Step 2762: val loss: 38.168819427490234\n",
      "Step 2763: train loss: 0.9107924699783325\n",
      "Step 2763: val loss: 38.15239334106445\n",
      "Step 2764: train loss: 0.9104171991348267\n",
      "Step 2764: val loss: 38.135955810546875\n",
      "Step 2765: train loss: 0.9100410342216492\n",
      "Step 2765: val loss: 38.119537353515625\n",
      "Step 2766: train loss: 0.9096657633781433\n",
      "Step 2766: val loss: 38.103092193603516\n",
      "Step 2767: train loss: 0.9092906713485718\n",
      "Step 2767: val loss: 38.086673736572266\n",
      "Step 2768: train loss: 0.9089155197143555\n",
      "Step 2768: val loss: 38.07025146484375\n",
      "Step 2769: train loss: 0.9085405468940735\n",
      "Step 2769: val loss: 38.0538215637207\n",
      "Step 2770: train loss: 0.9081657528877258\n",
      "Step 2770: val loss: 38.03739929199219\n",
      "Step 2771: train loss: 0.9077911376953125\n",
      "Step 2771: val loss: 38.020973205566406\n",
      "Step 2772: train loss: 0.907416820526123\n",
      "Step 2772: val loss: 38.004554748535156\n",
      "Step 2773: train loss: 0.9070426821708679\n",
      "Step 2773: val loss: 37.98813247680664\n",
      "Step 2774: train loss: 0.9066687822341919\n",
      "Step 2774: val loss: 37.971710205078125\n",
      "Step 2775: train loss: 0.9062945246696472\n",
      "Step 2775: val loss: 37.95528793334961\n",
      "Step 2776: train loss: 0.9059211611747742\n",
      "Step 2776: val loss: 37.938865661621094\n",
      "Step 2777: train loss: 0.9055476188659668\n",
      "Step 2777: val loss: 37.92245864868164\n",
      "Step 2778: train loss: 0.9051740765571594\n",
      "Step 2778: val loss: 37.90604019165039\n",
      "Step 2779: train loss: 0.9048011898994446\n",
      "Step 2779: val loss: 37.88962936401367\n",
      "Step 2780: train loss: 0.9044283628463745\n",
      "Step 2780: val loss: 37.87320327758789\n",
      "Step 2781: train loss: 0.9040554761886597\n",
      "Step 2781: val loss: 37.85679244995117\n",
      "Step 2782: train loss: 0.9036831855773926\n",
      "Step 2782: val loss: 37.840370178222656\n",
      "Step 2783: train loss: 0.9033107757568359\n",
      "Step 2783: val loss: 37.82395935058594\n",
      "Step 2784: train loss: 0.902938723564148\n",
      "Step 2784: val loss: 37.80754852294922\n",
      "Step 2785: train loss: 0.902566134929657\n",
      "Step 2785: val loss: 37.79114532470703\n",
      "Step 2786: train loss: 0.902194619178772\n",
      "Step 2786: val loss: 37.77473831176758\n",
      "Step 2787: train loss: 0.9018229246139526\n",
      "Step 2787: val loss: 37.75832748413086\n",
      "Step 2788: train loss: 0.9014514684677124\n",
      "Step 2788: val loss: 37.74191665649414\n",
      "Step 2789: train loss: 0.9010801315307617\n",
      "Step 2789: val loss: 37.725521087646484\n",
      "Step 2790: train loss: 0.9007092714309692\n",
      "Step 2790: val loss: 37.709110260009766\n",
      "Step 2791: train loss: 0.9003380537033081\n",
      "Step 2791: val loss: 37.692710876464844\n",
      "Step 2792: train loss: 0.8999670147895813\n",
      "Step 2792: val loss: 37.67631530761719\n",
      "Step 2793: train loss: 0.8995965123176575\n",
      "Step 2793: val loss: 37.6599006652832\n",
      "Step 2794: train loss: 0.8992258906364441\n",
      "Step 2794: val loss: 37.64351272583008\n",
      "Step 2795: train loss: 0.898855984210968\n",
      "Step 2795: val loss: 37.627105712890625\n",
      "Step 2796: train loss: 0.8984857797622681\n",
      "Step 2796: val loss: 37.61071014404297\n",
      "Step 2797: train loss: 0.8981159925460815\n",
      "Step 2797: val loss: 37.594322204589844\n",
      "Step 2798: train loss: 0.8977462649345398\n",
      "Step 2798: val loss: 37.57792282104492\n",
      "Step 2799: train loss: 0.8973771929740906\n",
      "Step 2799: val loss: 37.561527252197266\n",
      "Step 2800: train loss: 0.8970073461532593\n",
      "Step 2800: val loss: 37.54513931274414\n",
      "Step 2801: train loss: 0.8966383934020996\n",
      "Step 2801: val loss: 37.528751373291016\n",
      "Step 2802: train loss: 0.8962691426277161\n",
      "Step 2802: val loss: 37.51236343383789\n",
      "Step 2803: train loss: 0.8959004282951355\n",
      "Step 2803: val loss: 37.49597930908203\n",
      "Step 2804: train loss: 0.8955317735671997\n",
      "Step 2804: val loss: 37.47959518432617\n",
      "Step 2805: train loss: 0.8951632380485535\n",
      "Step 2805: val loss: 37.46321105957031\n",
      "Step 2806: train loss: 0.8947951197624207\n",
      "Step 2806: val loss: 37.44681167602539\n",
      "Step 2807: train loss: 0.8944271206855774\n",
      "Step 2807: val loss: 37.4304313659668\n",
      "Step 2808: train loss: 0.8940590620040894\n",
      "Step 2808: val loss: 37.41405487060547\n",
      "Step 2809: train loss: 0.8936908841133118\n",
      "Step 2809: val loss: 37.397674560546875\n",
      "Step 2810: train loss: 0.8933237195014954\n",
      "Step 2810: val loss: 37.38128662109375\n",
      "Step 2811: train loss: 0.8929560780525208\n",
      "Step 2811: val loss: 37.36491394042969\n",
      "Step 2812: train loss: 0.8925889134407043\n",
      "Step 2812: val loss: 37.34852981567383\n",
      "Step 2813: train loss: 0.8922218084335327\n",
      "Step 2813: val loss: 37.332157135009766\n",
      "Step 2814: train loss: 0.8918546438217163\n",
      "Step 2814: val loss: 37.3157844543457\n",
      "Step 2815: train loss: 0.8914886713027954\n",
      "Step 2815: val loss: 37.299407958984375\n",
      "Step 2816: train loss: 0.891122043132782\n",
      "Step 2816: val loss: 37.28303527832031\n",
      "Step 2817: train loss: 0.8907554745674133\n",
      "Step 2817: val loss: 37.26666259765625\n",
      "Step 2818: train loss: 0.8903892636299133\n",
      "Step 2818: val loss: 37.25028991699219\n",
      "Step 2819: train loss: 0.8900232315063477\n",
      "Step 2819: val loss: 37.23392105102539\n",
      "Step 2820: train loss: 0.8896573185920715\n",
      "Step 2820: val loss: 37.217559814453125\n",
      "Step 2821: train loss: 0.8892919421195984\n",
      "Step 2821: val loss: 37.20119857788086\n",
      "Step 2822: train loss: 0.88892662525177\n",
      "Step 2822: val loss: 37.184837341308594\n",
      "Step 2823: train loss: 0.8885608911514282\n",
      "Step 2823: val loss: 37.1684684753418\n",
      "Step 2824: train loss: 0.8881961107254028\n",
      "Step 2824: val loss: 37.1521110534668\n",
      "Step 2825: train loss: 0.8878310918807983\n",
      "Step 2825: val loss: 37.1357536315918\n",
      "Step 2826: train loss: 0.887466549873352\n",
      "Step 2826: val loss: 37.119388580322266\n",
      "Step 2827: train loss: 0.887101948261261\n",
      "Step 2827: val loss: 37.1030387878418\n",
      "Step 2828: train loss: 0.8867371678352356\n",
      "Step 2828: val loss: 37.0866813659668\n",
      "Step 2829: train loss: 0.8863730430603027\n",
      "Step 2829: val loss: 37.0703239440918\n",
      "Step 2830: train loss: 0.8860089182853699\n",
      "Step 2830: val loss: 37.05398178100586\n",
      "Step 2831: train loss: 0.8856452703475952\n",
      "Step 2831: val loss: 37.037628173828125\n",
      "Step 2832: train loss: 0.8852812051773071\n",
      "Step 2832: val loss: 37.02128219604492\n",
      "Step 2833: train loss: 0.8849176168441772\n",
      "Step 2833: val loss: 37.00493240356445\n",
      "Step 2834: train loss: 0.884554386138916\n",
      "Step 2834: val loss: 36.988582611083984\n",
      "Step 2835: train loss: 0.8841913342475891\n",
      "Step 2835: val loss: 36.972232818603516\n",
      "Step 2836: train loss: 0.8838281631469727\n",
      "Step 2836: val loss: 36.95589065551758\n",
      "Step 2837: train loss: 0.8834652900695801\n",
      "Step 2837: val loss: 36.939544677734375\n",
      "Step 2838: train loss: 0.88310307264328\n",
      "Step 2838: val loss: 36.9232177734375\n",
      "Step 2839: train loss: 0.8827401399612427\n",
      "Step 2839: val loss: 36.90687561035156\n",
      "Step 2840: train loss: 0.8823782205581665\n",
      "Step 2840: val loss: 36.890533447265625\n",
      "Step 2841: train loss: 0.8820158243179321\n",
      "Step 2841: val loss: 36.87419509887695\n",
      "Step 2842: train loss: 0.8816537857055664\n",
      "Step 2842: val loss: 36.85786056518555\n",
      "Step 2843: train loss: 0.8812916874885559\n",
      "Step 2843: val loss: 36.84153747558594\n",
      "Step 2844: train loss: 0.8809305429458618\n",
      "Step 2844: val loss: 36.825191497802734\n",
      "Step 2845: train loss: 0.88056880235672\n",
      "Step 2845: val loss: 36.80887222290039\n",
      "Step 2846: train loss: 0.8802076578140259\n",
      "Step 2846: val loss: 36.792537689208984\n",
      "Step 2847: train loss: 0.879846453666687\n",
      "Step 2847: val loss: 36.776214599609375\n",
      "Step 2848: train loss: 0.8794856071472168\n",
      "Step 2848: val loss: 36.759891510009766\n",
      "Step 2849: train loss: 0.8791247606277466\n",
      "Step 2849: val loss: 36.74356460571289\n",
      "Step 2850: train loss: 0.878764271736145\n",
      "Step 2850: val loss: 36.72724533081055\n",
      "Step 2851: train loss: 0.8784035444259644\n",
      "Step 2851: val loss: 36.7109260559082\n",
      "Step 2852: train loss: 0.8780434727668762\n",
      "Step 2852: val loss: 36.694610595703125\n",
      "Step 2853: train loss: 0.8776830434799194\n",
      "Step 2853: val loss: 36.678287506103516\n",
      "Step 2854: train loss: 0.8773234486579895\n",
      "Step 2854: val loss: 36.6619758605957\n",
      "Step 2855: train loss: 0.8769634366035461\n",
      "Step 2855: val loss: 36.64566421508789\n",
      "Step 2856: train loss: 0.8766037821769714\n",
      "Step 2856: val loss: 36.62935256958008\n",
      "Step 2857: train loss: 0.8762446641921997\n",
      "Step 2857: val loss: 36.613033294677734\n",
      "Step 2858: train loss: 0.8758851885795593\n",
      "Step 2858: val loss: 36.596736907958984\n",
      "Step 2859: train loss: 0.8755264282226562\n",
      "Step 2859: val loss: 36.58042526245117\n",
      "Step 2860: train loss: 0.8751676678657532\n",
      "Step 2860: val loss: 36.564117431640625\n",
      "Step 2861: train loss: 0.8748084902763367\n",
      "Step 2861: val loss: 36.547821044921875\n",
      "Step 2862: train loss: 0.8744505047798157\n",
      "Step 2862: val loss: 36.531517028808594\n",
      "Step 2863: train loss: 0.8740921020507812\n",
      "Step 2863: val loss: 36.51520919799805\n",
      "Step 2864: train loss: 0.8737336993217468\n",
      "Step 2864: val loss: 36.49890899658203\n",
      "Step 2865: train loss: 0.8733755946159363\n",
      "Step 2865: val loss: 36.48261642456055\n",
      "Step 2866: train loss: 0.8730182647705078\n",
      "Step 2866: val loss: 36.46632766723633\n",
      "Step 2867: train loss: 0.8726599812507629\n",
      "Step 2867: val loss: 36.450035095214844\n",
      "Step 2868: train loss: 0.8723028898239136\n",
      "Step 2868: val loss: 36.43374252319336\n",
      "Step 2869: train loss: 0.8719453811645508\n",
      "Step 2869: val loss: 36.417449951171875\n",
      "Step 2870: train loss: 0.8715881109237671\n",
      "Step 2870: val loss: 36.40116882324219\n",
      "Step 2871: train loss: 0.871230959892273\n",
      "Step 2871: val loss: 36.38488006591797\n",
      "Step 2872: train loss: 0.8708742260932922\n",
      "Step 2872: val loss: 36.36859893798828\n",
      "Step 2873: train loss: 0.8705180287361145\n",
      "Step 2873: val loss: 36.35231399536133\n",
      "Step 2874: train loss: 0.8701614141464233\n",
      "Step 2874: val loss: 36.33603286743164\n",
      "Step 2875: train loss: 0.8698046803474426\n",
      "Step 2875: val loss: 36.319759368896484\n",
      "Step 2876: train loss: 0.8694489598274231\n",
      "Step 2876: val loss: 36.30348205566406\n",
      "Step 2877: train loss: 0.8690930604934692\n",
      "Step 2877: val loss: 36.28721237182617\n",
      "Step 2878: train loss: 0.8687369227409363\n",
      "Step 2878: val loss: 36.270938873291016\n",
      "Step 2879: train loss: 0.8683812022209167\n",
      "Step 2879: val loss: 36.254669189453125\n",
      "Step 2880: train loss: 0.8680258989334106\n",
      "Step 2880: val loss: 36.2384033203125\n",
      "Step 2881: train loss: 0.8676708340644836\n",
      "Step 2881: val loss: 36.22213363647461\n",
      "Step 2882: train loss: 0.8673155903816223\n",
      "Step 2882: val loss: 36.205875396728516\n",
      "Step 2883: train loss: 0.8669605255126953\n",
      "Step 2883: val loss: 36.18960189819336\n",
      "Step 2884: train loss: 0.8666059374809265\n",
      "Step 2884: val loss: 36.173336029052734\n",
      "Step 2885: train loss: 0.8662512898445129\n",
      "Step 2885: val loss: 36.15708923339844\n",
      "Step 2886: train loss: 0.8658968806266785\n",
      "Step 2886: val loss: 36.14082717895508\n",
      "Step 2887: train loss: 0.8655425906181335\n",
      "Step 2887: val loss: 36.124576568603516\n",
      "Step 2888: train loss: 0.865188479423523\n",
      "Step 2888: val loss: 36.10832214355469\n",
      "Step 2889: train loss: 0.8648343086242676\n",
      "Step 2889: val loss: 36.092071533203125\n",
      "Step 2890: train loss: 0.8644806742668152\n",
      "Step 2890: val loss: 36.07581329345703\n",
      "Step 2891: train loss: 0.8641271591186523\n",
      "Step 2891: val loss: 36.059574127197266\n",
      "Step 2892: train loss: 0.8637735247612\n",
      "Step 2892: val loss: 36.04331970214844\n",
      "Step 2893: train loss: 0.8634206056594849\n",
      "Step 2893: val loss: 36.027076721191406\n",
      "Step 2894: train loss: 0.8630672693252563\n",
      "Step 2894: val loss: 36.010833740234375\n",
      "Step 2895: train loss: 0.8627141714096069\n",
      "Step 2895: val loss: 35.99459457397461\n",
      "Step 2896: train loss: 0.8623612523078918\n",
      "Step 2896: val loss: 35.97835922241211\n",
      "Step 2897: train loss: 0.8620090484619141\n",
      "Step 2897: val loss: 35.962120056152344\n",
      "Step 2898: train loss: 0.8616568446159363\n",
      "Step 2898: val loss: 35.94587707519531\n",
      "Step 2899: train loss: 0.8613042831420898\n",
      "Step 2899: val loss: 35.92965316772461\n",
      "Step 2900: train loss: 0.8609525561332703\n",
      "Step 2900: val loss: 35.913414001464844\n",
      "Step 2901: train loss: 0.8606003522872925\n",
      "Step 2901: val loss: 35.89719009399414\n",
      "Step 2902: train loss: 0.8602482676506042\n",
      "Step 2902: val loss: 35.8809700012207\n",
      "Step 2903: train loss: 0.8598973155021667\n",
      "Step 2903: val loss: 35.864742279052734\n",
      "Step 2904: train loss: 0.8595457077026367\n",
      "Step 2904: val loss: 35.848514556884766\n",
      "Step 2905: train loss: 0.8591944575309753\n",
      "Step 2905: val loss: 35.83230209350586\n",
      "Step 2906: train loss: 0.8588435053825378\n",
      "Step 2906: val loss: 35.81608200073242\n",
      "Step 2907: train loss: 0.858492374420166\n",
      "Step 2907: val loss: 35.799869537353516\n",
      "Step 2908: train loss: 0.8581416010856628\n",
      "Step 2908: val loss: 35.783653259277344\n",
      "Step 2909: train loss: 0.8577909469604492\n",
      "Step 2909: val loss: 35.7674446105957\n",
      "Step 2910: train loss: 0.8574410676956177\n",
      "Step 2910: val loss: 35.7512321472168\n",
      "Step 2911: train loss: 0.8570906519889832\n",
      "Step 2911: val loss: 35.73502731323242\n",
      "Step 2912: train loss: 0.8567407727241516\n",
      "Step 2912: val loss: 35.71881866455078\n",
      "Step 2913: train loss: 0.8563901782035828\n",
      "Step 2913: val loss: 35.70262145996094\n",
      "Step 2914: train loss: 0.8560404777526855\n",
      "Step 2914: val loss: 35.68641662597656\n",
      "Step 2915: train loss: 0.8556910753250122\n",
      "Step 2915: val loss: 35.67021942138672\n",
      "Step 2916: train loss: 0.8553417325019836\n",
      "Step 2916: val loss: 35.65401840209961\n",
      "Step 2917: train loss: 0.8549927473068237\n",
      "Step 2917: val loss: 35.6378173828125\n",
      "Step 2918: train loss: 0.8546436429023743\n",
      "Step 2918: val loss: 35.62162780761719\n",
      "Step 2919: train loss: 0.8542946577072144\n",
      "Step 2919: val loss: 35.605445861816406\n",
      "Step 2920: train loss: 0.8539458513259888\n",
      "Step 2920: val loss: 35.58924102783203\n",
      "Step 2921: train loss: 0.8535971641540527\n",
      "Step 2921: val loss: 35.573062896728516\n",
      "Step 2922: train loss: 0.8532488942146301\n",
      "Step 2922: val loss: 35.5568733215332\n",
      "Step 2923: train loss: 0.8529006838798523\n",
      "Step 2923: val loss: 35.54069137573242\n",
      "Step 2924: train loss: 0.8525525331497192\n",
      "Step 2924: val loss: 35.52452087402344\n",
      "Step 2925: train loss: 0.8522047996520996\n",
      "Step 2925: val loss: 35.50834274291992\n",
      "Step 2926: train loss: 0.8518568873405457\n",
      "Step 2926: val loss: 35.4921760559082\n",
      "Step 2927: train loss: 0.8515089750289917\n",
      "Step 2927: val loss: 35.47600173950195\n",
      "Step 2928: train loss: 0.851161777973175\n",
      "Step 2928: val loss: 35.45982360839844\n",
      "Step 2929: train loss: 0.8508143424987793\n",
      "Step 2929: val loss: 35.44365310668945\n",
      "Step 2930: train loss: 0.8504672050476074\n",
      "Step 2930: val loss: 35.427486419677734\n",
      "Step 2931: train loss: 0.8501204252243042\n",
      "Step 2931: val loss: 35.41132736206055\n",
      "Step 2932: train loss: 0.849774181842804\n",
      "Step 2932: val loss: 35.39515686035156\n",
      "Step 2933: train loss: 0.8494270443916321\n",
      "Step 2933: val loss: 35.37900924682617\n",
      "Step 2934: train loss: 0.8490809202194214\n",
      "Step 2934: val loss: 35.36284255981445\n",
      "Step 2935: train loss: 0.8487343788146973\n",
      "Step 2935: val loss: 35.346683502197266\n",
      "Step 2936: train loss: 0.8483883142471313\n",
      "Step 2936: val loss: 35.330535888671875\n",
      "Step 2937: train loss: 0.8480421304702759\n",
      "Step 2937: val loss: 35.314388275146484\n",
      "Step 2938: train loss: 0.8476966023445129\n",
      "Step 2938: val loss: 35.29823303222656\n",
      "Step 2939: train loss: 0.84735107421875\n",
      "Step 2939: val loss: 35.282081604003906\n",
      "Step 2940: train loss: 0.8470054864883423\n",
      "Step 2940: val loss: 35.26594543457031\n",
      "Step 2941: train loss: 0.846660315990448\n",
      "Step 2941: val loss: 35.24980163574219\n",
      "Step 2942: train loss: 0.8463152647018433\n",
      "Step 2942: val loss: 35.233665466308594\n",
      "Step 2943: train loss: 0.8459699749946594\n",
      "Step 2943: val loss: 35.217525482177734\n",
      "Step 2944: train loss: 0.8456256985664368\n",
      "Step 2944: val loss: 35.20138931274414\n",
      "Step 2945: train loss: 0.8452804684638977\n",
      "Step 2945: val loss: 35.18526077270508\n",
      "Step 2946: train loss: 0.8449364900588989\n",
      "Step 2946: val loss: 35.169124603271484\n",
      "Step 2947: train loss: 0.8445917963981628\n",
      "Step 2947: val loss: 35.15299987792969\n",
      "Step 2948: train loss: 0.8442476987838745\n",
      "Step 2948: val loss: 35.136871337890625\n",
      "Step 2949: train loss: 0.8439038991928101\n",
      "Step 2949: val loss: 35.120750427246094\n",
      "Step 2950: train loss: 0.8435599207878113\n",
      "Step 2950: val loss: 35.1046257019043\n",
      "Step 2951: train loss: 0.8432163000106812\n",
      "Step 2951: val loss: 35.08851623535156\n",
      "Step 2952: train loss: 0.8428727984428406\n",
      "Step 2952: val loss: 35.07240295410156\n",
      "Step 2953: train loss: 0.8425295948982239\n",
      "Step 2953: val loss: 35.05628204345703\n",
      "Step 2954: train loss: 0.8421865701675415\n",
      "Step 2954: val loss: 35.040164947509766\n",
      "Step 2955: train loss: 0.8418434262275696\n",
      "Step 2955: val loss: 35.02406311035156\n",
      "Step 2956: train loss: 0.8415005803108215\n",
      "Step 2956: val loss: 35.007957458496094\n",
      "Step 2957: train loss: 0.8411579132080078\n",
      "Step 2957: val loss: 34.991844177246094\n",
      "Step 2958: train loss: 0.840815544128418\n",
      "Step 2958: val loss: 34.97574234008789\n",
      "Step 2959: train loss: 0.8404731750488281\n",
      "Step 2959: val loss: 34.95964431762695\n",
      "Step 2960: train loss: 0.8401311039924622\n",
      "Step 2960: val loss: 34.94355392456055\n",
      "Step 2961: train loss: 0.8397890329360962\n",
      "Step 2961: val loss: 34.927459716796875\n",
      "Step 2962: train loss: 0.8394469022750854\n",
      "Step 2962: val loss: 34.91136932373047\n",
      "Step 2963: train loss: 0.839105486869812\n",
      "Step 2963: val loss: 34.89528274536133\n",
      "Step 2964: train loss: 0.8387638330459595\n",
      "Step 2964: val loss: 34.87919616699219\n",
      "Step 2965: train loss: 0.8384225964546204\n",
      "Step 2965: val loss: 34.86310958862305\n",
      "Step 2966: train loss: 0.8380812406539917\n",
      "Step 2966: val loss: 34.84702682495117\n",
      "Step 2967: train loss: 0.8377397656440735\n",
      "Step 2967: val loss: 34.830955505371094\n",
      "Step 2968: train loss: 0.8373994827270508\n",
      "Step 2968: val loss: 34.81486892700195\n",
      "Step 2969: train loss: 0.837058424949646\n",
      "Step 2969: val loss: 34.79879379272461\n",
      "Step 2970: train loss: 0.836717963218689\n",
      "Step 2970: val loss: 34.7827262878418\n",
      "Step 2971: train loss: 0.8363775014877319\n",
      "Step 2971: val loss: 34.766658782958984\n",
      "Step 2972: train loss: 0.8360376358032227\n",
      "Step 2972: val loss: 34.750579833984375\n",
      "Step 2973: train loss: 0.8356972932815552\n",
      "Step 2973: val loss: 34.73452377319336\n",
      "Step 2974: train loss: 0.8353573083877563\n",
      "Step 2974: val loss: 34.71845626831055\n",
      "Step 2975: train loss: 0.8350175619125366\n",
      "Step 2975: val loss: 34.702396392822266\n",
      "Step 2976: train loss: 0.8346778154373169\n",
      "Step 2976: val loss: 34.68634033203125\n",
      "Step 2977: train loss: 0.8343383073806763\n",
      "Step 2977: val loss: 34.670284271240234\n",
      "Step 2978: train loss: 0.8339993357658386\n",
      "Step 2978: val loss: 34.65423583984375\n",
      "Step 2979: train loss: 0.8336600065231323\n",
      "Step 2979: val loss: 34.638179779052734\n",
      "Step 2980: train loss: 0.833321213722229\n",
      "Step 2980: val loss: 34.622135162353516\n",
      "Step 2981: train loss: 0.8329826593399048\n",
      "Step 2981: val loss: 34.60608673095703\n",
      "Step 2982: train loss: 0.8326438069343567\n",
      "Step 2982: val loss: 34.590057373046875\n",
      "Step 2983: train loss: 0.8323053121566772\n",
      "Step 2983: val loss: 34.57401657104492\n",
      "Step 2984: train loss: 0.8319669961929321\n",
      "Step 2984: val loss: 34.5579719543457\n",
      "Step 2985: train loss: 0.8316290974617004\n",
      "Step 2985: val loss: 34.54193878173828\n",
      "Step 2986: train loss: 0.8312913775444031\n",
      "Step 2986: val loss: 34.52589416503906\n",
      "Step 2987: train loss: 0.8309531211853027\n",
      "Step 2987: val loss: 34.5098762512207\n",
      "Step 2988: train loss: 0.8306156396865845\n",
      "Step 2988: val loss: 34.49384689331055\n",
      "Step 2989: train loss: 0.8302780389785767\n",
      "Step 2989: val loss: 34.47782516479492\n",
      "Step 2990: train loss: 0.8299406170845032\n",
      "Step 2990: val loss: 34.46180725097656\n",
      "Step 2991: train loss: 0.8296033143997192\n",
      "Step 2991: val loss: 34.44578170776367\n",
      "Step 2992: train loss: 0.8292664289474487\n",
      "Step 2992: val loss: 34.42975616455078\n",
      "Step 2993: train loss: 0.8289293050765991\n",
      "Step 2993: val loss: 34.413753509521484\n",
      "Step 2994: train loss: 0.8285927772521973\n",
      "Step 2994: val loss: 34.39773941040039\n",
      "Step 2995: train loss: 0.8282564282417297\n",
      "Step 2995: val loss: 34.3817253112793\n",
      "Step 2996: train loss: 0.8279200792312622\n",
      "Step 2996: val loss: 34.365726470947266\n",
      "Step 2997: train loss: 0.8275836706161499\n",
      "Step 2997: val loss: 34.34972381591797\n",
      "Step 2998: train loss: 0.8272472620010376\n",
      "Step 2998: val loss: 34.33372116088867\n",
      "Step 2999: train loss: 0.8269115686416626\n",
      "Step 2999: val loss: 34.31772232055664\n",
      "Initial weight values: Parameter containing:\n",
      "tensor([[ 0.5878, -2.7377,  0.4842,  0.3327,  0.3814]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAG2CAYAAACZEEfAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAavhJREFUeJzt3Xl8VOXZ//HPmYHEsA2ELSEkEKKAyiJEwIAiCC5gQQQrYlusj1K1iCK2Cra1LrVB21pc0Fq1Wn+PCD4KuIILi8giIogEEZQYBEJAwjLBEALJnN8fwwyZmTPZSGbL9/16zQvmnDMzFwFmrrnv674vwzRNExEREZEYZQt3ACIiIiL1ScmOiIiIxDQlOyIiIhLTlOyIiIhITFOyIyIiIjFNyY6IiIjENCU7IiIiEtMahTuASOByudizZw/NmzfHMIxwhyMiIiLVYJomR44coUOHDthswcdvlOwAe/bsITU1NdxhiIiISC3s2rWLjh07Bj2vZAdo3rw54P5htWjRIszRiIiISHUUFRWRmprq/RwPRskOeKeuWrRooWRHREQkylRVgqICZREREYlpSnZEREQkpinZERERkZimZEdERERimpIdERERiWlKdkRERCSmKdkRERGRmKZkR0RERGKakh0RERGJaUp2REREJKYp2REREZGYpmRHRERE6o8zH/JWuH8NEzUCFRERkfqx4RV4504wXWDYYNQT0HdiyMPQyI6IiIjUPWf+qUQH3L++MzUsIzxKdkRERKTuHcw9leh4mOVw8PuQh6JkR0REROpeYoZ76qoiww6JXUIeipIdERERqXuOFHeNjmF33zfsMGqW+3iIRXSy8+yzz9KrVy9atGhBixYtyMrKYtGiRd7zQ4YMwTAMn9utt94axoj9REAFuoiISNj0nQhTc+CGd92/hqE4GSJ8NVbHjh2ZOXMmZ511FqZp8t///perrrqKL7/8knPPPReASZMm8dBDD3kf06RJk3CF62vDK/D2HYAJGDD6ybD9JYuIiISNIyUsozkVRXSyM2rUKJ/7jzzyCM8++yyfffaZN9lp0qQJSUlJ4QgvOGd+hUQH969v3wEZw8L+Fy4iItLQRPQ0VkXl5eXMnTuX4uJisrKyvMdfffVV2rRpQ48ePZgxYwZHjx6t8rlKS0spKiryudWpXWs5leh4mLDr87p9HRERkUgWIeUcET2yA5CTk0NWVhbHjh2jWbNmLFiwgHPOOQeA66+/nk6dOtGhQwc2bdrEvffey7Zt25g/f36lz5mdnc2DDz4YivBFREQapgjZUBDAME3Tfwgiohw/fpydO3fidDp54403eOGFF/jkk0+8CU9FS5cuZdiwYWzfvp2MjIygz1laWkppaan3flFREampqTidTlq0aHH6QTvz4Z/n4jO6Yxgw9WtNY4mISOxz5sOsHr777Bh2d5FyHX4OFhUV4XA4qvz8jvhprLi4OM4880wyMzPJzs6md+/ePPHEE5bXDhgwAIDt27dX+pzx8fHeFV6eW51ypLgLkr37CxjQ99d1+xoiIiKRKoI2FIQoSHb8uVwun1GZijZu3AhAcnJyCCMKou9EGP7AyTsmrH/JPdqz4ZVwRiUiIlL/ImhDQYjwmp0ZM2YwYsQI0tLSOHLkCHPmzGH58uV88MEH5ObmMmfOHEaOHEnr1q3ZtGkTd911F4MHD6ZXr17hDt09hPfRn/0Omu75S63KEhGRWObZUPCdqe4RnTBuKAgRnuz8+OOPTJw4kYKCAhwOB7169eKDDz7g0ksvZdeuXXz88cfMmjWL4uJiUlNTGTduHH/84x/DHbbbwVwCV2ThHtY7+L2SHRERiW19J7q/3B/83j2iE8bPvYgvUA6F6hY41YhVkTK4h/WmblayIyIicppipkA5anmKlDFOHTMM97CeEh0REYllEbK/jkdET2NFPc8QnmczwdT+SnRERCS2RWC7JCU79c2RAo6rwx2FiIhI/YvQdkmaxhIREZG6EaHtkpTsiIiISN04erBmx0NEyY6IiIjUjX2bLQ8folmIA/GlZEdEREROnzPf3S3Aj8uEKxeUMm/dzjAE5aZkJ5QibCmeiIhIndm11vLwnPJh7DFbM2N+DgXOkhAH5aZkJ1Q2vOLuAPvfUe5f1SNLREQagNWucwH3CM9Lq/LCEoOSnVBw5rt7Ynk6wJou932N8IiISKxIHYDPRrqAyzTY4DrLe/+FFXlhGd1RshMKlq3uXbD2X+GJR0REpK55Ogec7Hbuwsb0spvZS2vvJS5gR+HRkIemTQVDwdPq3j/hWf00DLhVuyqLiEhsqND8c3/jDvzf01t9TtsNg85tmoQ8LI3shIIjBbImW5w42QFdREQkVjhSIP0i2nfMYOa4ntgN99SW3TD469geJDsSQh6SRnZCZcBt7pGcijtLGjZ323sREZEYNL5fGoO7tmVH4VE6t2kSlkQHNLITOt65zArFW6YJuUvCF5OIiEgdK3CWsDq30FuInOxIICujddgSHdDITmhlDPNrGWLCO1PD3iBNRESkLsxbt5MZ83Nwme51WdNHdOeWizPCHZZGdkLqYC4BDdLMctXtiIhI1CtwlngTHXB/2mUv2spzK3LDGhco2Qmtxk2DHA99ZbqIiEhdyiss9iY6FT26aGvYdk72ULITSieKrY9vWRjSMEREROpaRryTLNvXJHHA57jLDM/eOhUp2QmlxAz8d5cEYPVT2k1ZRESi16onaP9CJq/FPcKq+Du41r7Meypce+tUpGQnlBwpkPlrixMm7Po81NGIiIicvlVPwkf346lJtRsmf230IkkcwGYQtr11KlKyE2rte1gfP3owtHGIiIicLmc+fHx/wOFGhovOtn2YFjU84aBkJ9SaJNbsuIiISKQ6mItVRlNuGuxwtccE7pu/WQXKDY5FV1gAWqaFPBQREZHT4un9WIFpwsyy67wNQMtNUwXKDY5nJ2V/L1wCG14JfTwiIiK15UiBUU/gOpnwlJsGfy27nufLR3kvsRmEvUBZOyiHQ7tzrY+/PUW7KYuISFQpyPg5445BmrGPHa723hEdj5sv7BL2AmUlO+Gwc03wc7s+B8fVoYtFRETkNOQVFrPHbM0es3XAORtw44WdQx6TP01jhUNaVvBzWpUlIiJRJL1NU2wWpagGcO+I7mEf1QElO+HRMRNSzg93FCIiIqct2ZFA9tie2A13xmOcvJnAo4u3Mm/dznCGByjZCZ+sydbHtQRdRESizPh+aaycPpSnJ/TBME61vHaZWnresAVbgu7cFfJQRERETleyI4HEZnEBzUC19Lwhc6TApQ8GHv/ofvXJEhGRqGRVv6PeWFV49tln6dWrFy1atKBFixZkZWWxaNEi7/ljx44xefJkWrduTbNmzRg3bhz79u0LY8Q15Ei1OKg+WSIiEiWc+ZC3gn27c1mdWwjgU79jN4yI6I0V0UvPO3bsyMyZMznrrLMwTZP//ve/XHXVVXz55Zece+653HXXXbz33nv83//9Hw6Hg9tvv52xY8eyatWqcIcuIiIS2za8Au/cCaaLNqbBP8pu5g3XULLH9mTl9KHsKDxK5zZNwp7oABimGSltuqonMTGRv/3tb1xzzTW0bduWOXPmcM011wCwdetWzj77bNasWcMFF1xQ7ecsKirC4XDgdDpp0aJFfYUeyJkP/zyXU6VcgGHA1K+1saCIiEQuZz7M6gGmy3uozLRxYekT7DfasHL60JAkOdX9/I7oaayKysvLmTt3LsXFxWRlZbF+/XpOnDjB8OHDvdd0796dtLQ01qypZNM+oLS0lKKiIp9bWHhaR3j6ihg2GPWkEh0REYlsB3N9Eh041ek8EgqS/UX0NBZATk4OWVlZHDt2jGbNmrFgwQLOOeccNm7cSFxcHC1btvS5vn379uzdu7fS58zOzubBBy2Kg8Oh70R3i4iD30NiFyU6IiIS+TwNQP1Gdna42kdEQbK/iB/Z6datGxs3bmTt2rXcdttt3HDDDWzZsuW0nnPGjBk4nU7vbdeuMC/3dqRA+kVwZC+sfhp2rw9vPCIiIpVxpMDwB/CkEWWmjfvKbmK/0SYiCpL9RfzITlxcHGeeeSYAmZmZrFu3jieeeILx48dz/PhxDh8+7DO6s2/fPpKSkip9zvj4eOLj4+sz7JpbcBt8NefU/XPGwLX/DVs4IiIiQW14BT5+AHCBYVB80R+5uvOvuStCCpL9RfzIjj+Xy0VpaSmZmZk0btyYJUuWeM9t27aNnTt3kpVVSe+pSLR7vW+iA7BlISx5KCzhiIiIBOXM967CAsA0cax8hKw2xyIy0YEIH9mZMWMGI0aMIC0tjSNHjjBnzhyWL1/OBx98gMPh4KabbmLatGkkJibSokULpkyZQlZWVo1WYkWEbxdbH//0H3D+TarjERGRyPH2lIDiZMxyd+1phH5eRXSy8+OPPzJx4kQKCgpwOBz06tWLDz74gEsvvRSAf/7zn9hsNsaNG0dpaSmXX345zzzzTJijroVm7YOf27YY+t8UulhERESC2b0ecpcEHHZhw5bYJQwBVU9EJzsvvvhipefPOOMMZs+ezezZs0MUUT3pNgLev9v63IHtoY1FREQkmJ3WW7u8W9afgo3HuOXiEMdTTVFXsxOTHCnQ81rrc63PDG0sIiIiwaRl4b8TsWnC82UjeXTR1rB3Nw9GyU6kGP6A9fETxSENQ0REJKiOmZSccy2e3gumCW+UX0QOZ+KCiNtM0EPJTqRwpMClFquv1AVdREQiSJNrn2fB+a/w0IlfMrr0IX5fdhsQGd3Ng1GyE0nUBV1ERKLA2FFX0f6yaXyNu9QiUrqbBxPRBcoiIiISYZz5cDCXW87LYPR5l0RUd/NglOxEktQBgAH+5V/OneGIRkRExNeGV05tKGjYSB71BMl9J4Y7qippGiuSBKvb+fhB1e2IiEh4OfPh7Tsq7Jzswnz7jqj4fFKyE2k6nBd4zLMzpYiISLjsWov/zIOByarl74cnnhpQshNpEjPAsPhr2bMh9LGIiIhU4dW1OyN2fx0PJTuRxpECwx8MPK6pLBERCSfnroANBctNgw2us9jww6GwhFRdSnYikaayREQkkjjz4eMHMCocMk2YWTaBvbT2bjIYqZTsRCLLqSwbRHCTNRERiWEHcwM6nRsG5JhdMIDMzq3CE1c1KdmJRI4UGPUE+ObQlp1mRURE6l3jppY9sYpdcUwf2T2i99gBJTuRK2OYO232MuGdqarbERGRkDtw+KDP129wf0Q1tR2nV0rLcIRUI0p2IpXFkKHqdkREJNTmrdvJqFcLKDd9050y08YuMyli+2FVpGQnUmkJuoiIhFmBs4QZ83NwmfBC+UhvwlNm2vhj2c3cMfbiiJ/CArWLiFyeJegf/cn3+McPQo9r3OdFRETqUV5hMdfYlpHd6AXshkm5Cf8quxLHxVO4s/95UZHogEZ2IpuWoIuISBhlxDu9iQ6A3YCb7Yu45Ox2UZPogJKdyGY1lWXYtQRdRERCov2JfG+i49HIcNHocF6YIqodJTuRzLME3bC77xt2GDVLU1giIhIaFl+6y0wbo17dw7x1O8MUVM2pZifS9Z3oXoZ+8Hv3iI4SHRERCRVHCvS6DvOrORi499ZZUD6IPWZrps/PYXDXtlExnaWRnWjgSIH0i5ToiIhIaDnzMb+a691jxzDgavsqkjiAaRLxPbE8lOxEE2c+5K3QxoIiIhISB3ZtwcB3z7dGhovOtn0AEd8Ty0PTWNFiwyvwzp3ujQYNm7uWp+/EcEclIiIxLM+VREvT8ClSLjNt7HC1ByDfWRKu0GpEIzvRwJl/KtEB969qHSEiIvUspdOZ/KHsZspMd7pQZtq4r+wm9tIagMcWbaMgChIejexEg8paR6iOR0RE6kmyI4E+Y+7g4vm9STX2ssPV3pvoAJSbJjsKj0Z8kbKSnWjgWfpXMeHRfjsiIlLfnPmMb7ODIZMH8sXBBG6f86XPabthqDeW1BHttyMiIqG24RWY1QP+O4r2L57PlWUfM3NcT+yGe22W3TD469geET+qA2CYZrTUUtefoqIiHA4HTqeTFi1ahDuc4Jz52m9HRETqnzPfnej4zyhMzaGARHYUHqVzmyZhT3Sq+/mtaaxo4khRkiMiIvWvklrR5PSUsCc5NaVprGilPXdERKS+7Pky8FgU14pqZCcaac8dERGpL858+PiBwOPDH4ja2YWIHtnJzs6mX79+NG/enHbt2jFmzBi2bdvmc82QIUMwDMPnduutt4Yp4hDQnjsiIlKfrKawADr0CX0sdSSik51PPvmEyZMn89lnn/HRRx9x4sQJLrvsMoqLi32umzRpEgUFBd7bY489FqaIQ6CyPXdEREROV2IGpl964DJsUTuFBRE+jbV48WKf+y+//DLt2rVj/fr1DB482Hu8SZMmJCUlVft5S0tLKS0t9d4vKio6/WBDRXvuiIhIPSogkSdO3MRfGr1II8NFmWnjjydu4k4SSQ53cLUU0SM7/pxOJwCJiYk+x1999VXatGlDjx49mDFjBkePHq30ebKzs3E4HN5bampqvcVc5wL23LFF9TyqiIhElrzCYuaWD+XC0ie47vgfubD0CeaWD+WllTvCHVqtRc0+Oy6Xi9GjR3P48GFWrlzpPf7vf/+bTp060aFDBzZt2sS9995L//79mT9/ftDnshrZSU1Njfx9dipa9QR89GfABAy49EEYdGe4oxIRkShX4CxhYPZS/JMDmwGrpl8SUcvOY26fncmTJ7N582afRAfgN7/5jff3PXv2JDk5mWHDhpGbm0tGRoblc8XHxxMfH1+v8dYrb6W855+iCR/dDxgw6I7wxSUiIlEv2ZHApIvS+feneT7HXSZR0QfLSlRMY91+++28++67LFu2jI4dO1Z67YABAwDYvn17KEILj2CV8h//WauyRESk1gqcJazOLeTKXsnYDN9z0dIHy0pEj+yYpsmUKVNYsGABy5cvJz09vcrHbNy4EYDk5Ggto6qGxAzAAP9BRtOlTugiIlIr89btZMb8HFyme8rq6j4pLPxyD+WmGVV9sKxEdLIzefJk5syZw1tvvUXz5s3Zu3cvAA6Hg4SEBHJzc5kzZw4jR46kdevWbNq0ibvuuovBgwfTq1evMEdfjxwp7hqdj+73OxHdSwNFRCQ8Cpwl3kQH3FNWC7/cw/zfZnH0uCsi+mCdjoiexnr22WdxOp0MGTKE5ORk723evHkAxMXF8fHHH3PZZZfRvXt37r77bsaNG8c777wT5shDYNCdcOnDuEd4PEzIXRKuiEREJErlFRZ7Ex2PctPk6HEXWRmtozrRgQgf2alqoVhqaiqffPJJiKKJQD3Guet0zAqFyu9MhYxhmsoSEZFqS2/TFJuBT8ITzTU6/iJ6ZEeqoN2URUSkDiQ7Esge25MOHCDL9jUdOBDVNTr+InpkR6pguZuy6nZERKTm0nfO59P4B7AbJuWmwfqdD0C/qWGOqm5oZCeaeXdTrlC3Y6puR0REambf7lwyN7kTHQC7YdJ304Ps250b3sDqiJKdaJcxzG8Fuqku6CIiUiP7f9jiTXQ8GhkuCn/YGqaI6paSnWh3MJfA/XZUtyMiItXXaf8y/NcEmSYkNSkLT0B1TMlOtPPU7VSkLugiIlJdznyabXzRpyIC3BUSjQ7lWT8myijZiXYBXdDtMGqWlp6LiEj17FqLYXHYNOGHJrGxQa9WY8WCvhPdtTsHv3eP6CjRERGRasrJP0xPi+Ofuc6m89lZIY+nPijZiRWOFCU5IiJSI/PW7eR/lx3h7fjAhb2fdrqDrBjZZ0fTWLFo93pY/bT7VxEREQueflhptv2W9To/P6s8PIHVA43sxJoFt8FXc07d7309XP1s+OIREZGI5OmHZVoV7ADprZuFNqB6pJGdWLJ7vW+iA+77GuERERE/nn5YG1xdcfllPC7TYF/L2ChOBiU7seWLl4IcfzmkYYiISOTz9MMygOfLR1J+MuEpNw2ml93MFwdjo14HNI0VW445rY/v/Sq0cYiISFQYb1/Oz8+4Exsuyk34V9mVvFx2BXtpzWCzyodHDY3sxJI+v7A+vneT2keIiIiPfbtzMd92JzoAdgNuti8CwAAyO7cKY3R1S8lOLOl2BbQ92+KEqfYRIiLiNW/dTu565k2Mk4mORyPDRbptHzPH9SQ5Rpadg5Kd2PPLN8FqL8w9G0IeioiIRB7PkvPvXUneOh0P07Az67djGd8vLUzR1Q8lO7HGkQKXPhR4/OMHNJUlIiLeJed7ac2MspspM92pgGnYMUbNon3HjDBHWPeU7MSiDucFHjNdsPZfIQ9FREQii2fJOcDr5UO5sPQJrj/+J368aZ27/VAMUrITi6w6oYN7V2WN7oiINGieJef2k9sm7zfacNXV18bkiI6Hlp7HIkcKZE2G1U/5nXC5C5XVQ0tEpEEb3y+NIcknKPzhG9p06k77jrFVo+NPIzuxasBtBBQqG3Z3V3QREWnYNrxC+xfP59yPfkH7F8+HDa+EO6J6pWQnVjlSYPST7gQH3L+OmqVRHRGRhs6ZD+/c6a7lBPev70yN6TIHTWPFsr4TIWOYe+oqsYsSHRERgYO5pxIdD7M8pssclOzEOkdKzP7jFRGRmvvwuyNcaoLhvyVb4yZhiScUNI3VkDjzIW9FTA9ViohIcAXOEl5e/nVgogNw4mjI4wkVjew0FBteOTVHa9hg1BMxu5+CiIhYyyssPrlzsrsXlocLG7YYXsCikZ2GoAEWo4mISKCmcXYutm/CqLBat9w0uO/ETRSQGMbI6peSnYagsmI0ERFpEOat28ltz7zDXxu9gM0wfc4tL+/FjsLYncZSstMQNG4a5HjsFqOJiMgpnuafnYy92P0SHbth0sX2I53bxO5ngpKdhuBEsfXxLQtDGoaIiISHp/lnnkWn8zLTxnVXDCbZkRCm6Oqfkp2GIDGDgN2UAdaoV5aISEPgaf7p3+ncZdj46bK/M3pw/zBHWL8iOtnJzs6mX79+NG/enHbt2jFmzBi2bdvmc82xY8eYPHkyrVu3plmzZowbN459+/aFKeII5UiBgbcHHjddqtsREWkAKjb/fL18KBcff5KlF/wH29TNtBx0U7jDq3cRnex88sknTJ48mc8++4yPPvqIEydOcNlll1FcfGpa5q677uKdd97h//7v//jkk0/Ys2cPY8eODWPUEWrAbYGd0A2b6nZERBqAr3YdoujYCf57TQfeG2Uyf/JALrliXIPZdNYwTdOs+rLIsH//ftq1a8cnn3zC4MGDcTqdtG3bljlz5nDNNdcAsHXrVs4++2zWrFnDBRdcYPk8paWllJaWeu8XFRWRmpqK0+mkRYsWIfmzhMWGV9xLzs3yU8e0546ISEy7+/WNvLkhn2vty8hu9Dx2A0wMjNFPRv17f1FREQ6Ho8rP74ge2fHndDoBSEx07wWwfv16Tpw4wfDhw73XdO/enbS0NNasWRP0ebKzs3E4HN5bampq/QYeKfpOhJs+wqd+R3vuiIjErK92HeLNDfkkcYCZJxMdAAMT19t3NJj3/qhJdlwuF1OnTmXQoEH06NEDgL179xIXF0fLli19rm3fvj179+4N+lwzZszA6XR6b7t27arP0CPLiWLAbzBPe+6IiMSkz3ccBCDT9i02v3UqNkwObVsZhqhCL2raRUyePJnNmzezcuXp/8XEx8cTHx9fB1FFocQM99RVxU0GDbu7K7qIiMSU/p3dMyEOfrI8vys/n1ahDChMomJk5/bbb+fdd99l2bJldOzY0Xs8KSmJ48ePc/jwYZ/r9+3bR1JSUoijjBKOFHeNjmF33zfsMGpWgylSExFpSHqntmJc3xQO08zy/Im4lqENKEwiemTHNE2mTJnCggULWL58Oenp6T7nMzMzady4MUuWLGHcuHEAbNu2jZ07d5KVlRWOkKND34mQMcw9dZXYRYmOiEiMKnCWMKRbWwo5D9fX+ExluUyDjr0vDl9wIRTRyc7kyZOZM2cOb731Fs2bN/fW4TgcDhISEnA4HNx0001MmzaNxMREWrRowZQpU8jKygq6EktOcqQoyRERiWHz1u1k+ps5/Ny+jOxGL2AzwDTBMNzNP9f3eoD+HTPCHWZIRPTSc8Ow2PUXeOmll/j1r38NuDcVvPvuu3nttdcoLS3l8ssv55lnnqnRNFZ1l67FLGe+u1loYoYSIBGRGFDgLGFg9lLac4BV8Xf49MNyYXBgwvu07TYwjBHWjep+fkf0yE518rAzzjiD2bNnM3v27BBEFIM2vALv3OkuWNaeOyIiMSGvsBgTSLcFNv60YdI2rtz6gTEqKgqUpZ44808lOqA9d0REYkR6m6YYuBt/uvwaf5oYDW4FrpKdhuxgru8SdNCeOyIiMWLSRZ5FPb4jO9YFIrEtoqexpJ5pzx0RkZgzb91OZszPwWXCjEaLAzYTBNP9pbYB1WhqZKch0547IiIxpcBZ4k10kjjAJPv7FlfZGtyXWo3sNHTac0dEJGbkFRbjOjlrdWOjxdgMi4U+A29vcO/1SnbEd8+d3eth5xpIy4KOmeGNS0REaiS9TVNsBrQzD3Cz/b2A8yY2jAG3hiGy8FKyI6csuA2+mnPqfu/r4epnwxePiIjU2KAz21CW+7W3w3lFxZm30KyBjeqAanbEY/d630QH3Pd3rw9PPCIiUiPz1u1k0MylfPpdYZAl59Bs8JTwBBdmSnbEbeca6+O7PgttHCIiUmMVC5NP8V9y3hAXnbsp2RG3tCCNU48eDG0cIiJSYxULk8G9c3LQJecNkJIdceuYCedcFXj8039oR2URkQjnKUz2yHMlUe43jdWQ91FTsiOnnDPG4qAJK/4W6khERKQGkh0JZI/t6XPshfKRlJ1MeMwGvo+akh2p2vqXNbojIhLhPs9zlx1ca1/Gqvg7uKXRexjAv8t+xo83rWvQTZ6V7MgpqQOCnGi487wiItHgq12HeHNDPkkcILvRC95O53bD5ObG79O++RlhjjC8lOzIKY4UuPRhixM2aNwk5OGIiEjVCpwlvLwqD3AXJtv9dk22ma4G/4VVyY74GnQHXPoQvv80XPDicNjwSriiEhERC/PW7WRg9lIWbCwAIN51DNOiQwTHi0MbWIRRsiOBBt0JN38MRoVKftMF70xV7Y6ISIQocJYw/c0cn910Mux7fd66vQ7mhiqsiKRkR6ydKCbg64FZ3uCHQkVEIkVeYTH+gzi55UmBb90AqReEKKrIpGRHrDVuan38++UhDUNERKylt/F9n77Wvoz/xP/Dd1DehJJzrm3wjZ2V7Ii1E0Hmdz99XFNZIiIR4MeiY97fJ3GAmY2eD9w12TBocvkDIY0rEtU62Vm8eHFdxiGRJjED64lfVfWLiESCz3ecaudziW2DRXsIMLR1CHAayc7IkSPp1q0bTzzxBEVFRXUZk0QCRwoMf8j6nKayRETCrmVCY+/v2xmHg1xlNNgWERXVOtk5++yz+e6775g2bRopKSncdtttbN68uS5jk3AbdAdcdHfg8U//DqueCH08IiICuJec3/Nmjvf+kvI+1kvOL7q7wbaIqKjWyc7XX3/N0qVLufrqqyktLeW5556jd+/eDB06lDfffBOXy1WXcUq4dBliffyjB1S7IyISBgXOEmbMz/FJbnI4kzfKL/JNeM4eA8P+FOrwItJpFSgPGTKEN954g7y8PP7whz/Qrl07PvnkE6699lo6derEI488wo8//lhXsUo4qHZHRCSi5BUW47IYxfl92W1cVfoQzsEPws1LYfx/Qx9chKqT1VgpKSk8/PDD7Ny5k1dffZULLriA/Px87r//ftLS0vjVr37F2rVr6+KlJNSC1e4Yds0Di4iEgf+Sc3Cvxsqyfc2PtGJLp181+KXm/up06Xnjxo2ZMGECn3zyCdOnT8c0TY4fP86rr77KwIEDueiii5T0RKOAFhI2yPptOCMSEWmwkh0J/Kxnkve+p8v5a3GPsCr+Ds7euzB8wUWoOk129u3bx8MPP0x6ejqPPvooAH369OHee+8lNTWVVatWceGFF/L222/X5ctKKAy6E+7aDAPvAANY/RTMOhdWPRnuyEREGpxJg90j6+79dXy7nLf8+PeqqfRTJ8nO6tWruf766+nUqRMPPPAAe/fuZezYsaxYsYL169eTnZ3N999/z+zZswF44IEH6uJlJRzWPO3ukwXurTk/+pNWZomIhNjWvUcAmNn439gMtfapSqPaPvDYsWO8+uqrzJ49m6+++grTNGnVqhWTJk1i8uTJpKam+lxvs9m47bbbeP/99/n4449PO3AJg4O5pxKdij56AHpco+WNIiIh4FmN1ZPtXGzLCbzAsKmm0k+tk52UlBQOHz6MaZqce+653HHHHfzyl78kISGh0se1b9+e48eP1/ZlJZw8K7MCNnM4uTJLyY6ISL0pcJaQV1jMu5v24DLhKvsq68WynQfr/dhPrZOdw4cP87Of/Yw77riDYcOGVftx99xzD7/61a9q+7ISTp6VWR9Z7Nuw6ilIvyj0MYmINADz1u1kxvwcnyXnTSi1vrhlqvXxBqzWNTvfffcdb731Vo0SHYCuXbty8cUXV/v6FStWMGrUKDp06IBhGCxcuNDn/K9//WsMw/C5XXHFFTWKSWpg0B3Q/5bA49s/gCUPhz4eEZEY55m28t9bZ7PZ2foByX3qPaZoU+tkp0uX0MwHFhcX07t3b29xs5UrrriCgoIC7+21114LSWwNVss06+Of/kMrAERE6liwTQSbGscDqgpMDOimL/z+aj2NFSojRoxgxIgRlV4THx9PUlJSpddUVFpaSmnpqeE/NTKtobSsICdM2PU5OK4OaTgiIrEsvU1TbAY+CU8SB7iv0Ryfmh0TMC59UPU6Fup0n51wWb58Oe3ataNbt27cdtttHDhwoNLrs7OzcTgc3pv/yjGpQsdM6DTI+tzRg6GNRUQkxiU7Esge2xP7yczGAO5pNDegONkAaNws1OFFBcM0LfukRiTDMFiwYAFjxozxHps7dy5NmjQhPT2d3Nxc7rvvPpo1a8aaNWuw2+2Wz2M1spOamorT6aRFixb1/ceIDc58+Oe5uL9L+Bn9FPSdGPKQRERiWYGzhB2FR+kSf5h2L/TFaiEWF98LQ+8LdWhhU1RUhMPhqPLzO+Knsapy3XXXeX/fs2dPevXqRUZGBsuXLw9aPB0fH098fHyoQoxNjhQY/SS8fQcBCc/bd0DGMA2liojUoWRHAsmOBNj8SfCLzro8dAFFkZiYxqqoS5cutGnThu3bt4c7lNjXdyKM/LvFiZO1OyIiUmcKnCWszi3Euedb6wuSeqkBaBBRP7Ljb/fu3Rw4cIDk5ORwh9IwNEm0Pn5AW5WLiNSVivvsvNBoMcOtPr27Vb6YpyGL+GTnp59+8hmlycvLY+PGjSQmJpKYmMiDDz7IuHHjSEpKIjc3l3vuuYczzzyTyy/XUF5IpA7AXRbnN5W17CHYtwmu/W84ohIRiRkV99lJ4gCX2DdaX6gprKAifhrriy++oE+fPvTp494kadq0afTp04f7778fu93Opk2bGD16NF27duWmm24iMzOTTz/9VDU5oeKp3bEqlduyEJY8FOqIRERiSsV9dtJtewMbf4K7TlJTWEFF/MjOkCFDqGzB2AcffBDCaMRS34lweBeseCzw3Kf/gPNvUrGyiEgt5eQ7vb/PcyVRbhrYKyY8hs29ClaCiviRHYkSXSvZsfOruaGLQ0QkhhQ4S3h00Vbv/b20ZkbZzbg8H9+GHUY9oS+UVYj4kR2JEh0zoV0P+HFz4LmlD0Gzttp7R0SkhqxaRbxePpTx439NZvNDkNhFiU41aGRH6s7oJ4Ofe/tO9c0SEamBAmcJB34qxeZXEmk3DDp0yoD0i5ToVJNGdqTudMyEjEsgd6nFSRcc/F7/MUVEqqHiUnMDMAxobx4gw7aP8Vdc7N5cUKpNIztSt0Y/jeXKLIDVKqATEalKxaXm4N7YY7xtGWvOuJNX4/7C6GWXw4ZXwhpjtFGyI3XLuxTdwncfwMLfhjYeEZEo41+nk8QB/troBQxc7gOmC965Q6UBNaBkR+pe34kw+B7rcxtfhddVqCwi4s/TDqJpnN2nTqev7dvAvXVMteWpCdXsSP3oeoX1vjsAW96C3eu1AZaIyEkVa3RsBlxxbhKLv97rrdmR06ORHakfHTOh/bnBz+/6LHSxiIhEMP8aHZcJ72/e673fwThA4N66BqT2D2WYUU3JjtSfUZUUJKdeELo4REQimNVeOh5JHGB6ozkYFYZ3TIBLH9Tq1hpQstOAeOaDC5wloXnBjpnQ+/rA4xmXQPOk0MQgIhLh0ts0DdhLx+PGRoux+50zADr0re+wYoqSnQZi3rqdDJq5lOufX8ugmUuZt25naF746mfh5qUw4DZIHwIY7n14ZvXQ0kkRESDZkUD22J7YDd+sJokD3Gx/3+IRNvfOyVJtKlBuAKzmg++bv5nBXduGZmOqjpnukZxZPTg5AHty6eRUd6deDcWKSAM3vl8ag7u2ZUfhUTblH+axRdtIN/b6Nvz0GHi73jdrSMlOA2A1H1xumry3qYAreyV7r0lv07T+kp+Due4EpyKz3L100nF1/bymiEgUSXYkkOxIICujNaN7d2DPD2mYC7IxKr53GjYYcGv4goxSSnYagPQ2TTHwjql4/eW9b3jkvW/g5DmbAdljezK+X1rdB5GY4f5P6p/wvPFrOH5ETUJFRCpIdiSQ3KsHlD3hHgU3y092OJ+lUZ1aULITgwqcJdUeqamYAFWc3oLKR3s8r9E0zk7x8fKqX8uRAqOegHfuDEx43r4D2p2rfXdEpMHzef/mILTqDDd9BCeOqsP5aVCyE2P8N6bKHtuT1MQmAaM6wZSbJo+8+w3v5RRg4q76n3RROjdemO5NZiq+hke1RoX6ToSy4/D+3X4nTHjhEhj9lHeEpyYJm4hItCtwlvDSyjye/zQPE7jOvozsxi+6W0QYNveXxfSLwh1m1DJMM3CrooamqKgIh8OB0+mkRYsW4Q4nqKoSgAJnCYNmLg1IQp6fmMmkV9YH3cehOjzJzOCubQNew8NuGKycPjQgNp+4dy2CN260fhHDgKlfM+/b8oCErV6m1kREIoD/F8gkDrAq/g7f4mTDDlNzNLLjp7qf3xrZiRJWIzaeBMCTTBwsPh6QhLhMuOm/6xnarS3Ltu23fG6reh5/nimuWdf1Dpo0lZsmOwqP+iQ7/nHPGtGR0cFe0TQ5uG0lMxY0C9/KMRGREPJfLQuQbrNYhWWWw8HvlezUkpKdMAo2UuN/vLKl4yu+3e9zLljiEizRmX19H/p2asV7mwr4y8li5WDKTRObYWAzCDqy07lNE596Hv+471pUyOCf/YOWH91tGemhwgJc5lkBr+ufRImIxAKr1bJ5riTKTSNwZEd769Sakp0wCTZSE6zmxmrp+PodhwK+EXjqbKozY2U3DPp2akWyI4EreyXz1/e/qXSqy3P9vSO6k/3+1oDz94zo5pN8GQYB/VzKTZNvksZw5oSzafPaiIAGd10+f4Dr7Dczt3yoz+t2btOkGn8iEZHo0jTOHnBssH0TRsV3ccPQKqzTpB2UwyDYSM1Xuw5ZHm8aZw/YStxuGBBkhKWyRMfzPHbD4K9je3hHS/x38LQZMPisNpbX90xxWD53x5YJPvFbVYPZDYNN+YcZ8PIh7j0xiXK/awxM/hr3IinGQcs4RURiSfHxcp/7SRwgu9ELvu/5puHegFVqTSM7YRBsk791Ow5ZHj963EX22J7cN38z5abpTQAyO7WyHD0Jxm4YzP9tFkePu+jcpklAAlFxB0/P+QJnCRt+OITLNDm/cyJg/U0EYM7aHyyTL89Ik90wuOeKbjy6aCsuE14vH0qxeQaz43wbhtpMF+9cHce21hdYxikiEity8p0+9zNt31rsmuxSvc5pUrITQhVrWfynmgwgvU2TgHoYzxROVkbrgEQEYPKQDJ5ellvla3sSpN6prSq9zrODp0fFaamK02pWVuUetDzu+ePcOqQLPTs6fP58611dA+emgcT3f0PWpQ9Bxp1V/tlERKJRgbOERxedKgm41r6M7EYvBF6oep3TpmQnRCrW4ljV1JjAzf9dz9i+KSz8co/PCE7FqSb/lU7PLK860bnzkjO5bkBatUZICpwlfLHjIIZhkNoqwXJa7d8Ta9dtd/ayXH50HvMZjdpLa/5QdjPZcS/6bokOmB/dj+HMh5GP1er1REQikdUKWs/0VeCojk31OnVAyU498yQPPrUsQa41gfkb8rn7sq4cL3Mx7Ox29E5tZblq66tdh5g+P8dnCitYYXLXpObVSnTmrdvJ9DdzKq35KTdNbn5lfZXPFcz/bcgPODa3fCg/lZ7B037TWQZgfv4cJYf20OQX/1vr1xQRiRT+X3w979uWy80BrvkP9FD/wNOlZKceWe00XBUT+PuH3wLw1NLtjO2bwoIv832mkQDLpCTYy/TtVPnUFbiTsqoSHe/r1MM2lF+4ulJugt2vENsAEr59hw8/fI/LLruyWs+l3ZdFJBL5L07xrJ41DLjJ9h7myVWsHiZgtNSGqnVBq7HqidVGUTVlAm9uyPeZRprxZg4z5lsnJXbDYMaI7j7LuQ3cdTdVySssrnZLCX8/65VUy0eespfWzCybYJlIGQaUfvokBc6SKp9n3rqdDJq5lOufX8ugmUuZt27naccmIlIX1v8QuAjFBJ652GSYfaNPogPu929OHA1RdLFNyU49sVpxVRdcWC83txnw17E9GH1eB/D7ZnDf/M1VJgqezug1ZTcMeqa0rMUjA73oGsVLZZdbJjwjbZ+x5wff+qQCZwmrcwu9f7ZgS/qrkySJiNSneet2MmXOl5bnzvr+5YBEx81QYXIdUbJTT9LbNA3YG6cuGBDwvDZgwW8HMr5fGi+tzLPcyG9HYeXfDpIdCUwf0b1GsdgNg3tGdOPRxYEbDNbGoDPbcOLybD5znW3xWtDl2Nfe+1YjOMGW9G/44VCdxCciUhueL2JW33+TOECXfR9ZP/Ciu1WYXEeU7NSTgE366uh5Jw1OD9j8796R3b2FzP/+NC/gMTaDau1A3LOj9WaBP+uZHDDq85vB6aycPpSeKY4aj2ANPquN5SjSp98Vkv3+Vqad+G3AZoMALd+/ha/feJj/t2YH09/0HcGZMT+H/ENHLb8d3T7nS01niUjYVDbSn2n71vqLccr5MOxP9RpXQxLxyc6KFSsYNWoUHTp0wDAMFi5c6HPeNE3uv/9+kpOTSUhIYPjw4Xz33XfhCdbP+H5prJw+lNcmXcCCyQNPe6THZsCNg9IZ3y+Ne67ohoH7g/7RRVuZt24n64OMYEzoX71l51ajUXbDYNLg9IBrX/x0R9DHeFTcgbmiVdsP8NBV5waNo4DWzCibRJnp+2ADk3Ny/s7Od2cGfENymfD7N3Isp8CqO5UnIlIfKnufzLJ9HeTE7fUXUAMU8clOcXExvXv3Zvbs2ZbnH3vsMZ588kn+9a9/sXbtWpo2bcrll1/OsWPHQhyptWRHAlkZremd6u4pdTpuvrCLd1fjRxdv9X7ge2pTDh09bvm4rC6tqx1rxVEjzz4/xcfLA5KLis05/UeafjEgjacmnMej1/TiievOC3idctOkVZO4SpO/18uHcseJwP/shgHTG71GEgeq9Wfyj1dEJNT83yc9kjjABPsy6wel9g9BZA1HxC89HzFiBCNGjLA8Z5oms2bN4o9//CNXXXUVAK+88grt27dn4cKFXHfddaEMNSjPUugOjjNq/Rw24MYLOwPB2020Soiz3Jk5s3PVS889grWMCLazs/9jNu0+zKOLt/Lq2p3uKbYR3S0fm9m5Fdlje1quWPMsxdzg6orLNLD57T1hN+D3jV7j7rLqf/OxGwZN4myszi3UknQRCbmK75NN4mzsOlhCm8K12D+1GI4eeIdqdepYxCc7lcnLy2Pv3r0MHz7ce8zhcDBgwADWrFkTNNkpLS2ltLTUe7+oqKhe4itwlvCflXm8uDLPu09ObfjvpOwZErVKIGaO68mMN3Nw4U6Qssf1rPEHu/9OzZ5vJf69ufyvAfjFC5/51NI8tmgb917RnccWbwt4rOc//0ur8nhhRR6uCn9Wz5vC2tW5XLB9VkAtzlj7auIp5/ay6rWTuPzc9lz9zGpvN/bpI7pzy+CMGv1cREROR8X31t6prcDZH1baoOLu8YYNBtwapghjV1QnO3v37gWgffv2Psfbt2/vPWclOzubBx98sF5js9qNuDZL0f905dmM7JVc7eTDamSmLlTneYONOPXq2JKV04daPjbZkcB9I8/hxkHplucHbenPQ/ZL+GWjpT7PaxhwpX0tb5StZzmZVca/6Ou93noe04Ts97dSVHKC319+elOLIiK15kiBUU/AO1PBLHf3wFJriHoR1clObc2YMYNp06Z57xcVFZGamlpnz1/ZMsOaOr9zK8vEorLkw39kpq5U9bzBRpw88VX2WM95z9456W2aepOnp8uuZoJ9aeDuygb8J/4fZJddz/PlPwv63AbWuz7PXpZLizMac8vFGuERkfpRcUd3IHB3974TIWOYu6t5YhclOvUkqpOdpCT3zr379u0jOTnZe3zfvn2cd955QR8XHx9PfHx8vcVVlxsKHj3uCnquvpKa2qrOdFdlKrbXsBlw7xXdMQzYa7ZmdtlVTGn0VsB0ls2A+xrNoTlHebz8Wsvn9WzJbvVX8uiirYw+r0NE/RxFJDb498GCk+9HBjw8tBW/PKsMEjPcCY6SnHoV1clOeno6SUlJLFmyxJvcFBUVsXbtWm677bawxdU0zm553GbA1X1OdTWvSsUi4GhR22k0q92PH1u8jclDMnh6WS6Pl4+ni7GXK+1rA7dUN2BKo4UAPF5+LTbcbyjVyTddwIYfDtGqqe+3rYrd3zM7WY+uqQeXiARj1QfL4+e2ZUxY9QKsNt01OqOecI/wSL2J+GTnp59+Yvv27d77eXl5bNy4kcTERNLS0pg6dSp/+ctfOOuss0hPT+dPf/oTHTp0YMyYMWGLufh4ueXxJ6/rw896d+B3l3fzrlyqWLg7pk8HbyJU01GRSFKbEadg9T6DzmxL8zMaM3PRVm4vu5M3ytbzn/h/BBR7exKeAWd3Zn+vW7jdb1t2E7j8nPZ8sGVfwGtPPnmtAcwcF9ho1QCmj/QtaPYfhcoe25Px/dSwT0Tcgo3wJ3GA7EYvnOpwbrrcNTsZwzS6U48iPtn54osvGDp0qPe+p9bmhhtu4OWXX+aee+6huLiY3/zmNxw+fJgLL7yQxYsXc8YZtV/mfboqWy0Fp5KBrIzWjD6vg88oiCcRqsvi4mhQWb1PVkZrUlolcPucL1lOJtll13NfozmWIzz9tz/Bj0MmWj7Xz3onWyY7HibBu8lnv78VTLjl4oygPbgGd23boP7ORCQ4q/c0gBsbLT6V6HiY5e6aHSU79SbiNxUcMmQIpmkG3F5++WUADMPgoYceYu/evRw7doyPP/6Yrl27hjXmYJvzWX0QepIezzn/+w1FVT+zzE6tvKM5z5f/jKfKxlh3SMekfe5Cy+c6v3Nilc1OK5v6enTRVu/UldUolDYtFBGPZEdCwEaySRxgkv29wIsNmxp+1rOIH9mJVvW1BDyWVbXCrGLx8xOu8YxKOUH6Hos3jmUPMf6cTQye/q+A55o+ojvZi2rXuNQF7Cg86l1V4W9T/mGyMqq3W7WIxL6eKb79BvsG64PV9waN6tQzJTv1KNJWS0WDyn5mAckQveGf72M5HrNlIcnN2pE88m8+h2+5OAMMmLloq+XIUGU8DVXf3rjH8vyji7YyurdWdomIW9M4u89K0IHB+mClXxyqkBqsiJ/GEqnIZ5rPkQKjn4Rgk1Of/xuWPBxw+JbBGSz87cAqp7T8TejvLkAONjLkMuGlVYFd50Wk4Zm3bidXP7Pam+gkcYDr1QcrbJTsSHTrOxEmzA1+/tO/w/v3BBzunepurVGTFh6vrt3JU0u+q/SaF1bkWXZX92yWqM7rIrHPfxEDuAuT/fv8AeqDFSKaxpLo1+0KOGcMbFloff7z5+CnvXDtKz6Hx/dLo0mcnSmvbaz2S732+a5Kz3vqeipOZWmZukjD4r+IodLCZPXBCgmN7EhsuPa/0P83wc9veQt2rw84fH7nxIDRHRvBm7ZWVeZjgM9GkMGWqWuERyR2eZade9zeaIEKk8NMyY7EjpF/g4t+F/z8rs8CDlktec8e15PssTWb4vLwT4ZeOtnxviItUxeJbRXfV5I4wAT7UusLVZgcMprGktgy7E9QesQ9deUv9QLLhwRb8j64a1teWpXHCyvyCN6hLJBnGqvAWcLznwYWLHtWdQWjNhQi0cvz/3dw17asnD6Ug5uXYP/I6kpDhckhpGRHYs/Ix9w1OlveOnWs9/XQPAk2z3ffTx3gM3xsteQ92ZHAfSPPYUB6Ijf9N3AKzIoNdyJT4Czh3U17LKe9br6wS9AkRvU9ItHL8v/vsS+tL770IU1hhZCSHYlN177irtHZ9Zl7ROfHr+Gf5+Iz0XTR79wjQZWYt24n976ZE/R8xT00DCB7XE9WfLs/YCWGhw248cLOls+lNhQi0cvq/+/3C7MxG88J3Obiorth0B2hDrFBU7IjsatjpvvmzIcXhxNQUfPp36EgB375uuXDPW9ewfxlzLkMO7s9G344hGni7X02aOZSy0SnYgsMq6mqytpQKNkRiWxWK7DubfSa9X5eXYZaHZV6pGRHYt/BXHdnYSvbP3BvPGgxwhOsazG4m44OO7s9yY4Erux1KhFZnVto+Zg/XXk2I3slk+xI8BnqNgx3C4tbBmdYNg6sqr5HRCKD///foPvqoD5Y4aDVWBL7EjPc+1kE8+k/3KM/fvyXj3rYDJg5tqflaEtOvjPweuD8zq1IdiTw1a5DTK8w1G2a7o7qz32S613BUbGbu2nCim/3V/UnFJEw81+BdbPVvjoAlz6gWp0wULIjsc+RAqOeIGhbCUxY8lDAPjz+y9JtwG8Gp7Nq+iWWRcMFzhIetWgl4QKufmY1d7++kTGzV1v25PJ0VB/cta3PbJuJ9uURiRbj+6Xx74l9ubZLKXart5vMG2HQnSGPSzSNJQ1F34mQMQzmTYQ9XwSe3zTXfTtnjHuDwpOslqV7Wj/4Lw2vbNrLZcKbGwJHj7zncS9ZL/zpWMAKLtXtiESHu1/fyJsb8unJUabGG37TWDYY/PuwxdbQaWRHGg5HCvxmycmNB4OM8mxZCG9O8pnWqth8dN66nQyauZTrn1/LoJlLmbdup/e6YNNe1WE3DDblH+bOuRstz6luRySyfbXrEG9uyOda+zIWxt+PzTC9o7imYYfRT2j6KoyU7EjDM+xPcM1/gp/PeR1m9YANvr20qmr94D/tVV02A24d0oVHF221HBka06eDRnVEItzzK74niQPMbPS8dwrLOFmw/Fbmy+7RZQkbJTvSMKUOqPy86YJ3pvqM8FS2NNxjfL80Vk4fyh+vPNvyaT3/4eyGwYgeSRi43wyfWZYbdApswYZ83t20h692HVLndJEI4JnK9vxfLHCW8G7OXv7ZeHZgrz0DejcPXLggoaWaHWmYHCkw+il4e0rwa8xyeGuyuytxtyssl4ZbTTG5l6Mn89f3vwm4dv5vszh63EWTOBtXP7PaW59TWYNRF3D7nFO7sGpnZZHwsdolOTWxCT3ZzgW2wAUKAOmtm4U4SvGnkR1puPpOhLu2QM9rg1/z/TJ4bTw8d7Fl01DPJoH+gl3bO7UVWRmtKT5eHnQkpyp10Tnd/5upiFTNaip7xvwctv94hKvsq7GawTZBPbAigEZ2pGFzpMC452H4A7D2X7DmaesNCAs2wuu/Zvy1L1s2DbUSrMEoBG5AVlOns0JL/bdEasdqKttlwv1vbeGVxrssH1OaOpgzVJgcdhrZEQF30nPZwzDuxeDXbFkA827wWZ1VlWDX+o/81HQRl0HtdlauqshaRIILtuIyiQMMsn0dcNwEzrj8gXqPS6qmZEekoqoKl79ZCF++Wicv5Slmfm3SBTx01bk1e3Atl7hXp8haRKwFW3GZbttruYmgkTHM3Z9Pwk7JjkhFnsLlyrz1W3h2IHz+gmWbiZrwjPwMP6d9jfIX08QnQSlwlvDupj2881V+paM0Vt9MtY+PSPV5vqTMvr6P9/9sniuJctP/f7BR9XuJhIySHRF/nsLltAuDX7Pva3j/bvjnOe5Goqcp2ZHAzHE9q70poc2AJnHu/77z1u1kYPZSbp/zJVNe28jAbN/NDv1fp7pF1iISqMBZQl5hMX07tWLmOPf/pb205g9lk3B5evAZdhj9pDYRjCCGaVp16mlYioqKcDgcOJ1OWrRoEe5wJJLMu8E9dVWVzoPh1++c9ssVOEvYUXiUTbsPuzcZrORamwH3XtGdmYu2BixdtwGrZlwSNInxvE5VRdYicspzn+R6/795ivuHJJ+g8IdvaNPpbNo3PwMOfu/uaq5EJySq+/mtZAclO1KFL191T11VJXUA3PRhnb1sgbOE9zYV8Jf3vgl6jUHwPXpem3QBWRmt6ywekYbsuRW5ZL/vu4/OdfZlZMe9iGG6wLC5Gw5rp+SQqu7nt6axRKrS5xfQ+/qqr9u1Fv63kj17asizOWFlU1uVfVPZtPtwncUi0pB9tetQQKKTxAEeafSCO9EBy13XJXIo2RGpjqufhZuXQvselV+3/QPYvb7OXraqfluVlfjMXLRVS8pFTtO8dTsZ88zqgOPuFVh+XzfMcvc0lkQcJTsi1dUxE25bBf1vqfy6bz+AzfPdtzr4lldxifqMEd19iounj+ge9HEmaEm5yGnw7EvlX+zRk+0MMLZQ7j+0atjd9ToScbSDskhNjXzMXXz40f3W51c8WuGO4V6VcZrz+MmOBO8y9dHndfApLj5y7ARPL8sNeExtNx4Uacg8q63S2zS13Jfqb42e5Rr7pxiGewsIz07kGHYYNUuFyRFKyY5IbQy6E3pcA69PhPwvKrnQdDcbPfIjnDehTt4IPYmPx+8u787ybfvZvKfI57rpI7prpZVIDfi3Urn3iu4+bV16st2b6ACnemGNfBy6XaFEJ4JF/TTWAw88gGEYPrfu3YMP7YvUGUcKTFriruW5/K8w8h/Br132sHtPnlVP1HkYBc4SthQUBRyfuWhr0P12RMSXVSuVRxdt5d4KU8cPx70U0OzTACg/pkQnwkV9sgNw7rnnUlBQ4L2tXLky3CFJQ9IxE7ImQ7cR7uWnlfno/jpfsWE11A7umh2rvlfqeC4SyLLJJ3DgyHFWTh/K/17RiN5GXsDj3F3NLwhFiHIaYiLZadSoEUlJSd5bmzZtwh2SNESOFPc+G0FWTnmtf8k9yvPhH+sk6QnWnBAC+17NW7eTQTOXcv3zaxk0M/hOy+CbFClBkliX3qap5erGF1a6V1dd+ONrlv+1jQ591f8qCsREzc53331Hhw4dOOOMM8jKyiI7O5u0tLSg15eWllJaWuq9X1QUOAUgUit9J0LGMNg4F5Y9VPm1q5+C1U9D5q9h8O9rPQzuWZ5ecQjewwZs//EIhT8dIy2xScAw/Yw3c+ie1Jzeqa18HlexdsHz/l5x19jx/YL//xKJRsmOBCZdlM6/P/UdvXGZsOeHXJK/ecv6gSP/HoLo5HRF/Q7KixYt4qeffqJbt24UFBTw4IMPkp+fz+bNm2nevLnlYx544AEefPDBgOPaQVnq1ILb4Ks51b9+9FOntWqrwFnCS6vyeGFFHi4q3125IsOAmRUSmAJnCYNmLrWcGgP3kveV04eq+FliQsXVV0DAv327YfDF1UW0et9iy4mMYfCr+SGKVKw02HYRhw8fplOnTjz++OPcdNNNltdYjeykpqYq2ZG6t3s9fPIYfPcBVaceNrj5YzhRDIkZtR7pKXCWsH7HIabM/TJgf5Bg7IbB/N9mUXy8nAM/lTLltY2VXq9WFBIL/EcwrzovmaJjZSzfth+X6f5/8dr539J/84PuHZIrMmwwdbMKk8OsuslOTExjVdSyZUu6du3K9u3bg14THx9PfHx8CKOSBqtjJvxinrs2Z+2/3FNXQZMeF7xwyam7nYfA8PtrXA+Q7EggsVlx0ETHasSn3DQZM3u1d6qqslEhu2Fo/x6Jev6rr0xg4cYC7/kBnVsx6bwz6PfBLwITHU72wVKiEzViokC5op9++onc3FySk5PDHYrIKY4UuOxhuOtryLyxeo/Zsdyd/Lz68xq/XLCiZRvw2DU9LQsxPcmNywSMU28OBqdqru2GwV/H9tAUlkS9YKsYPdbuOMSLb398qvdVRdf8Rw0/o0zUj+z87ne/Y9SoUXTq1Ik9e/bw5z//GbvdzoQJE8IdmkggR4p7l9XE9OA7MPv77kN4fhhc+0q1v0l6ipanV9jq3gCu7pvCvW/m+IzaVNw0zcM04enr+5DYNN47ilNx12aRaOep0alMniuJctPw7YFl2CG1fz1GJvUh6pOd3bt3M2HCBA4cOEDbtm258MIL+eyzz2jbtm24QxMJzrMD867P3fd3rYW1zwa/Pv8L93L1rlfA4HuqNbU1vl8ag7u2ZcMPhzBNSE1M4OpnVvskNjbg+YmZTHplfcDxvp1aeRObAmcJ5skUqWJBpxIfiVYrvt0f9FwSB8i0fYsJzCybwL2N5tLIcKklRBSL+mRn7ty54Q5BpHYcKeC42v37lmmVJzse3y523zIugdFPV/mmm+xI4Mpe7oRkdW6h5aZpx064uPDMNqz4rtDn+Ipv9zO+X5qWoUvM8dTrWLnWvoyZjZ73TgO7TIOZZRO4ZcI4WqeerUQnSsVczY5IVOqYCb2vr/71uUvhn+fChleq/RCrOh7DgMlzvvRJdDxmvJnDV7sOBRRxVqztsdqhWSTSBavXSeIA2RUSHQCbYTK98VwlOlFOyY5IpLj6WXefrQG/BUd1RkvMU60nnPmQt6LSHZk9dTyePj+2KjbicQHrdhyqtIjTs0OzdliWaBKsgP/+Rv/FblnY74KD39d/YFJvon4aSySmdMx030Zku/foWfGYe9oqGLPcvaR9zdOnlsee9wsY84zl5Z46nh2FRyn86Vil++kYQFwjw7KAuaKV3+3nFy/kejtFa2pLIp0n8b9v/mbKT1bwv9X4D/SyBfa+Atx76iR2CWGEUtdiblPB2qjupkQiYbHqSfjoT0FO2nCPwfhJSIRbV1Y67F7gLGFg9tKggzuegR/j5J2abFCoHZYl0hU4S3h00VYWbtzDENbzUvw/grS1M2D0k1pqHqGq+/mtaSyRSDfoDrhrC4x83L0ay1MmbNih5zjrx5QcdK/eWvUEbJ7vvvlNcSU7Epg+onvAQysWIXt+rclXIv/moyKRZt66nQzMXsrCjXsA+Gvci8H79968RIlODNA0lkg0cKRA/5vcN2e+u34gsQusf7nyx/ns5RP4DfWWizPAgEcXbfVuj/8/F3bm+U+DDOdX09HjJ1idW6jl6RJxPCuxPPn70sZTSTYOW1/c/xZ1NI8RmsZC01gSxXav920xUSUbTHgNDmyHtCzvG3mBs8S7aSAENkOsKc8UmGp4JNKszi3k+ufXArCg0X2cZ99hParTvAPc/U1og5Ma0zSWSENQ0yXruOC18fDhH9xJ0ksjwZlPsiOBrIzWJDsSrFdtWQh2HLQ8XSJXepumGAb8v0Z/CZ7oAIz/35DGJfVLyY5ItPMsWU/uU/PH/rDKvV/PO1N96nrG90tj5fShvDbpAhb8dmBALy0DuHdEd8seW/5UwyORwrP794upi7nQviV4otN5sKavYoxqdkRiQcdMuGW5O1nxtKBw7oKP769GdbEJ619y3wB6XgvDHyDZkUKyI4HVuYUBK7ZMIMWRgFGNVVrqki6RwLMTeDvzAKvjXwme6DRLhl+/E9LYpP4p2RGJJRVbUAD0GHcq+Wmc4J7CqkrO6+7bRb+DhFac1bqv5V47q78/UGVdjwHcdGFnQD21JHw8RckuEzJt3wadgjVN+PG692gf2vAkBJTsiMQy/+TnnDGwZWH1Hvvp3wFoCyxJupD//bETn5d3I4czAZj7+U5vIXJl/v1pHs9/mue9zgBmjlPRsoRGgbOEdzftwWW620GcZey2vM404a9l13NJaUslOzFIq7HQaixpYJY8BJ/+o0YP8WwuaJqQ4+rEc+WjWe/qyujB5/Pipzu8u9BWlwGsnnGJRnikTvmPHlZsYjvJ/g7TG72G/eTUa8VpLNOEp8qu4gnXddoQM8pU9/NbyQ5KdqQB8tT25K2A9f+p1VO4TCi67HFKelzPhh8OMXnOlzV6/NMT+vCz3h0ATXHJ6auY2NgMuPeK7jy62L1/1DT760xptDAgwTEMKD/Z1fzF8p+RrRHHqFPdz29NY4k0RJ7prR5Xw+DfwfxJ7pVZNWAzoOVH02jZ2KRlgYtkzqCA1tV+/OGS46zOLSRnt9P7oQRwff9Upgw7S0mPVFvFmhxwJ+KPLtqKC+tEB9yJzkMnfsli1wB+dmE/Vl3YWf/mYphGdtDIjgjg3qDw2w+gWXv3iM83C2v0cNOEZeW9+N7swPdmMktdfdkbJPnxb0lh5VF9y5ZqqrhRYEW/sb/LjEZzLFdeuTD4cuxKOnTKUJITxTSyIyI14+m4Du62FLvXw67PYMtbsCvwg8SfYcAljTZxCZsAcPES2Scm8Hz5qIDrMKsubJ7+Zg5N4uyc3zlRH0ZSqaZx9oBjKcZBZjR+zXIvKNOEI4P/RGavHvUfnEQEjeygkR2RKv3vtbD9gxo/zDRho6sLB83mrHN144eUnzHywvOZ8trGaj+HWk5IZeat28n0N3N8kmcD+M/FJQxde1PA9aYJW86cxLm/+nvIYpT6o5EdEak7v3zdPdKz4jH3VFeV4zJuhgF97N8DMIyvMPe/zpH8qdiM/tXuveVpOTG4a1uN8IgP/6ae4F5enm7by6ET3cCwgenynjOBIwOmcu7IB0Meq4SXkh0RqZ6OmXD9vFNd1/dsgI/+THUTH3B/427++SyWdbqU63+4ijSjgB/MJPaYlRc2e1pOKNmRivIKi32S5kn2d5nR6DVshkn5eoOj5/6cM755A5vpotw0eLRsAhltb6IaW2tKjFGyIyI140hx39Ivgh7X1HgllwGk7f2IlfEfY2BiYuPVsiEUuZrQznaYd8sGsBzfvkRqOSFW0ts09e7uPc0+jymN3vIWI9sNkzO+eYOrjz1AgnGcHa727KU1do0SNkhKdkSk9hwpcOP7sHs9RTnv8sWqjxlq3xS879BJ7tPmyd+7+GWjpd5z4+wr+c6VzJ/L/4c8VxL7aM09V3Qjr7AYQB9S4pXsSCB7bE9yF2b7JDoeNtNFgnGcz1zneI9plLBhUrIjIqevYyYtOmayv82NXDj/E3ob3zLR/hED7N9UqzN6RYYBZ9kKeM3+CC4TXi67nI8+OJ+XXEn8aLRWsbL4GN/Vjtl4juW/s3LT4AeXb/MHjRI2TEp2RKTOjO+XxuCu17Cj8Cid2tyHcWQLW955kpYFy0k2Dru7pEOVCZDnG7rNgBsbfcD/GB/gMuHpsqu4b76haYgGpsBZwsdb9vHjkWMMP7s9vVNbnTq59tmg/55eK7+EURVamtgNg7+O7aF/Ow2Qlp6jpeci9a3AWcL+b9bQ6egmHF0vgh+/xnznTowKK2WqwzRhSfl5JI/4Pee2aQQHtkNa1qn9gSTmzFu3k3vfzPE5Nq5vCv+49jx3sfysHj4rrjxcJlx0/GnemH4NADsKj9K5TRMlOjFGS89FJGIkOxJIvuAS4BL3gY6ZGBnD+GbhY3T7/mVs1ZzrMgwYZt+I8dEvfE/0vh6ufrZOY5bwK3CWMH2+b6KTxAH2bPyalRlwYWJR0ETnvrLfcMfYi73JjZKchk3JjoiEhyOFs294gn277+DwtlWkf/00cQe3AYFdqSuyPP7VHOh8IZQcgoRWOA/9yI4mvWh39kB9yEWxl1bmUXHu4X77y9zQ6EPsBpS//Qhfnj2NPgF76djYPOJN7jw7S3/34qVpLDSNJRIxdq/H+e2n5CydxyD7FneNTyWJjxVPTZBpwvvl/dl96b/o2dGhjupRpsBZQla2e5Xe1Szn4biXaWoc9/m3UG4a/DT4TzhWPgJmORh2GDUL+k4MT9AScprGEpHo0zETR8dM8puP5er5C+lr28ZQYwMX1iDx8Zw2DBhp/5yXPpzBU+b5/GAmMX1Ed0anHoPEDPeyeYlYnq0GPou7jfaG0/Lv3W6Y7KUtjqk57o0uE7vo71UsKdkRkYjjXtU1iR2FR9m0+zD/WPwefW3baMkRftvoHRoZLm/iU9WU16nVXAYsMU9lQz1/Dt1GQuoAfUDWoQJnCXmFxac9kuY6vJt1cb+hjfFTpQlufCPbqY0uRYLQNBaaxhKJdAXOEtbvOMQdc7+knXmAzrZ9nOEqoYt9H4fKm/B4/L9rNNUVIGMYpGRC1yu0sus0zFu3kxnzc3CZp9nAdcMrmG9PqXKLApcJX45bpe7lDZimsUQkZiQ7EvhZ7wSKj5cxY34Oe13uXlrLy93nB5Z/wzX2T2tV4wNA7hL3bcVjkNQLOg2C1mdCtxEaMagmT1NOT6+qmjRw9YwGZcQ7aX/4K6hGomOaMKNsElM7ZdTNH0Bimi3cAdSV2bNn07lzZ8444wwGDBjA559/Hu6QRKQOFDhLWJ1bSIGzhPH90ljw24EBH4S/L7uN0aUP8dCJXzKv7GLKT37g1mrceu8mWPssvH83/PMcmDMePn/BvaeLBOXflBNOtWaozLx1O7l15r8pfvka2jzfF964sdLrTRN2uRLJKn0Kx8CbVHQu1RITIzvz5s1j2rRp/Otf/2LAgAHMmjWLyy+/nG3bttGuXbtwhycitRRsWmTmuJ7cN38z5abpbQSZw5nklJ8JwKzya+hs28f1HQsZtf95DLPc3YmrNqM+3y52396/G1plQNuz4MxLNerjp2JTTo+qWjMUOEto9tbNLIxbW+Xfi2nCT2YcU45P8TaKvfHCznUQuTQEMVGzM2DAAPr168fTTz8NgMvlIjU1lSlTpjB9+vQqH6+aHZHIU+AsYdDMpQEfniunDyXZkUCBs4QdhUdpEmfj6mdW+1xnAC/ckMmws5PAmc+BXd8w6tU9XGlbw4xGc6q9iWGVzvsFtEiBZu2V/OBOTj1JqKc1w+Cuba0Llp357Fswg3Z5gQ08/ZkmrCw/h1+V/dF7zDBg9fRLNLLTwDWYmp3jx4+zfv16ZsyY4T1ms9kYPnw4a9assXxMaWkppaWl3vtFRUX1HqeI1Exl0yLJjgTvDdwjPv4jQMPOTnI/yJHCtsJ49pilPF/+M94pz6Kv7Ttusr9HX1vu6RU2b3z11O/fvxtSL4C45pB2AZw3ocElP+5VdG29rRlWfLvfm7B2MA7wyOAmDO2ZDlsWwOqnaY8ZtFFauQl/O3Et8UYZS8vPI4czfc6bJupeLtUW9clOYWEh5eXltG/v29m2ffv2bN261fIx2dnZPPjgg6EIT0RqqappkYpLnOFUfY7VWHXF59pLa953teZ91wX0YjtD7V/S28hliH3T6Y/47PrM/WvuR7DsYej/GzhWBG3Ogt4NI/nxJKGeguVzze3c0WgBw+wbsa01MddW3QjWNGFm2QSed40Keo26l0tNRH2yUxszZsxg2rRp3vtFRUWkpqaGMSIR8ZfsSCB7bM+AaZFkR0JALY9pundOBvev/quAkh0J3HtFd7IX+X4B2sSZbDpZ55NUdoC+tu9IYy+tjSL62r6jrz23yg/mSn3+71O/X/owtD0bWnV233r+PKaXuecVFvOkfRZX2j/3GT2rznLy7LLreb78Z0GvsYG6l0uNRH2y06ZNG+x2O/v27fM5vm/fPpKSkiwfEx8fT3x8fCjCE5HT4D8tUnHEoOISZ38Vp7s8enZ0VPpanhGfU08CPU9s5xL7Rn5l/5DWVWxuVy37v3HfwL3iq9257imvlExomRp1Ozt7RtfOKvuWtvnLwB4PCS2hSSK9tq+kqV+iU5lyE94qH8hjZRPYS+tKr33q+j5c2avD6f8BpMGI+mQnLi6OzMxMlixZwpgxYwB3gfKSJUu4/fbbwxuciJy2irU5YF3L489qisNqWqwqnhVeT5Rfw9Us59rGn5DKj6TYD3n7b51WAvTj1+7bFy+eOtbzWhhwK5wohsZN4fAP7uORtNPz7vVsXvY6O775gkGNvqalURIwZNOU6v1syk14vvxKXi67wpvk2IB7R3YnxZHAHXO/DJjK7NupVZ39UaRhiPpkB2DatGnccMMNnH/++fTv359Zs2ZRXFzMjTdWvl+DiEQfq6TFMMAwwQU+010VeabFZryZg4uaW8AQFpwYAkDvE9vJtH/LofImpNkLGWz76vSLnT1yXnffrGTeCOmD4UQJbFsEx46AvRF0yqrfmqBti+G7D+Gsy+Cr1zC3LKQH0CMu+EOC/Sg8CWKZafBC+UifJMej4shN8fEyy6lMkZqIiaXnAE8//TR/+9vf2Lt3L+eddx5PPvkkAwYMqNZjtfRcJLoEW+JccbormK92HWLM7NWc7hvfgM6tWLvjkPd+T7Yzwb6UdsYhvnB1438Sc2j30zen+So1lD4EOl/knkqqqEmi78iQMx92rXX/3nO8YkIDp37/6eOwe22dhOcy4emyMaw2e7DD1d5yuqri9gIenm0Gqvq7lYanup/fMZPsnA4lOyLR53Q+ACe/up73cvbWU2RuNuDjUSW0/vJpWhR+eXqFznXCgNFPun/79h2cKuk23AXTh/K8V5ruo95fa8sziuOuxxnEY2XXWSY4ntfyJK616qclDZKSnRpQsiPSsHy16xBXzV5d6TW9UlqweU9RjWp8/Hl6dXUwDvBMz+84z7UVdn8Oxw7X/klPi3EyqNpM5NWMC4OZJ65jk5kRdBTHBiyYPJB2Lc7QyI3USoPZVFBEpKZ6p7ZiXN8U3txg3e/KZsBzE88H4N8rcnlp1Q+1eh3PV8k9ZmvG5bRh5fT7SHYkcODLtzj+zYc0adUWh+sI7M05Na1Ur8xaNgyrmsuExeXns9LVk8M0Z6PrLApoHTBd6D+K0zvVXWysJEfqk5IdEWmQ/nHteUzM6sQXOw5x6Ohxnl3+vWURbGanxFonOxWVmyavrd1JYXEpc9Y2AkZiADPH9WT8lWmwez18+4G79USH8+DEUfh+mbtm5rQrjNxMDAyLkZ3Kpqs8uZFnlMrEnQyWm5BT3onGRjmfl3fjOdeYKpeMAzx9fR8Sm8ZrFEdCStNYaBpLRILXABU4SxiYvbSO0g1rb00e6B3h8Ldvdy77f/iG9m0SaXvCvZ/YD/v2c2TTe7Q++CVJNme1VoG5TIO8rL/Sunk8LT68G5vh/hOVmwa7zLZ0sv3oXU4Pp5KbL1xnMfvEaIY2+oplZb3ZSmc62/YFnZrysAFY7IDtX3wscjo0jSUiUgP++/lUPD59ZHey37duP1MXxsxezfQR3enZ0eHTMNO9U/RWXKaBzThE9thMPs87yJsb4oDfAO5VYJfYvyTOPEGcUcal/c7F0aodf/9gqzdBO0xzvjK78kbWNWwpLGZaaWP62r4DYIPrLPbSmteHOGm7dwUPbe2ACd7kxtNhfHnZqd2e97qqHsG5d0R3WjZprGXjEhGU7IiIVKFnSuW7L58uE7ytLDyNTAd3bRuwU/T0N3MCRpg8Gx+CezSlf+ZAOqe2oleTwOX5nkRjn99u0YYB45c7MBnlralZXpaJDRjXN4UFG/JrvDdRr44tycpoXe0tAUTqk5IdEZEgPO0QmsbZg+6+bABXndeBhRv31Mlrukx3b6/x/ToGvF5VU2kuYMwz7lGiWwZnWCYaK77dHxA/fr3FDGDKJRkMO7s9W/ceYcGX+XCyD9nF3dqybKvvc/izgXcH62AjZiKhpJodVLMjIoH8m41e3SeF+Rvyg64uijQzRnTnloszfI4VOEsYNHNpwO7TwT4FPKVANf3zWb22SH2o7ue3LYQxiYhEBatmo/ODLFOvLBEI50aC2Yu28u6mPRQ4S7zHrPqKmWYlK7GoeaJz+9AMJToScZTsiIj4sUwKsP7gtxnWycKMEd1ZPeMSrh+QWulr9Uqpv9Hk2+d8yaCZS5m3bidwqq9YRXbD4IoeSaf9Wjbcf+bfXd79tJ9LpK4p2RER8WOVFFix4S4mnjmuJ/aT679tBswYeWoaZ+7nuyp9jk35RacbbqU8NUAFzhJvM1RPrHbD4J4R3fjg6+CtMwzw/izshsG4vinex3uvMdxdyjWiI5FKBcoiIn48SYFnNZONwJEdmwELfntqfxyrYuD1PxyqdbuJuqwFKjdNdhQeJdmRwPh+aT6xWo1ieQRrsjoxqxNjnlntrfUxTXhs0TZG9+6gYmSJSEp2REQs+CcFK77dH7CUu+JGgP6rjuat28n0N3Nq/fq/HZrBs8tzg64Aq0kiZDcM7+ooq1j9n88AnprQh8zOrbzXVby++Hh5QFFzxYRKJNIo2RERCaJiUuCf/FT2oe4pcK7NyIwBTB/pXjqeltiEGW/mBOxxU9PnveeKbjVLQgx8Eh1/nmk+/92RKyZUIpFENTsiItWU7EggK6N1lYlDZVNDlfnLmHNZPeMSbhnsrn0Z3y+NBZMHBrSDCFZPFOx4r44tK43VP1TThB2FR4M+xqr2R7sjSyTTyI6ISB2zGvlwb9R3Jr1THTSJa8ym3Yd5dNFWXJwsdB7Xk/H90oBTmxmmt2lK79RWzKxQP+RJLADvqI8BTBqczpU9k7n6mdU1GnGp7ShNTUa6RMJNmwqiTQVFpO7NWxfYrsGTzHhYNR/138wwe6w7CbK6tuIxcI/S5Ox28tjibZW+bm1iFYlE1f38VrKDkh0RqR/BOqlXdr3/DsfV6RTunyDdO6I7vVJa1mjEpbJYK440aQRHIom6nouIhFlN+0JZ1fpUtcrJarfnxxZtqzJBqm6swUaaRKKJCpRFRCJEsB2OK6ufqSxBAncytDq30KdtRHVZJVKeDQpFoomSHRGRCFGbVU6VJUjz1u1k0MylXP/8Wp+2EdVVVSIlEi00jSUiEkFqusrJf7dnn9VaFqMyg7u2rfb0lvbTkVihZEdEJMLUtNbHKkFanVtY4/ofqzisEikVKUu0UbIjIhID/BOkuhqV0X46EgtUsyMiEoPqcpfj6u4cLRKpNLIjIhKjNCoj4qZkR0QkhtW0/kckFmkaS0RERGKakh0RERGJaUp2REREJKYp2REREZGYpmRHREREYlrUJzudO3fGMAyf28yZM8MdloiIiESImFh6/tBDDzFp0iTv/ebNm4cxGhEREYkkMZHsNG/enKSkpGpfX1paSmlpqfd+UVFRfYQlIiIiESDqp7EAZs6cSevWrenTpw9/+9vfKCsrq/T67OxsHA6H95aamhqiSEVERCTUDNM0zaovi1yPP/44ffv2JTExkdWrVzNjxgxuvPFGHn/88aCPsRrZSU1Nxel00qJFi1CELSIiIqepqKgIh8NR5ed3RCY706dP59FHH630mm+++Ybu3bsHHP/Pf/7DLbfcwk8//UR8fHy1Xq+6PywRERGJHFGd7Ozfv58DBw5Uek2XLl2Ii4sLOP7111/To0cPtm7dSrdu3ar1ek6nk5YtW7Jr1y4lOyIiIlHCMzNz+PBhHA5H0OsiskC5bdu2tG3btlaP3bhxIzabjXbt2lX7MUeOHAFQ7Y6IiEgUOnLkSPQlO9W1Zs0a1q5dy9ChQ2nevDlr1qzhrrvu4pe//CWtWrWq9vN06NCBXbt20bx5cwzDqLP4PBmnRoyqRz+v6tPPqvr0s6oZ/byqTz+r6quvn5Vpmhw5coQOHTpUel1UJzvx8fHMnTuXBx54gNLSUtLT07nrrruYNm1ajZ7HZrPRsWPHeooSWrRoof8INaCfV/XpZ1V9+lnVjH5e1aefVfXVx8+qshEdj6hOdvr27ctnn30W7jBEREQkgsXEPjsiIiIiwSjZqUfx8fH8+c9/rvYS+IZOP6/q08+q+vSzqhn9vKpPP6vqC/fPKiKXnouIiIjUFY3siIiISExTsiMiIiIxTcmOiIiIxDQlOyIiIhLTlOyEyOjRo0lLS+OMM84gOTmZX/3qV+zZsyfcYUWkHTt2cNNNN5Genk5CQgIZGRn8+c9/5vjx4+EOLSI98sgjDBw4kCZNmtCyZctwhxNxZs+eTefOnTnjjDMYMGAAn3/+ebhDikgrVqxg1KhRdOjQAcMwWLhwYbhDiljZ2dn069eP5s2b065dO8aMGcO2bdvCHVZEevbZZ+nVq5d3M8GsrCwWLVoU8jiU7ITI0KFDef3119m2bRtvvvkmubm5XHPNNeEOKyJt3boVl8vFc889x9dff80///lP/vWvf3HfffeFO7SIdPz4cX7+859z2223hTuUiDNv3jymTZvGn//8ZzZs2EDv3r25/PLL+fHHH8MdWsQpLi6md+/ezJ49O9yhRLxPPvmEyZMn89lnn/HRRx9x4sQJLrvsMoqLi8MdWsTp2LEjM2fOZP369XzxxRdccsklXHXVVXz99dehDcSUsHjrrbdMwzDM48ePhzuUqPDYY4+Z6enp4Q4jor300kumw+EIdxgRpX///ubkyZO998vLy80OHTqY2dnZYYwq8gHmggULwh1G1Pjxxx9NwPzkk0/CHUpUaNWqlfnCCy+E9DU1shMGBw8e5NVXX2XgwIE0btw43OFEBafTSWJiYrjDkChy/Phx1q9fz/Dhw73HbDYbw4cPZ82aNWGMTGKN0+kE0HtUFcrLy5k7dy7FxcVkZWWF9LWV7ITQvffeS9OmTWndujU7d+7krbfeCndIUWH79u089dRT3HLLLeEORaJIYWEh5eXltG/f3ud4+/bt2bt3b5iikljjcrmYOnUqgwYNokePHuEOJyLl5OTQrFkz4uPjufXWW1mwYAHnnHNOSGNQsnMapk+fjmEYld62bt3qvf73v/89X375JR9++CF2u52JEydiNqANrGv68wLIz8/niiuu4Oc//zmTJk0KU+ShV5uflYiE3uTJk9m8eTNz584NdygRq1u3bmzcuJG1a9dy2223ccMNN7Bly5aQxqB2Eadh//79HDhwoNJrunTpQlxcXMDx3bt3k5qayurVq0M+nBcuNf157dmzhyFDhnDBBRfw8ssvY7M1nNy8Nv+2Xn75ZaZOncrhw4frObrocPz4cZo0acIbb7zBmDFjvMdvuOEGDh8+rJHVShiGwYIFC3x+bhLo9ttv56233mLFihWkp6eHO5yoMXz4cDIyMnjuuedC9pqNQvZKMaht27a0bdu2Vo91uVwAlJaW1mVIEa0mP6/8/HyGDh1KZmYmL730UoNKdOD0/m2JW1xcHJmZmSxZssT7oe1yuViyZAm33357eIOTqGaaJlOmTGHBggUsX75ciU4NuVyukH/2KdkJgbVr17Ju3TouvPBCWrVqRW5uLn/605/IyMhoMKM6NZGfn8+QIUPo1KkTf//739m/f7/3XFJSUhgji0w7d+7k4MGD7Ny5k/LycjZu3AjAmWeeSbNmzcIbXJhNmzaNG264gfPPP5/+/fsza9YsiouLufHGG8MdWsT56aef2L59u/d+Xl4eGzduJDExkbS0tDBGFnkmT57MnDlzeOutt2jevLm3BszhcJCQkBDm6CLLjBkzGDFiBGlpaRw5coQ5c+awfPlyPvjgg9AGEtK1Xw3Upk2bzKFDh5qJiYlmfHy82blzZ/PWW281d+/eHe7QItJLL71kApY3CXTDDTdY/qyWLVsW7tAiwlNPPWWmpaWZcXFxZv/+/c3PPvss3CFFpGXLlln+O7rhhhvCHVrECfb+9NJLL4U7tIjzP//zP2anTp3MuLg4s23btuawYcPMDz/8MORxqGZHREREYlrDKoQQERGRBkfJjoiIiMQ0JTsiIiIS05TsiIiISExTsiMiIiIxTcmOiIiIxDQlOyIiIhLTlOyIiIhITFOyIyIiIjFNyY6IiIjENCU7IiIiEtOU7IiIiEhMU7IjIjFlypQpGIbBRRddRFlZWcD5P/zhDxiGQd++fTl27FgYIhSRUFPXcxGJKcePH2fQoEF88cUX3HvvvcycOdN7bvHixYwcOZLmzZuzfv16zjzzzDBGKiKhomRHRGJOXl4effv2xel08t577zFixAh2795Nnz59KCws5PXXX+fnP/95uMMUkRDRNJaIxJz09HRefvllTNPkV7/6FXl5eVx33XUUFhZy++23K9ERaWA0siMiMevuu+/m8ccfx+Fw4HQ6Of/881m1ahVxcXHhDk1EQkjJjojErLKyMnr37s2WLVto2rQpOTk5pKenhzssEQkxTWOJSMxau3Yt3377LQDFxcXk5OSEOSIRCQclOyISkwoLC7nuuusoKyvjxhtvxDAMfv3rX/PDDz+EOzQRCTElOyISczyFybt372bixIn85z//4e677+bQoUOMHz+eEydOhDtEEQkhJTsiEnOys7NZvHgx55xzDs8884z3WFZWFmvXruWee+4Jc4QiEkoqUBaRmLJixQouueQS4uPjWbduHeecc4733M6dO+nTpw8HDx5k4cKFXHXVVWGMVERCRSM7IhIz9u/fz4QJEygvL2f27Nk+iQ5AWloaL7/8MoZhcOONN7Jjx47wBCoiIaWRHREREYlpGtkRERGRmKZkR0RERGKakh0RERGJaUp2REREJKYp2REREZGYpmRHREREYpqSHREREYlpSnZEREQkpinZERERkZimZEdERERimpIdERERiWlKdkRERCSm/X+dcLohq/ov1AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "else \"cpu\")\n",
    "\n",
    "#dataset for training with 10 samples\n",
    "x_train1, y_train1 = create_dataset(torch.tensor([0,-5,2,1,0.05]).T, torch.tensor([-3, 3]), 10, 0.5, 0)\n",
    "\n",
    "#dataset for validation\n",
    "x_validate1, y_validate1 = create_dataset(torch.tensor([0,-5,2,1,0.05]).T, torch.tensor([-3, 3]), 500, 0.5, 1)\n",
    "\n",
    "model1 = nn.Linear(5, 1, bias = False) #since we are creating 2 new datasets, i performed on a new model just in case of not having probplems with re-running multiple time the same model\n",
    "model1 = model1.to(DEVICE)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "sample_size = 500\n",
    "x_train1 = torch.from_numpy(x_train1)\n",
    "x_train1 = x_train1.float()\n",
    "x_train1 = x_train1.to(DEVICE)\n",
    "\n",
    "y_train1 = torch.from_numpy(y_train1.reshape((10, 1))).float().to(DEVICE)\n",
    "x_validate1 = torch.from_numpy(x_validate1.reshape((sample_size, 5))).float().to(DEVICE)\n",
    "y_validate1 = torch.from_numpy(y_validate1.reshape((sample_size, 1))).float().to(DEVICE)\n",
    "\n",
    "num_steps = 3000\n",
    "\n",
    "print(f\"Initial weight values: {model1.weight}\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "  model1.train()\n",
    "  optimizer.zero_grad()\n",
    "  y_ = model1(x_train1)\n",
    "  loss = loss_fn(y_, y_train1)\n",
    "  print(f\"Step {step}: train loss: {loss}\")\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  model1.eval()\n",
    "  with torch.no_grad():\n",
    "    y_ = model1(x_validate1)\n",
    "    val_loss = loss_fn(y_, y_validate1)\n",
    "  \n",
    "  print(f\"Step {step}: val loss: {val_loss}\")\n",
    "\n",
    "print(f\"Initial weight values: {model1.weight}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_validate1[:,1].cpu().numpy(), y_validate1.cpu().numpy(), \".\")\n",
    "ax.plot(x_validate1[:,1].cpu().numpy(), y_.cpu().numpy(), \".\")\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=16)\n",
    "ax.set_ylabel(\"y\", fontsize=16)\n",
    "\n",
    "# Bonus: Plot the evolution of each coefficient of w as a function of the gradient descent iterations.\n",
    "# This plotting was made with 500 training samples and 500 validation samples so with the initial given conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1d2e77-c76a-438a-a90d-7957785b8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "else \"cpu\")\n",
    "\n",
    "model = nn.Linear(5, 1)\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "num_steps = 3000\n",
    "w0_evolution = []\n",
    "w1_evolution = []\n",
    "w2_evolution = []\n",
    "w3_evolution = []\n",
    "w4_evolution = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  y_ = model(x_train)\n",
    "  loss = loss_fn(y_, y_train)\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  w0_evolution.append(model.weight[0,0].item())\n",
    "  w1_evolution.append(model.weight[0,1].item())\n",
    "  w2_evolution.append(model.weight[0,2].item()) \n",
    "  w3_evolution.append(model.weight[0,3].item()) \n",
    "  w4_evolution.append(model.weight[0,4].item()) \n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    y_ = model(x_validate)\n",
    "    val_loss = loss_fn(y_, y_validate)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac90886-324f-4db6-8971-67ac02c73644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Evolution')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAG3CAYAAABPMqr+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQGZJREFUeJzt3Xl8VNXh///3TEgmREgCBBIIAcLqAgZKIEYqSEkJ7nxqvz+KfGQplUrB0sa2gguoteKnVj5a1NqqlfqpFJcqWqRRGwm4BIEAKgpBNsOWsEkSEwiQnN8fMSMz2SaZfeb1fDzmEXLvufeeuSTMm3PPYjHGGAEAAMDO6u8KAAAABBoCEgAAgBMCEgAAgBMCEgAAgBMCEgAAgBMCEgAAgBMCEgAAgBMCEgAAgJN2/q5AsKqtrdWhQ4fUsWNHWSwWf1cHAAC4wBijiooK9ejRQ1Zr0+1EBKQ2OnTokFJSUvxdDQAA0Ab79+9Xz549m9wfEgFp3bp1evjhh1VYWKjDhw/rtdde08SJE5s9Jj8/Xzk5Ofrss8+UkpKiu+++W9OnT3f5mh07dpRUd4NjY2PdqD0AAPCV8vJypaSk2D/HmxISAamyslJpaWn68Y9/rB/84Actlt+7d6+uueYa3XrrrXrhhReUl5enn/zkJ+revbuys7Ndumb9Y7XY2FgCEgAAQaal7jEhEZCuuuoqXXXVVS6Xf+qpp5SamqpHHnlEknTRRRfp/fff1//+7/+6HJAAAEDoCstRbAUFBcrKynLYlp2drYKCgiaPqa6uVnl5ucMLAACEprAMSCUlJUpMTHTYlpiYqPLycp06darRYxYvXqy4uDj7iw7aAACErrAMSG2xYMEClZWV2V/79+/3d5UAAICXhEQfpNZKSkpSaWmpw7bS0lLFxsaqffv2jR5js9lks9l8UT0AAOBnYdmClJmZqby8PIdt77zzjjIzM/1UIwAAEEhCIiB9/fXX2rp1q7Zu3Sqpbhj/1q1bVVxcLKnu8djUqVPt5W+99Vbt2bNHv/nNb7Rjxw49+eSTeumll/TLX/7SH9UHAAABJiQC0qZNmzRs2DANGzZMkpSTk6Nhw4Zp4cKFkqTDhw/bw5Ikpaam6s0339Q777yjtLQ0PfLII3rmmWcY4g8AACRJFmOM8XclglF5ebni4uJUVlbGRJEAAAQJVz+/Q6IFCQAAwJMISAAAAE7Ccpg/AADwsz3LpE8flKoOSObsNxutkmolWSRbF+niX0sX5filegQkAADgvoOrpC3zpfLdqgs5td/ssEoykiznbTvX8vmqS6Qtt0ufPyTdeMQLFW4eAQkAADTu4Crp44VSeZFUe+abjfWtPOf/2YXA01bVR6XtS3zekkRAAgAg3OxZJn18n3TqkOoCThtbeXxl/6sEJAAA0EYutficafzYQJbyA59fkoAEAEAwaNDHJ4BbfDzJ1tUvHbUJSAAA+FvVAWnjPOnw29+0/Piwj0/AiNS3gc/6zSi2XzGKDQCAkHV8o7TlTunYeqn29Dcb68PP+SO+Qkk71QUeixxbu+q3WaXYgdKw30nJ1/qvmk0gIAEA4K6qA3UB6MDrUk3VNxvrA1CN6gJCKDi/lcfi+GdLpHRBT2nwAqnvdD/W0TMISAAAuKLJeX6M6kJQMItS4608FikmWbr0npAIPa1BQAIAoF6jw9+DuQ9QEy0+lkipywhp+B/qvqIBAhIAILw02RIUbCGosfATvi0+nkZAAgCEpo8XSkVLpXOV+jZIBEsIOr+Dc2j28Ql0BCQAQPCqHx5/KFcyZxQ8j8ScW3+sUmRHaeBsKe1+/1YNkghIAIBg0Ogw+UDuHO0cgCKk6ATpol/6bV4ftA4BCQAQWOyPxr7+ZkOgDpN3mucnsqM0cA4tQCGCgAQA8A97Z+ldqgtBgbhOWJToAxSeCEgAAO9r0GE6kILQeS1BlnZSl3SGv4OABADwoEY7TQdCGLJ+86IlCK4hIAEA2ub4RmlTjnRis2TOqW7kmL/XFDvvkVi7C6Se10lDH5Rievq5Xgg2BCQAQMsarDXm747T9aPE5PdV3xGaCEgAgIYc+gzVL7jqDwQh+AcBCQDCnX39scOqaxXyV5+hbx6PRbSX+v1YSn/UT/UACEgAEF4aPCrzx4zT9R2m20mxA6Vhv5OSr/VDPYCmEZAAIJQ5LMx6Vr5/VBYlydBhGkGHgAQAocT+uOyQfP+orJ0kK32FEBIISAAQzLYvkT5fIlUfl3S6xeKeEynWF0MoIyABQDDZvkT67GHpzFeSqn10UaukCNYaQ1ghIAFAIPNLC1EkfYYQ9ghIABBIHIbc+6KFKFKy2qS4QdKl9zKaDPgGAQkA/Kl+uY7jm+SbFqJIHpUBLiAgAYCv2WepLpPXl+uwxLAwK9AGBCQA8LY9y6RPH6ybpNGc8u61LO2lLiOk4X+o+wqgTQhIAOANm34h7XpWqv3aixexSpZoqUs6gQjwMAISAHiCfcbqXfJq52rrBVKPCXXrlDG6DPAaAhIAtJW9L9FJ713DEkMLEeAHBCQAcJXDQq/l3rmGtQND7oEAQEACgOYcXFXXUlT2mWS8sbaZTYpJli69h1FmQAAhIAGAs/pRZ5W75JVh+BEdpX4/rutHBCAgEZAAQDpvBut9nj83j82AoENAAhC+7Au/lnj+3JGdma0aCGIEJADhxWstRdFS7EBp2O9oJQJCAAEJQOjzViiiLxEQsqz+roCnPPHEE+rTp4+io6OVkZGhDRs2NFv+0Ucf1aBBg9S+fXulpKTol7/8pU6f9sVCkQB84uAqadVgablFWj/Dc+EosrN0yT3STUaaVE44AkJUSLQgvfjii8rJydFTTz2ljIwMPfroo8rOzlZRUZG6devWoPzy5cs1f/58/fWvf9Xll1+unTt3avr06bJYLFqyZIkf3gEAjzi+UdqUIx3/QB4dfRbTl2H4QJixGGO8vJS092VkZGjEiBF6/PHHJUm1tbVKSUnRbbfdpvnz5zcoP3fuXG3fvl15eXn2bbfffrs++ugjvf/++41eo7q6WtXV3y4fUF5erpSUFJWVlSk2NtbD7whAq2z6hbTzCUnnPHfOTsMZdQaEoPLycsXFxbX4+R30j9jOnDmjwsJCZWVl2bdZrVZlZWWpoKCg0WMuv/xyFRYW2h/D7dmzR6tXr9bVV1/d5HUWL16suLg4+yslJcWzbwRA62xfIv2zZ90jtJ2Pyf1wZK0LRWP+Vff47KpNhCMgjAX9I7Zjx46ppqZGiYmJDtsTExO1Y8eORo+56aabdOzYMX33u9+VMUbnzp3TrbfeqjvvvLPJ6yxYsEA5OTn27+tbkAD4kH1B2M88d87E70tDf8c6ZwAcBH1Aaov8/Hw9+OCDevLJJ5WRkaFdu3Zp3rx5+u1vf6t77rmn0WNsNptsNpuPawpA0jeP0JZKqvXM+Xh8BqAFQR+QEhISFBERodLSUoftpaWlSkpKavSYe+65RzfffLN+8pOfSJKGDBmiyspKzZo1S3fddZes1qB/8ggEv+1LpM+XSNUHPXM+QhGAVgj6JBAVFaXhw4c7dLiura1VXl6eMjMzGz2mqqqqQQiKiIiQJIVAn3UgeB3fKP3n+3X9irbc7n446jBQuuw5+hQBaLWgb0GSpJycHE2bNk3p6ekaOXKkHn30UVVWVmrGjBmSpKlTpyo5OVmLFy+WJF133XVasmSJhg0bZn/Eds899+i6666zByUAPrRnmbRxrlRT6f65ontKF/1Suiin5bIA0ISQCEiTJk3S0aNHtXDhQpWUlGjo0KHKzc21d9wuLi52aDG6++67ZbFYdPfdd+vgwYPq2rWrrrvuOv3ud7/z11sAwk/VAWnjPOngqx44WYQ0cC6TNgLwmJCYB8kfXJ1HAYCTPcukwl9JZ4+7fy5GoAFoJVc/v0OiBQlAENj0C2nnH+X2DNcdBkqDFzCrNQCvIiAB8B770h+Nz1DvOh6hAfAtAhIAz9u+RPr4bqn2lHvnSRgtDf8Dj9AA+BwBCYDnfLxQ+uy37p3DEl3Xr4hRaAD8iIAEwD2eGo1GaxGAAEJAAtA2B1dJH/1UOn3IjZNESMN+T2sRgIBDQALQOtuXSFvnS+Zs28/Bsh8AAhwBCYBrti+RtvxKbg3Tv+QeKe1+j1UJALyFgASgee52vLZGS2l0ugYQXAhIABq36RfSzsfafjydrgEEMQISgG9VHZC23Cl9+X9tP8fAeUzoCCDoEZAA1M14/eE0qWJ7G09glYY9zGM0ACGDgASEs6oD0lujpFPFbTs+ooM0YinrogEIOQQkIBxVHZDyb5BObm7b8Rf0l767nP5FAEIWAQkIJ+4+SosbJo19Q4rp6dl6AUCAISAB4WLVxVJ5G4NRn5uloQ8SjACEDQISEOrcGa7PiDQAYYqABIQqdyZ4ZMZrAGGOgASEmu1LpC23t+3YYY8wVB8AREACQoc7a6URjADAAQEJCHYHV0lrJ0qqaf2xPEoDgEYRkIBgdXyj9J9xUk1F64+lxQgAmkVAAoKNO7Nf02IEAC4hIAHBJP866dCq1h/HcH0AaBUCEhAM9iyT1s9o/XEdBknX7/B4dQAg1BGQgEB2cJW07kbJnGndcbZE6cp/sVYaALQRAQkIVK/2kE4fbt0xER2krHcJRgDgJgISEGjaNAN2pDTmVSn5Wq9UCQDCDQEJCBQHV0lrr1erJ3q87Dmp73Rv1AgAwhYBCfC3qgPSm2nS2ROtO44h+wDgNQQkwJ/enyQVv9S6Y2xJ0o2t7JsEAGgVAhLgDwdXSWtvkFTbioPoZwQAvkJAAnzttRTp1IHWHUM/IwDwKQIS4Cttmewx6Rrpe22YORsA4BYCEuBtVQekf10k1Xzt+jGRnaRrPpFienqvXgCAJhGQAG9qSyfsMf+inxEA+BkBCfCG4xultzIl1bh+DI/TACBgEJAAT8sdIZ3Y5Hp5a4x0fRGP0wAggBCQAE85vlF66zK1aug+o9MAICARkABPaG2rEZM9AkBAIyAB7mhLqxGdsAEg4BGQgLZqbasRnbABIGgQkIDWanWrUTtp4l46YQNAECEgAa3x9nelYx+4Xr7n/5NGt3IeJACA3xGQAFdUHZBeHyCZ0y4eQKsRAAQzq78r4ClPPPGE+vTpo+joaGVkZGjDhg3Nlj958qTmzJmj7t27y2azaeDAgVq9erWPaougsunn0soU18NRz/8n3XSWcAQAQSwkWpBefPFF5eTk6KmnnlJGRoYeffRRZWdnq6ioSN26dWtQ/syZM/r+97+vbt266ZVXXlFycrK+/PJLxcfH+77yCGyvdpdOl7hYmFYjAAgVFmOM8Xcl3JWRkaERI0bo8ccflyTV1tYqJSVFt912m+bPn9+g/FNPPaWHH35YO3bsUGRkpEvXqK6uVnV1tf378vJypaSkqKysTLGxsZ55IwgcxzdKb410vTx9jQAgKJSXlysuLq7Fz++gf8R25swZFRYWKisry77NarUqKytLBQUFjR7zxhtvKDMzU3PmzFFiYqIGDx6sBx98UDU1Ta+btXjxYsXFxdlfKSkpHn8vCBC5I1oRjiKkifsJRwAQYoI+IB07dkw1NTVKTEx02J6YmKiSksYfjezZs0evvPKKampqtHr1at1zzz165JFH9MADDzR5nQULFqisrMz+2r9/v0ffBwJA1QFpeZTrcxvFf0e66RyP1AAgBIVEH6TWqq2tVbdu3fSXv/xFERERGj58uA4ePKiHH35YixYtavQYm80mm83m45rCZzb9XNq51PXy2RukLiO8Vx8AgF8FfUBKSEhQRESESktLHbaXlpYqKSmp0WO6d++uyMhIRURE2LdddNFFKikp0ZkzZxQVFeXVOiPAvNZTOnXQtbKsoQYAYSHoH7FFRUVp+PDhysvLs2+rra1VXl6eMjMzGz1m1KhR2rVrl2prv50JeefOnerevTvhKJxUHZCWt3M9HA2YSzgCgDAR9AFJknJycvT000/rb3/7m7Zv367Zs2ersrJSM2bMkCRNnTpVCxYssJefPXu2Tpw4oXnz5mnnzp1688039eCDD2rOnDn+egvwtY/vrpvbSE13zLez2Oo6Yo9oxSM4AEBQC/pHbJI0adIkHT16VAsXLlRJSYmGDh2q3Nxce8ft4uJiWa3fZsGUlBS99dZb+uUvf6lLL71UycnJmjdvnu644w5/vQX40uo06eQnrpXtkillf+jd+gAAAk5IzIPkD67Oo4AAszxaUnWLxSTRERsAQpCrn98h0YIEtKg1Ez9GxEqTyrxbHwBAQAuJPkhAs97+ruvhqM90whEAgBYkhLiX4qRz5a6VnbifSR8BAJIISAhly6MknW25XEQHaVKF16sDAAgePGJD6Dm+UVpukUvhqEsm4QgA0AAtSAgtb39XOvaBa2UZpQYAaAIBCaHD5f5GkdJNZ7xeHQBA8OIRG0LD8ijXwlF0MuEIANAiAhKCW9UB1/sbDZgr/eCA16sEAAh+PGJD8Nr0c2mni+ujMYQfANAKBCQEp5WpUtW+lstZoqXJp7xeHQBAaCEgIfi82FGq+brlctHJPFIDALQJfZAQXJZHuRaOuo0lHAEA2owWJASP5RbXyjG/EQDATQQkBL7jG11cbLaddJMLo9kAAGgBAQmBLf9a6dCbLZdjPTUAgAe53QepuLhYs2fP1oABAxQTE6OIiIhGX+3akcXQSv9Ody0cte9NOAIAeJRbqWXHjh0aNWqUTp48KWNMs2Vb2g84eDVFOu1CJ+s+M6TL/+r9+gAAwopbLUh33XWXvvrqK40fP17r169XWVmZamtrm3wBLnkpzrVwNPT3hCMAgFe41YK0du1a9erVS6+//rqioqI8VSeEsxUXSLVVLZdjZmwAgBe5FZCqqqr0ve99j3AEz1gepZbXVGOkGgDA+9wKSH379lVlZaWn6oJwttwqqaV+alHSTdW+qA0AIMy51Qfp5ptv1rp163T06FFP1QfhaLlFLYajqK6EIwCAz7gVkG6//XZlZmbqqquu0rZt2zxVJ4QTV2bHjk6WfnjE+3UBAOAbbj1iGz9+vM6ePavNmzdr6NCh6tWrl3r16iWrtWHuslgsysvLc+dyCDWuhKML+kk37PJ+XQAAOI/FuDFBUWNBqMkLWSyqqalp66UCTnl5ueLi4lRWVqbY2Fh/Vye4VB2QVqa0XI45jgAAHubq57dbLUhr1qxx53CEo8//IG39dcvlLr5LGvqA9+sDAEAj3ApIY8aM8VQ9EA623i19/ruWyw39vXSxCyEKAAAvYYE0+Mamn0s7l7ZcjgkgAQABwGMB6dChQ1q7dq0OHjwoSUpOTtbo0aOVnJzsqUsgWOWNk0rfbbncTazXBwAIDG4HpLKyMs2dO1crVqxosN6a1WrV5MmTtXTpUsXFxbl7KQSjf6dLXxW2XI5wBAAIIG4FpNOnTysrK0ubN2+WMUZpaWnq16+fJGnPnj3aunWrXnjhBe3YsUPvvfeebDabRyqNIPF6f6lyd8vlCEcAgADj1kSRS5cuVWFhoYYNG6ZNmzZpy5YteuWVV/TKK69o8+bNKiws1PDhw1VYWKilS13of4LQ8e90whEAIGi5NQ9Senq6du3apd27d6tLly6Nljl27Jj69++v/v37a9OmTW2uaKBhHqRmuPRYjUVnAQC+5+rnt1stSDt37tTYsWObDEeSlJCQoLFjx6qoqMidSyFYvN7fhXAURTgCAAQ0twJSTU2NIiMjWywXGRnZoAM3QpArj9WsMSw6CwAIeG4FpNTUVK1bt06nTp1qssypU6e0bt06paamunMpBLq3R7XccmSNkX5U6Zv6AADgBrcC0vXXX68jR45oypQpOnr0aIP9R48ete+bOHGiO5dCIHt7lHTswxYK2QhHAICg4VYn7a+++krDhg3T/v37FRMTowkTJthbivbs2aPc3FydOnVKvXv31ubNmxUfH++pevsdnbS/8Z9x0pEWJoGk5QgAECB8slhtp06dtGbNGk2ePFkbNmzQP//5T1ksFklSfe7KyMjQ8uXLQyoc4Rsbf044AgCEJLdn0k5NTdX69ev1wQcfKD8/32GpkSuvvFKjRo1yu5IIQJ8/LH3R0txWPFYDAAQntx6xhbOwfsRWdUBamdJ8GUu0NLnpzvsAAPiDT+ZBQphqKRwpinAEAAhqBCS0znJLCwUimecIABD0WhWQIiIi1K5dO+3cudP+vauvdu3c7u4Ef2sxHLWTbjrjk6oAAOBNrQpIxhiHGbGNMS6/vD2T9hNPPKE+ffooOjpaGRkZ2rBhg0vHrVixQhaLhXmaWrI8quUyLB8CAAgRrQpItbW1qq2t1cCBAx2+d/XlLS+++KJycnK0aNEibd68WWlpacrOztaRI0eaPW7fvn361a9+pSuuuMJrdQsJKy6Q1EL4uYm+/gCA0BESfZCWLFmiW265RTNmzNDFF1+sp556SjExMfrrX//a5DE1NTWaMmWK7rvvPvXt27fFa1RXV6u8vNzhFRZeipNqq5ovQzgCAIQYtwLS888/rw8/bGmJCWn9+vV6/vnn3blUk86cOaPCwkJlZWXZt1mtVmVlZamgoKDJ4+6//35169ZNM2fOdOk6ixcvVlxcnP2VktLSSK4Q8GpP6VwLQXDift/UBQAAH3IrIE2fPl3PPPNMi+WeffZZzZgxw51LNenYsWOqqalRYmKiw/bExESVlJQ0esz777+vZ599Vk8//bTL11mwYIHKysrsr/37QzwY/DtdOn2w+TKX3CXF9PRNfQAA8CGfDC0LpLkoKyoqdPPNN+vpp59WQkKCy8fZbDbZbDYv1iyA/Gec9FVh82USvyelPeCb+gAA4GM+CUhHjhxRTEyMV86dkJCgiIgIlZaWOmwvLS1VUlJSg/K7d+/Wvn37dN1119m31Xcgb9eunYqKitSvXz+v1DUouLK+Wqfh0rg839QHAAA/aHVAWrduncP3JSUlDbbVO3funD777DO9/fbbGjJkSNtq2IKoqCgNHz5ceXl59qH6tbW1ysvL09y5cxuUv/DCC/Xpp586bLv77rtVUVGhxx57LDz6FjXFlfXVontKV23yTX0AAPCTVgekK6+8UhbLtxMGvvXWW3rrrbeaPcYYo9mzZ7e+di7KycnRtGnTlJ6erpEjR+rRRx9VZWWlvd/T1KlTlZycrMWLFys6OlqDBw92OD4+Pl6SGmwPK1UHpK2/ab5Mu1jpByHe9woAALUhII0ePdoekNauXatu3brpwgsvbLRsVFSUevbsqRtvvFFXX321ezVtxqRJk3T06FEtXLhQJSUlGjp0qHJzc+0dt4uLi2W1hsSMBt6zsk/z+60x0v9X5pOqAADgbxbjRg9qq9Wq6dOnNzvfUKhydTXgoLDighbmOopifTUAQEhw9fPbrU7aa9asabQjNILIy51aCEftCEcAgLDjVkAaM2aMp+oBf1idLp092XwZ1lcDAIQhtwJScXFxq8r36tXLncvBk7beJZ1sYa6jbNcW/AUAINS4FZD69OnjMKKtORaLRefOnXPncvCUqgPS5w82X6bHNVKXEb6pDwAAAcatgNSrV69GA1Jtba0OHz5sD0S9e/d25zLwtJUttOR1Gi5duco3dQEAIAC5FZD27dvX5L5z584pNzdXt912m8aOHRuWI90C0ooLJDUzcDE6iYkgAQBhz2tLjbRr107XXnutUlJSNHLkSF122WWaNWuWty4HV7zas4URazbpB4d9Vh0AAAKV12dPTEtLU3p6up566ilvXwrNeWuUdPpgMwWs0k2nfVYdAAACmU+ml05OTtbOnTt9cSk0Zutd0vEPmy8z8Uvf1AUAgCDg9YBkjNEnn3yiyMhIb18KjXFlxNrA26SYnr6pDwAAQcCrAenYsWOaPXu2vvjiC1122WXevBSa8nr/5vd36Cel/9E3dQEAIEi41Um7b9++Te6rqKjQiRMnZIxRVFSU7rvvPncuhbZ4uZNkmlkmxNZVun6X7+oDAECQ8Nowf0mKiorS6NGj9cADD2jkyJHuXAqt9dblLSwjEindeMRXtQEAIKi4FZD27t3b5L6oqCh17dpV7dp5bSYBNOX4Rul4QfNlJu7xTV0AAAhCbqUXZsgOUG+10N/rkrvolA0AQDN8MswfPvRSnKTapvcnXC6lPeCz6gAAEIwISKFkdbp0rrzp/dFJ0vgPfFcfAACCVKsesTU3aq0lFotFu3fvbvPxaMHWu6SThc0UiGQZEQAAXNSqgNTSqLXmWCyWNh+LFrgyGSSdsgEAcFmrAlJzo9bgRytTm9/PTNkAALRKqwISo9YC0Ks9JZ1ren/7ZGbKBgCgleikHcw23iadPthMAZv0Xwd8Vh0AAEKFx2ZxPHTokNauXauDB+s+sJOTkzV69GglJyd76hI4X9UB6YvHmylgkW467bPqAAAQStwOSGVlZZo7d65WrFih2lrH+XesVqsmT56spUuXKi4uzt1L4Xz/urD5/dkf+aYeAACEILcC0unTp5WVlaXNmzfLGKO0tDT169dPkrRnzx5t3bpVL7zwgnbs2KH33ntPNpvNI5UOe6vTpZrKpvcnjJK6jPBdfQAACDFu9UFaunSpCgsLNWzYMG3atElbtmzRK6+8oldeeUWbN29WYWGhhg8frsLCQi1dutRTdQ5vLc13ZL1AGv++7+oDAEAIshhjTFsPTk9P165du7R792516dKl0TLHjh1T//791b9/f23atKnNFQ005eXliouLU1lZmWJjY31z0aoD0sqU5stM3M+QfgAAmuDq57dbLUg7d+7U2LFjmwxHkpSQkKCxY8eqqKjInUtBkt5ood8R8x0BAOARbgWkmpoaRUZGtlguMjKyQQdutNKHM6TaZvodxfRhviMAADzErYCUmpqqdevW6dSpU02WOXXqlNatW6fU1BZme0bTqg5I+5Y1vd8SLU1klnMAADzFrYB0/fXX68iRI5oyZYqOHj3aYP/Ro0ft+yZOnOjOpcLbyhYWCb7hC9/UAwCAMOFWJ+2vvvpKw4YN0/79+xUTE6MJEybYW4r27Nmj3NxcnTp1Sr1799bmzZsVHx/vqXr7nc86aa8eLp3c3PT+gbfxaA0AABe5+vnt1jxInTp10po1azR58mRt2LBB//znP2WxWCRJ9bkrIyNDy5cvD6lw5DPHNzYfjlhnDQAAr3B7Ju3U1FStX79eH3zwgfLz8x2WGrnyyis1atQotysZtvKym9kZwTprAAB4icfWYhs1ahRhyJMOrpLOfdX0/uwC39UFAIAw41Yn7X/9618M3/eWD/676X2d01lKBAAAL3IrIN1www1KSUnRHXfcoe3bt3uqTji4SjpX1sTOCGnCRp9WBwCAcONWQPrOd76jw4cP6+GHH9bgwYN1+eWX6+mnn1Z5ebmn6heevnyp6X1jVvqsGgAAhCu3AtKmTZv0ySef6Be/+IUSEhK0fv163XrrrerevbumTp2qd99911P1DC8RHRrfbkuUkq/1bV0AAAhDbgUkSRo8eLCWLFmigwcP6tVXX9W1116rs2fP6u9//7u+//3vKzU1Vffff7++/PJLT9Q3PHy9s/HtfW7ybT0AAAhTbk0U2ZSjR4/q//7v//Tcc8/ps88+k8VikdVq1dmzZz19Kb/x2kSRVQeklSmN70v/kzTwVs9dCwCAMOPq57fbLUiN6dq1q3JycrRhwwbNmzdPxhhGu7mqopllQ3ryeA0AAF/w2DxI51u/fr2ee+45vfTSS/YO2507d/bGpUJPuyb6H11ylxTT07d1AQAgTHksIB0+fFjPP/+8li1bpp07d8oYI6vVqvHjx2vGjBksVuuqr/c2vj0+zbf1AAAgjLkVkM6cOaOVK1dq2bJleuedd1RbWytjjPr166fp06dr+vTpSk5O9lRdAQAAfMKtPkjdu3fX5MmTlZubK5vNpptvvln5+fn64osvdNddd/k0HD3xxBPq06ePoqOjlZGRoQ0bNjRZ9umnn9YVV1yhTp06qVOnTsrKymq2vE91SG1iex+fVgMAgHDmVkD66quvdNlll+npp59WSUmJli1bptGjR3uqbi578cUXlZOTo0WLFmnz5s1KS0tTdna2jhw50mj5/Px8TZ48WWvWrFFBQYFSUlI0fvx4+0K7fnXu6ya2V/q2HgAAhDG3hvkXFRVp0KBBnqxPm2RkZGjEiBF6/PHHJUm1tbVKSUnRbbfdpvnz57d4fE1NjTp16qTHH39cU6dObbRMdXW1qqur7d+Xl5crJSXFS8P8e0k6/6/FKk38kk7aAAC4ySvD/J9//nl9+OGH9u/PD0fl5eU6ffp0o8f94x//UE5OTmsu5bIzZ86osLBQWVlZ9m1Wq1VZWVkqKHBtxfuqqiqdPXu22ZF2ixcvVlxcnP2VktLEXEVe4fGpqgAAQDNaFZCmT5+uZ555ptF9nTp10pw5cxrd9/bbb+uxxx5rfe1ccOzYMdXU1CgxMdFhe2JiokpKSlw6xx133KEePXo4hCxnCxYsUFlZmf21f/9+t+rdpIov1DAQGalil3euBwAAGvDYMH9jjLwwKbfXPfTQQ1qxYoXy8/MVHR3dZDmbzSabzeb9CnUcIMmiBo/YOvb3/rUBAIAkL00U6UsJCQmKiIhQaWmpw/bS0lIlJSU1e+wf/vAHPfTQQ/rPf/6jSy+91JvVdFPwBU8AAIKZV5Ya8aWoqCgNHz5ceXl59m21tbXKy8tTZmZmk8f9/ve/129/+1vl5uYqPT3dF1V1DY/YAADwu6BvQZKknJwcTZs2Tenp6Ro5cqQeffRRVVZWasaMGZKkqVOnKjk5WYsXL5Yk/c///I8WLlyo5cuXq0+fPva+Sh06dFCHDk0s9eErHQeoLreet3adJYJHbAAA+FBIBKRJkybp6NGjWrhwoUpKSjR06FDl5ubaO24XFxfLav22sexPf/qTzpw5ox/+8IcO51m0aJHuvfdeX1a9oZieUurN0t6/fbutz38zxB8AAB8KiYAkSXPnztXcuXMb3Zefn+/w/b59+7xfobaqOiDt/T/Hbfv+LqU9QEgCAMBHWh2Qdu3apeeff75V+3btov+Myyq+kMPjNUkyNXV9kAhIAAD4RKtm0rZarbJYLK2+iDFGFotFNTU1rT42ULk6E2erVR2QVvZWgz5IN+wjIAEA4CZXP79b1YLUq1evNgUktEJMTynjL9KGn9a1HFkipJF/JhwBAOBDrQpIAd13J5R0z5YuXy7JInXNJBwBAOBjIdNJO2Tsflb6aJbqHrFZ61qT+s30d60AAAgrQT9RZEipOnBeOFLd1w0/rdsOAAB8hoAUSJobwQYAAHyGgBRI7LNon4dZtAEA8DkCUiCpH8Fmiaj7nhFsAAD4BZ20A02/mVL8pdLR96Wu35W6jPB3jQAACDsEpEDDKDYAAPyOR2yBhFFsAAAEBAJSIGEUGwAAAYGAFEgYxQYAQEAgIAUSRrEBABAQ6KQdaPrNrFuLrWJXXcsR4QgAAJ8jIAWimJ4EIwAA/IhHbAAAAE4ISIGo6oBUuobh/QAA+AmP2AINE0UCAOB3tCAFEiaKBAAgIBCQAgkTRQIAEBAISIGEiSIBAAgIBKRAwkSRAAAEBDppBxomigQAwO8ISIGIiSIBAPArHrEBAAA4ISABAAA4ISABAAA4ISAFIpYaAQDAr+ikHWhYagQAAL+jBSmQsNQIAAABgYAUSFhqBACAgEBACiQsNQIAQEAgIAUSlhoBACAg0Ek70LDUCAAAfkdACkQsNQIAgF/xiA0AAMAJAQkAAMAJAQkAAMAJAQkAAMAJAQkAAMAJAQkAAMAJAQkAAMAJASkQVR2QStewSC0AAH4SMgHpiSeeUJ8+fRQdHa2MjAxt2LCh2fIvv/yyLrzwQkVHR2vIkCFavXq1j2ragt3PSit7S3nfq/u6+1l/1wgAgLATEgHpxRdfVE5OjhYtWqTNmzcrLS1N2dnZOnLkSKPlP/zwQ02ePFkzZ87Uli1bNHHiRE2cOFHbtm3zcc2dVB2QPpolqfabDbXShp/SkgQAgI9ZjDHG35VwV0ZGhkaMGKHHH39cklRbW6uUlBTddtttmj9/foPykyZNUmVlpVatWmXfdtlll2no0KF66qmnGr1GdXW1qqur7d+Xl5crJSVFZWVlio2N9cwbKV1T13LkbNwaKfFKz1wDAIAwVl5erri4uBY/v4O+BenMmTMqLCxUVlaWfZvValVWVpYKCgoaPaagoMChvCRlZ2c3WV6SFi9erLi4OPsrJSXFM2/gfB0HqMFfiSWibtFaAADgM0EfkI4dO6aamholJiY6bE9MTFRJSUmjx5SUlLSqvCQtWLBAZWVl9tf+/fvdr7yzmJ5Sxl/qQpFU93Xkn1m4FgAAH2vn7woEC5vNJpvN5v0L9Zspdc+WKnbVtRwRjgAA8LmgD0gJCQmKiIhQaWmpw/bS0lIlJSU1ekxSUlKryvtcTE+CEQAAfhT0j9iioqI0fPhw5eXl2bfV1tYqLy9PmZmZjR6TmZnpUF6S3nnnnSbLAwCA8BL0LUiSlJOTo2nTpik9PV0jR47Uo48+qsrKSs2YMUOSNHXqVCUnJ2vx4sWSpHnz5mnMmDF65JFHdM0112jFihXatGmT/vKXv/jzbQAAgAAREgFp0qRJOnr0qBYuXKiSkhINHTpUubm59o7YxcXFslq/bSy7/PLLtXz5ct1999268847NWDAAK1cuVKDBw/211sAAAABJCTmQfIHV+dRAAAAgSNs5kECAADwNAISAACAEwISAACAEwISAACAEwISAACAEwJSIKo6IJWuqfsKAAB8LiTmQQopu5+VPpolqVaStW7x2n4z/V0rAADCCi1IgaTqwHnhSHVfN/yUliQAAHyMgBRIKr7Qt+HoG6ZGqtjll+oAABCuCEiBpOMANfgrsURIHfv7pToAAIQrAlIgielZ1+fIElH3vSVCGvnnuu0AAMBn6KQdaPrNlLpn1z1W69ifcAQAgB8QkAJRTE+CEQAAfsQjNgAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEJAAAACcEpEBTdUAqXVP3FQAA+EU7f1cA59n9rPTRLEm1kqxSxl+kfjP9XSsAAMIOLUiBourAeeFIdV83/JSWJAAA/ICAFCgqvtC34egbpkaq2OWX6gAAEM4ISIGi4wA1+OuwREgd+/ulOgAAhDMCUqCI6VnX58gSUfe9JUIa+ee67QAAwKfopB1I+s2UumfXPVbr2J9wBACAnxCQAk1MT4IRAAB+xiM2AAAAJwQkAAAAJwQkAAAAJwQkAAAAJwQkAAAAJwQkAAAAJ0EfkE6cOKEpU6YoNjZW8fHxmjlzpr7++utmy992220aNGiQ2rdvr169eunnP/+5ysrKfFhrAAAQyII+IE2ZMkWfffaZ3nnnHa1atUrr1q3TrFmzmix/6NAhHTp0SH/4wx+0bds2LVu2TLm5uZo5c6YPaw0AAAKZxRhj/F2Jttq+fbsuvvhibdy4Uenp6ZKk3NxcXX311Tpw4IB69Ojh0nlefvll/fd//7cqKyvVrp1rc2eWl5crLi5OZWVlio2NbfN7AAAAvuPq53dQtyAVFBQoPj7eHo4kKSsrS1arVR999JHL56m/Sc2Fo+rqapWXlzu8AABAaArqgFRSUqJu3bo5bGvXrp06d+6skpISl85x7Ngx/fa3v232sZwkLV68WHFxcfZXSkpKm+sNAAACW0AGpPnz58tisTT72rFjh9vXKS8v1zXXXKOLL75Y9957b7NlFyxYoLKyMvtr//79bl8fAAAEpoBcrPb222/X9OnTmy3Tt29fJSUl6ciRIw7bz507pxMnTigpKanZ4ysqKjRhwgR17NhRr732miIjI5stb7PZZLPZXKo/AAAIbgEZkLp27aquXbu2WC4zM1MnT55UYWGhhg8fLkl69913VVtbq4yMjCaPKy8vV3Z2tmw2m9544w1FR0d7rO4AACD4BeQjNldddNFFmjBhgm655RZt2LBBH3zwgebOnasf/ehH9hFsBw8e1IUXXqgNGzZIqgtH48ePV2VlpZ599lmVl5erpKREJSUlqqmp8efbAQAAASIgW5Ba44UXXtDcuXM1btw4Wa1W3XjjjfrjH/9o33/27FkVFRWpqqpKkrR582b7CLf+/fs7nGvv3r3q06ePz+oOAAACU1DPg+RPzIMEAEDwCYt5kAAAALyBgAQAAOCEgAQAAOCEgBRoqg5IpWvqvgIAAL8I+lFsIWX3s9JHsyTVSrJKGX+R+s30d60AAAg7tCAFiqoD54Uj1X3d8FNakgAA8AMCUqCo+ELfhqNvmBqpYpdfqgMAQDgjIAWKjgPU4K/DEiF17N9ocQAA4D0EpEAR07Ouz5Elou57S4Q08s912wEAgE/RSTuQ9Jspdc+ue6zWsT/hCAAAPyEgBZqYngQjAAD8jEdsAAAATghIAAAATghIAAAATghIAAAATghIAAAATghIAAAATghIAAAATghIAAAATghIAAAATghIAAAATghIAAAATliLrY2MMZKk8vJyP9cEAAC4qv5zu/5zvCkEpDaqqKiQJKWkpPi5JgAAoLUqKioUFxfX5H6LaSlCoVG1tbU6dOiQOnbsKIvF4rHzlpeXKyUlRfv371dsbKzHzhuKuFetw/1yHffKddwr13GvXOfNe2WMUUVFhXr06CGrtemeRrQgtZHValXPnj29dv7Y2Fh+gVzEvWod7pfruFeu4165jnvlOm/dq+ZajurRSRsAAMAJAQkAAMAJASnA2Gw2LVq0SDabzd9VCXjcq9bhfrmOe+U67pXruFeuC4R7RSdtAAAAJ7QgAQAAOCEgAQAAOCEgAQAAOCEgAQAAOCEgBZgnnnhCffr0UXR0tDIyMrRhwwZ/V8mn7r33XlksFofXhRdeaN9/+vRpzZkzR126dFGHDh104403qrS01OEcxcXFuuaaaxQTE6Nu3brp17/+tc6dO+frt+IV69at03XXXacePXrIYrFo5cqVDvuNMVq4cKG6d++u9u3bKysrS1988YVDmRMnTmjKlCmKjY1VfHy8Zs6cqa+//tqhzCeffKIrrrhC0dHRSklJ0e9//3tvvzWPa+leTZ8+vcHP2oQJExzKhMu9Wrx4sUaMGKGOHTuqW7dumjhxooqKihzKeOp3Lz8/X9/5zndks9nUv39/LVu2zNtvz6NcuVdXXnllg5+tW2+91aFMONyrP/3pT7r00kvtkz1mZmbq3//+t31/wP9MGQSMFStWmKioKPPXv/7VfPbZZ+aWW24x8fHxprS01N9V85lFixaZSy65xBw+fNj+Onr0qH3/rbfealJSUkxeXp7ZtGmTueyyy8zll19u33/u3DkzePBgk5WVZbZs2WJWr15tEhISzIIFC/zxdjxu9erV5q677jKvvvqqkWRee+01h/0PPfSQiYuLMytXrjQff/yxuf76601qaqo5deqUvcyECRNMWlqaWb9+vXnvvfdM//79zeTJk+37y8rKTGJiopkyZYrZtm2b+cc//mHat29v/vznP/vqbXpES/dq2rRpZsKECQ4/aydOnHAoEy73Kjs72zz33HNm27ZtZuvWrebqq682vXr1Ml9//bW9jCd+9/bs2WNiYmJMTk6O+fzzz83SpUtNRESEyc3N9en7dYcr92rMmDHmlltucfjZKisrs+8Pl3v1xhtvmDfffNPs3LnTFBUVmTvvvNNERkaabdu2GWMC/2eKgBRARo4caebMmWP/vqamxvTo0cMsXrzYj7XyrUWLFpm0tLRG9508edJERkaal19+2b5t+/btRpIpKCgwxtR9KFqtVlNSUmIv86c//cnExsaa6upqr9bd15w/9Gtra01SUpJ5+OGH7dtOnjxpbDab+cc//mGMMebzzz83kszGjRvtZf79738bi8ViDh48aIwx5sknnzSdOnVyuF933HGHGTRokJffkfc0FZBuuOGGJo8J13tljDFHjhwxkszatWuNMZ773fvNb35jLrnkEodrTZo0yWRnZ3v7LXmN870ypi4gzZs3r8ljwvVeGWNMp06dzDPPPBMUP1M8YgsQZ86cUWFhobKysuzbrFarsrKyVFBQ4Mea+d4XX3yhHj16qG/fvpoyZYqKi4slSYWFhTp79qzDPbrwwgvVq1cv+z0qKCjQkCFDlJiYaC+TnZ2t8vJyffbZZ759Iz62d+9elZSUONyfuLg4ZWRkONyf+Ph4paen28tkZWXJarXqo48+spcZPXq0oqKi7GWys7NVVFSkr776ykfvxjfy8/PVrVs3DRo0SLNnz9bx48ft+8L5XpWVlUmSOnfuLMlzv3sFBQUO56gvE8z/xjnfq3ovvPCCEhISNHjwYC1YsEBVVVX2feF4r2pqarRixQpVVlYqMzMzKH6mWKw2QBw7dkw1NTUOPwiSlJiYqB07dvipVr6XkZGhZcuWadCgQTp8+LDuu+8+XXHFFdq2bZtKSkoUFRWl+Ph4h2MSExNVUlIiSSopKWn0HtbvC2X176+x93/+/enWrZvD/nbt2qlz584OZVJTUxuco35fp06dvFJ/X5swYYJ+8IMfKDU1Vbt379add96pq666SgUFBYqIiAjbe1VbW6tf/OIXGjVqlAYPHixJHvvda6pMeXm5Tp06pfbt23vjLXlNY/dKkm666Sb17t1bPXr00CeffKI77rhDRUVFevXVVyWF17369NNPlZmZqdOnT6tDhw567bXXdPHFF2vr1q0B/zNFQEJAueqqq+x/vvTSS5WRkaHevXvrpZdeCpp/EBAcfvSjH9n/PGTIEF166aXq16+f8vPzNW7cOD/WzL/mzJmjbdu26f333/d3VQJeU/dq1qxZ9j8PGTJE3bt317hx47R7927169fP19X0q0GDBmnr1q0qKyvTK6+8omnTpmnt2rX+rpZLeMQWIBISEhQREdGgB39paamSkpL8VCv/i4+P18CBA7Vr1y4lJSXpzJkzOnnypEOZ8+9RUlJSo/ewfl8oq39/zf0MJSUl6ciRIw77z507pxMnToT9Pezbt68SEhK0a9cuSeF5r+bOnatVq1ZpzZo16tmzp327p373mioTGxsbdP8BaupeNSYjI0OSHH62wuVeRUVFqX///ho+fLgWL16stLQ0PfbYY0HxM0VAChBRUVEaPny48vLy7Ntqa2uVl5enzMxMP9bMv77++mvt3r1b3bt31/DhwxUZGelwj4qKilRcXGy/R5mZmfr0008dPtjeeecdxcbG6uKLL/Z5/X0pNTVVSUlJDvenvLxcH330kcP9OXnypAoLC+1l3n33XdXW1tr/Ec/MzNS6det09uxZe5l33nlHgwYNCspHRq46cOCAjh8/ru7du0sKr3tljNHcuXP12muv6d13323w2NBTv3uZmZkO56gvE0z/xrV0rxqzdetWSXL42QqHe9WY2tpaVVdXB8fPlNvdvOExK1asMDabzSxbtsx8/vnnZtasWSY+Pt6hB3+ou/32201+fr7Zu3ev+eCDD0xWVpZJSEgwR44cMcbUDQvt1auXeffdd82mTZtMZmamyczMtB9fPyx0/PjxZuvWrSY3N9d07do1ZIb5V1RUmC1btpgtW7YYSWbJkiVmy5Yt5ssvvzTG1A3zj4+PN6+//rr55JNPzA033NDoMP9hw4aZjz76yLz//vtmwIABDkPXT548aRITE83NN99stm3bZlasWGFiYmKCbuh6c/eqoqLC/OpXvzIFBQVm79695j//+Y/5zne+YwYMGGBOnz5tP0e43KvZs2ebuLg4k5+f7zA0vaqqyl7GE7979UOyf/3rX5vt27ebJ554IuiGrrd0r3bt2mXuv/9+s2nTJrN3717z+uuvm759+5rRo0fbzxEu92r+/Plm7dq1Zu/eveaTTz4x8+fPNxaLxbz99tvGmMD/mSIgBZilS5eaXr16maioKDNy5Eizfv16f1fJpyZNmmS6d+9uoqKiTHJyspk0aZLZtWuXff+pU6fMz372M9OpUycTExNj/uu//sscPnzY4Rz79u0zV111lWnfvr1JSEgwt99+uzl79qyv34pXrFmzxkhq8Jo2bZoxpm6o/z333GMSExONzWYz48aNM0VFRQ7nOH78uJk8ebLp0KGDiY2NNTNmzDAVFRUOZT7++GPz3e9+19hsNpOcnGweeughX71Fj2nuXlVVVZnx48ebrl27msjISNO7d29zyy23NPjPSLjcq8bukyTz3HPP2ct46ndvzZo1ZujQoSYqKsr07dvX4RrBoKV7VVxcbEaPHm06d+5sbDab6d+/v/n1r3/tMA+SMeFxr3784x+b3r17m6ioKNO1a1czbtw4ezgyJvB/pizGGON+OxQAAEDooA8SAACAEwISAACAEwISAACAEwISAACAEwISAACAEwISAACAEwISAACAEwISAACAEwISgKCye/duWSwWWa1WHT16tNEyf//732WxWGSxWPT3v/+90TJHjx6V1WqVxWLR7t27tW/fPvsx9a8HHnjA4Zjy8nLdc889uuaaa9SvXz/FxcUpKipKPXr00A033KA333yz0WuVlJQ0OPe9997r1n0A4F0EJABBpV+/fkpJSZExRmvXrm20zJo1a+x/zs/Pb7RMfn6+jDFKSUlRv3797NsvuOACTZs2TdOmTVNaWprDMUeOHNEDDzygdevWqVu3bho3bpyuv/56devWTW+88YauvfZazZkzp8G12rdv3+Q5AQSmdv6uAAC01tixY/X8889rzZo1+uEPf9hgf35+vrp27SqbzdZsQKo/1/kSEhK0bNmyRo9JSkpSQUGB0tPT1a6d4z+fa9as0bXXXqsnn3xS119/vbKzs+374uLi7Oe899579fHHH7v2RgH4DS1IAIJOfag5v6Wo3v79+7Vnzx6NGTNGY8aM0e7du7V///4G5eqPdQ5IzenQoYMuu+yyBuGo/jw/+tGPJElvv/22y+cEEJgISACCTn2o2b59u0pLSx321bcMXXnllRozZozDtnqlpaXavn27w7k8oT442Ww2j50TgH8QkAAEnd69eys1NVVSw/BT/319C5LUsKWpvkxqaqp69+7tkTpt3LhRL774oiwWi6677jqPnBOA/9AHCUBQGjt2rPbu3as1a9Zo0qRJ9u31/Y8uueQSWSwWJSUlNRmi3Gk9WrhwoYqLi3Xq1Cnt3btXGzduVFRUlP74xz8qMzOzzecFEBhoQQIQlBrrh1RcXKw9e/Zo9OjRslgskupakvbu3asvv/zSXq4t/Y+cvfHGG/rb3/6ml156SRs3blSHDh30+OOPa/bs2W0+J4DAQUACEJTqw83OnTt1+PBhSY6P1+o590MqKSlRUVGRwznaYuvWrTLGqKysTBs2bND111+vWbNmafz48aqoqGjzeQEEBgISgKCUnJysAQMGSPq2Rej8Dtr1nANS/dcBAwYoOTnZ7XrExsZqxIgReuGFF/Szn/1M7777ru677z63zwvAvwhIAIKW82O2/Px8denSRYMHD7aXufjii9W1a1d7GU88XmvKjBkzJEmvvfaax88NwLcISACC1vkBqbi4WHv37nXof1Rv9OjR+vLLL7Vv3z6PdNBuygUXXCCpbsZtAMGNgAQgaNU/Stu9e7d9zbXzH6/Vq3/M9sILL2jnzp1NlnNXXl6eJGngwIEePzcA3yIgAQhaSUlJuuiiiyRJjzzyiKTmA9KSJUskSRdddJGSkpJafb3ly5ersLCwwXZjjF599VXdfffdkqRZs2a1+twAAgvzIAEIamPHjtX27dt14sQJde7cWUOGDGlQZsiQIercubNOnDhhP6Yt3n77bU2ZMkU9e/bUpZdeqvj4eB0/flw7duywTyMwZ84cAhIQAmhBAhDUzg87jfU/kiSLxaIrrrii0WNa45ZbbtHPf/5zJSUlafPmzXr55Zf13nvvyWazadq0aXrvvff0+OOPN1oHAMGFFiQAQe2HP/yhjDEtllu5cqXb1xo1apRGjRrl9nkABD4CEgCc59ixY5o+fbok6cYbb/TIumplZWWaN2+epLoJJgEEPgISAJynsrJSf/vb3yRJ/fv390hAOnXqlP2cAIKDxbjSNg0AABBG6KQNAADghIAEAADghIAEAADghIAEAADghIAEAADghIAEAADghIAEAADghIAEAADghIAEAADg5P8H4zwNgqLmIPYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAG2CAYAAACUDjeHAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAP9ZJREFUeJzt3Xl8VPW9//H3TFYiJCyBDIQkbG5sYY+RK0uJorUqFnsRVBAtthR5QGMtolbjGq22lxa5UrVIrSL0ikB/VUEbCS5lkQCyiCBrAJmwJ4FAQjLn9wdkZMhMSDJ7zuv5eMyjcL7fOeczpwnz9nu+53sshmEYAgAAaOSswS4AAAAgEAg9AADAFAg9AADAFAg9AADAFAg9AADAFAg9AADAFAg9AADAFCKDXUCocDgc+v7779WsWTNZLJZglwMAAOrAMAyVlpaqXbt2slprH8sh9Jz3/fffKyUlJdhlAACABti3b5/at29fax9Cz3nNmjWTdO6kxcfHB7kaAABQFyUlJUpJSXF+j9eG0HNe9SWt+Ph4Qg8AAGGmLlNTmMgMAABMgdADAABMgdADAABMgdADAABMgdADAABMgdADAABMgdADAABMgdADAABMgdADAABMgdADAABMgdADAABMIWRDz6xZs9ShQwfFxsYqIyNDa9asqbX/iRMnNGnSJLVt21YxMTG64oor9OGHHwao2tqV2e0qWr1aZXZ7sEsBAMC0QvKBowsWLFB2drZmz56tjIwMzZgxQ8OHD9e2bdvUpk2bGv0rKip0/fXXq02bNnrvvfeUnJysvXv3qnnz5oEv/iI7Fy7U6pwcyeGQrFZl5OSo88iRwS4LAADTsRiGYQS7iItlZGSof//+euWVVyRJDodDKSkpmjx5sh555JEa/WfPnq2XXnpJ3377raKioup0jPLycpWXlzv/Xv1o+uLiYp89Zb3Mbtfi668/F3jOs1ituu2TTxRns/nkGAAAmFlJSYkSEhLq9P0dcpe3KioqVFBQoKysLOc2q9WqrKwsrVy50u17/vnPfyozM1OTJk1SUlKSunfvrueff15VVVUej5Obm6uEhATnKyUlxeefpXTvXpfAI0mGw6HSwkKfHwsAANQu5ELPkSNHVFVVpaSkJJftSUlJsnuYE7Nr1y699957qqqq0ocffqjf/e53+sMf/qBnn33W43GmT5+u4uJi52vfvn0+/RyS1CwtTbK6nmKL1apmqak+PxYAAKhdyIWehnA4HGrTpo1ee+019e3bV6NGjdJjjz2m2bNne3xPTEyM4uPjXV6+FmezKSMnR5bzwcditWpATg6XtgAACIKQm8icmJioiIgIFRUVuWwvKiqSzUNYaNu2raKiohQREeHcdvXVV8tut6uiokLR0dF+rbk2nUeOVNuBA1VaWKhmqakEHgAAgiTkRnqio6PVt29f5eXlObc5HA7l5eUpMzPT7XsGDhyoHTt2yHHB/Jnt27erbdu2QQ081eJsNiUNGEDgAQAgiEIu9EhSdna2Xn/9df3tb3/T1q1bNXHiRJ06dUrjx4+XJI0dO1bTp0939p84caKOHTumKVOmaPv27frggw/0/PPPa9KkScH6CAAAIMSE3OUtSRo1apQOHz6sJ554Qna7Xb169dLSpUudk5sLCwtlvWCCcEpKipYtW6Zf//rX6tmzp5KTkzVlyhRNmzYtWB8BAACEmJBcpycY6nOfPwAACA1hvU4PAACAPxB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKYR06Jk1a5Y6dOig2NhYZWRkaM2aNR77zp07VxaLxeUVGxsbwGoBAEAoC9nQs2DBAmVnZ+vJJ5/UunXrlJ6eruHDh+vQoUMe3xMfH6+DBw86X3v37g1gxQAAIJSFbOj54x//qAkTJmj8+PHq2rWrZs+erbi4OM2ZM8fjeywWi2w2m/OVlJTksW95eblKSkpcXgAAoPEKydBTUVGhgoICZWVlObdZrVZlZWVp5cqVHt938uRJpaWlKSUlRbfddpu2bNnisW9ubq4SEhKcr5SUFJ9+BgAAEFpCMvQcOXJEVVVVNUZqkpKSZLfb3b7nyiuv1Jw5c7RkyRK9/fbbcjgcuvbaa7V//363/adPn67i4mLna9++fT7/HAAAIHREBrsAX8nMzFRmZqbz79dee62uvvpq/eUvf9EzzzxTo39MTIxiYmICWSIAAAiikBzpSUxMVEREhIqKily2FxUVyWaz1WkfUVFR6t27t3bs2OGPEgEAQJgJydATHR2tvn37Ki8vz7nN4XAoLy/PZTSnNlVVVdq0aZPatm3rrzIBAEAYCdnLW9nZ2Ro3bpz69eunAQMGaMaMGTp16pTGjx8vSRo7dqySk5OVm5srSXr66ad1zTXXqEuXLjpx4oReeukl7d27Vz//+c+D+TEAAECICNnQM2rUKB0+fFhPPPGE7Ha7evXqpaVLlzonNxcWFspq/WGg6vjx45owYYLsdrtatGihvn376j//+Y+6du0arI8AAABCiMUwDCPYRYSCkpISJSQkqLi4WPHx8cEuBwAA1EF9vr9Dck4PAACArxF6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AACAKRB6AqDMblfR6tUqs9uDXQoAAKYVGewCGrudCxdqdU6O5HBIVqsycnLUeeTIYJcFAIDpMNLjR2V2+w+BR5IcDq3JyWHEBwCAICD0+FHp3r0/BJ7zDIdDpYWFQaoIAADzIvT4UbO0NMnqeootVquapaYGqSIAAMyL0ONHcTabMnJyZDkffCxWqwbk5CjOZgtyZQAAmA8Tmf2s88iRajtwoEoLC9UsNZXAAwBAkBB6AiDOZiPsAAAQZFzeAgAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoAQAApkDoCYAyu11Fq1erzG4PdikAAJhWZLALaOx2Llyo1Tk5ksMhWa3KyMlR55Ejg10WAACmw0iPH5XZ7T8EHklyOLQmJ4cRHwAAgoDQ40ele/f+EHjOMxwOlRYWBqkiAADMK6RDz6xZs9ShQwfFxsYqIyNDa9asqdP75s+fL4vFohEjRvi3wEtolpYmWV1PscVqVbPU1CBVBACAeYVs6FmwYIGys7P15JNPat26dUpPT9fw4cN16NChWt+3Z88e/eY3v9F1110XoEo9i7PZlJGTI8v54GOxWjUgJ0dxNluQKwMAwHwshmEY3uygsLBQubm5+ve//60DBw6ovLzc/YEsFlVWVtZ5vxkZGerfv79eeeUVSZLD4VBKSoomT56sRx55xO17qqqqNGjQIN133336/PPPdeLECS1evNht3/LycpdaS0pKlJKSouLiYsXHx9e5zroos9tVWlioZqmpBB4AAHyopKRECQkJdfr+9ururW+//VYDBw7UiRMndKnsVJ9sVVFRoYKCAk2fPt25zWq1KisrSytXrvT4vqefflpt2rTR/fffr88//7zWY+Tm5uqpp56qc03eiLPZCDsAAASZV5e3HnvsMR0/flw33HCDVq1apeLiYjkcDo+vujpy5IiqqqqUlJTksj0pKUl2D3c+ffHFF/rrX/+q119/vU7HmD59uoqLi52vffv21bk+AAAQfrwa6VmxYoVSU1O1ZMkSRUdH+6qmeistLdU999yj119/XYmJiXV6T0xMjGJiYvxcGQAACBVehZ6ysjL96Ec/8nngSUxMVEREhIqKily2FxUVyebmMtHOnTu1Z88e3XLLLc5t1SNLkZGR2rZtmzp37uzTGgEAQHjx6vJWp06ddOrUKV/V4hQdHa2+ffsqLy/Puc3hcCgvL0+ZmZk1+l911VXatGmTNmzY4HzdeuutGjp0qDZs2KCUlBSf1wgAAMKLVyM999xzj5599lkdPnxYrVu39lVNkqTs7GyNGzdO/fr104ABAzRjxgydOnVK48ePlySNHTtWycnJys3NVWxsrLp37+7y/ubNm0tSje3BUma3q3TvXjVLS2NSMwAAQeBV6HnooYeUl5enm266SXPnzvVpwBg1apQOHz6sJ554Qna7Xb169dLSpUudk5sLCwtltYbsMkMueP4WAADB59U6PT/60Y909uxZffnll7JarUpNTVVqaqrbMGKxWFwuV4Wa+tznXx9ldrsWX3+9y+MoLFarbvvkE0Z8AADwUsDW6cnPz3f+2eFwaM+ePdqzZ4/bvhaLxZtDha3anr9F6AEAIHC8Cj3Lly/3VR2NlvP5WxeN9PD8LQAAAsur0DN48GBf1dFoVT9/a01OjgyHg+dvAQAQJF6FHtRN55Ej1fyKK3Ro3Tq16dNHrXr0CHZJAACYjs9Cz/fff68VK1bowIEDkqTk5GQNGjRIycnJvjpE2OLuLQAAgs/r0FNcXKwHH3xQ8+fPr/F8LavVqtGjR2vmzJlKSEjw9lBhqcxu/yHwSJLDoTU5OWo7cCCXuAAACCCvQs+ZM2eUlZWldevWyTAMpaenOx/3sGvXLm3YsEHvvPOOvv32W33++eemfNYVd28BABAavFrdb+bMmSooKFDv3r21du1arV+/Xu+9957ee+89rVu3TgUFBerbt68KCgo0c+ZMX9UcVpqlpUkX365vsXD3FgAAAeZV6FmwYIHi4+O1bNky9enTp0Z779699eGHH6pZs2aaP3++N4cKbxev/9jw9SABAEADeRV6tm/frqFDh6pVq1Ye+yQmJmro0KHatm2bN4cKW/svWMDxQhv+538CWwgAACbnVeipqqpSVFTUJftFRUXVmORsFiW7d7vdvudf/1KZ3R7gagAAMC+vQk/Hjh312Wef6fTp0x77nD59Wp999pk6duzozaHCVnwtn9vTKBAAAPA9r0LPrbfeqkOHDumuu+7S4cOHa7QfPnzY2TZixAhvDhW22g8Z4rGt1MNzygAAgO95dcv6ww8/rHnz5mnJkiX65JNPdOONNzpHdHbt2qWlS5fq9OnTSktL029+8xufFNyYeLr0BQAAfM+r0NOiRQstX75co0eP1po1a7Rw4ULn09SN83coZWRkaN68eWrevLnXxYaj0r17PbYd/OILldntrNcDAEAAeL0ic8eOHbVq1Sp9+eWXys/Pd3kMxZAhQzRw4ECviwxnzdLSam3fn5+vK+68M0DVAABgXj579tbAgQNNH3DcibPZlPH001r9xBNu25nXAwBAYHg1kRl103nkSHUeNcptG/N6AAAIDEJPgER4WM+oel4PAADwr3qFnoiICEVGRmr79u3Ov9f1FRnpsytpYam29XoOb9gQuEIAADCpeoUewzBcVlY2DKPOL7OuyFyt1vV6CgsDVwgAACZVr+GXi4OL2YNMfcTZbErs3VtH1q+v0XZy374gVAQAgLkwpyeAmnm4xHVo7doAVwIAgPl4FXreeust/ec//7lkv1WrVumtt97y5lCNQqtu3dxuP1lYqKObNgW4GgAAzMWr0HPvvffqjTfeuGS/v/71rxo/frw3h2oUapvXs+eDDwJXCAAAJhSQy1vVj6QwqzK7XUWrV0uSWngY7TnsZq4PAADwnYDcR37o0CHFxcUF4lAhZ+fChVqdkyM5HJLVqqQBA9z2O7Z5M8/hAgDAj+odej777DOXv9vt9hrbqlVWVmrLli36+OOP1aNHj4ZVGMbK7PYfAo8kORwqWrXKY3+ewwUAgP/UO/QMGTLE+SR1SVq2bJmWLVtW63sMw9DEiRPrX12YK92794fAc4HY1q115vDhGtuPbdkSiLIAADCleoeeQYMGOUPPihUr1KZNG1111VVu+0ZHR6t9+/YaOXKkfvzjH3tXaRhqlpYmWa0uwcditaplt276Pj+/Rv8T51e6BgAAvlfv0JN/wZe11WrVTTfdpDlz5viypkYjzmZTRk6O1uTkyHA4ZLFaNSAnR8e/+85tf+b1AADgP15NZF6+fLlsfEHXqvPIkWo7cKBKCwvVLDVVcTabjm7apO1//7vb/szrAQDAP7wKPYMHD/ZVHY1anM3mMnrTqkcPXZaSolNuHj/BvB4AAPzDq9BTWM8HZaampnpzuEalZbdubkNPbXd3AQCAhvMq9HTo0MHlTq7aWCwWVVZWenO4RqVJ69Zut5/6/nsd3bRJrUx4iz8AAP7kVehJTU11G3ocDocOHjzoDDlpaWneHKZR6njzzR7n9ez54ANCDwAAPuZV6NmzZ4/HtsrKSi1dulSTJ0/W0KFDucPrIq169NBlqak65eYS4VHm9QAA4HN+e/ZWZGSkfvKTn2jx4sV655139Nprr/nrUGGr8+23u91+ZN06ldntAa4GAIDGze8PHE1PT1e/fv00e/Zsfx8q7DRLSfHYtt/N4oUAAKDhAvKU9eTkZG1nteEaWvfu7bGNW9cBAPAtv4cewzC0ceNGRUVF+ftQYSfOZlOLbt3ctnHrOgAAvuXX0HPkyBFNnDhR3333na655hp/Hipste7Tx+326lvXAQCAb3h191anTp08tpWWlurYsWMyDEPR0dF66qmnvDlUo8Wt6wAABIbfblmXzj1lfdCgQXr22Wc1YMAAbw7VaLXq0UNN2rXT6e+/r9F2eP36IFQEAEDj5FXo2b17t8e26OhotW7dWpGRXh3CFGzXXKPd779fYztPXQcAwHe8SiSstOwbrbp1cxt6JJ66DgCArwTklnXUrv2QIR7b9nzwQeAKAQCgESP0hIA4m02JHtbsYXVmAAB8o16Xt2q7W+tSLBaLdu7c2eD3N3YdfvITHfEwcZlLXAAAeK9eoedSd2vVxt3T2PGD9kOGaO0zz7htY3VmAAC8V6/QU9vdWvBO9erMx90EHFZnBgDAe/UKPYG+W2vWrFl66aWXZLfblZ6erpkzZ3pc7+f999/X888/rx07dujs2bO6/PLL9dBDD+mee+4JaM3eaN2nj9vQU706MwsVAgDQcCE7kXnBggXKzs7Wk08+qXXr1ik9PV3Dhw/XoUOH3PZv2bKlHnvsMa1cuVIbN27U+PHjNX78eC1btizAlTdcx5tv9thW8PvfB7ASAAAaH4thGIYvdvT9999rxYoVOnDggKRzT1YfNGiQkpOTG7S/jIwM9e/fX6+88ookyeFwKCUlRZMnT9YjjzxSp3306dNHN998s55xM1emvLxc5eXlzr+XlJQoJSVFxcXFio+Pb1DNvvD+0KE64yHYjcjLY6FCAAAuUFJSooSEhDp9f3s90lNcXKx77rlHaWlpuvvuuzVt2jRNmzZNd999tzp06KCxY8equLi4XvusqKhQQUGBsrKyfijUalVWVpZWrlx5yfcbhqG8vDxt27ZNgwYNctsnNzdXCQkJzldKSkq9avSXK0aP9ti2Pz8/cIUAANDIeBV6zpw5o6ysLM2bN09VVVXq2bOnbr/9dt1+++1KT09XVVWV3nnnHV1//fUuoyqXcuTIEVVVVSkpKclle1JSkuy1rFlTXFyspk2bKjo6WjfffLNmzpyp66+/3m3f6dOnq7i42Pnat29fnevzp0633uqxjbu4AABoOK9Cz8yZM1VQUKDevXtr7dq1Wr9+vd577z299957WrdunQoKCtS3b18VFBRo5syZvqrZo2bNmmnDhg366quv9Nxzzyk7O1v5HkZHYmJiFB8f7/IKBdV3cbmzPy8vwNUAANB4eBV6FixYoPj4eC1btkx9+vSp0d67d299+OGHatasmebPn1/n/SYmJioiIkJFRUUu24uKimSrZU6L1WpVly5d1KtXLz300EO64447lJubW/cPFCJauzmXklRRXKwDXOICAKBBvAo927dv19ChQ9WqVSuPfRITEzV06FBt27atzvuNjo5W3759lXfByIbD4VBeXp4yMzPrvB+Hw1Gvy2qhora7uHYuWRLASgAAaDy8esp6VVWVoqKiLtkvKipKDoejXvvOzs7WuHHj1K9fPw0YMEAzZszQqVOnNH78eEnS2LFjlZyc7BzJyc3NVb9+/dS5c2eVl5frww8/1N///ne9+uqr9f9gQdaqRw/Ftmnj9i6uo19/HYSKAAAIf16Fno4dO+qzzz7T6dOn1aRJE7d9Tp8+rc8++0wdO3as175HjRqlw4cP64knnpDdblevXr20dOlS5+TmwsJCWa0/DFSdOnVKv/rVr7R//341adJEV111ld5++22NGjWq4R8wiFKHD9f2v/+9xvbTRUUsVAgAQAN4tU7Po48+qhdeeEEjRozQX/7yF7Vu3dql/fDhw/rFL36hJUuWaPr06Xr22We9Lthf6nOffyAc3bRJyzw8ZDSxTx/d4CYQAQBgNvX5/vYq9Bw/fly9e/fWvn37FBcXpxtvvNE5orNr1y4tXbpUp0+fVlpamtatW6fmzZs39FB+F2qhR2KhQgAALqU+399eXd5q0aKFli9frtGjR2vNmjVauHCh82nq1VkqIyND8+bNC+nAE6quGD1aG//0J7dt+/PzdYWHkSAAAFCTzx5D8eWXXyo/P9/lMRRDhgzRwIEDfbF7vwvFkZ4yu12Lhw1z29aye3fduGBBgCsCACC0BGyk50IDBw4Mm4ATLuJsNiV06aLiHTtqtB3bvFlldjuXuAAAqCOv1un5f//v/9X7VnTUT1ota/bs+uc/A1gJAADhzavQc9tttyklJUXTpk3T1q1bfVUTLlDbs7i2v/tuACsBACC8eRV6+vTpo4MHD+qll15S9+7dde211+r1119XSUmJr+ozvdqexXXm0CEd3bQpwBUBABCevAo9a9eu1caNGzV16lQlJiZq1apV+uUvf6m2bdtq7Nix+vTTT31Vp6l1/ulPPbZtmDEjcIUAABDGfHb3VmVlpf71r3/pzTff1EcffaTKykpZLBalpqZq/PjxGjdunNLS0nxxKL/w591bZXa7SvfuVbO0tAZNPK7tLi6JNXsAAOZVn+9vr0Z6LhQZGakRI0ZoyZIlOnDggF5++WV17dpVe/fu1VNPPaUuXbr46lBhZefChVp8/fXKu+8+Lb7+eu1cuLDe+4iz2ZR0zTUe2/fz5HUAAC7JZ6HnQq1bt1Z2drbWrFmjKVOmyDAMU97lVWa3a3VOjlT92R0OrcnJUZndXu999Zo61WPbrkWLGlYgAAAm4pfQs2rVKv3iF79Qu3bt9Oc//1mS1LJlS38cKqSV7t37Q+A5z3A4VFpYWO99VT953Z3qNXsAAIBnPgs9Bw8e1Isvvqirr75aAwcO1Ouvv67S0lLdcMMNmj9/vnOlZjNplpYmWV1PscVqVbPU1Abt74rRoz22sWYPAAC18yr0VFRU6B//+Id+/OMfKzU1VY8++qi2bdumTp066ZlnntHevXv10Ucf6b//+78VHR3tq5rDRpzNpoycHFnOBx+L1aoBOTkNnnRc25o92955p0H7BADALLx6DEXbtm114sQJGYahuLg43XHHHbrvvvs0aNAgX9UX9jqPHKm2AweqtLBQzVJTvbrLKs5mU3yXLipx81iK8iNHdHTTJrXq0cObcgEAaLS8umXdarUqMzNT9913n0aNGqWmTZv6sraACsUHjrpzID9fKyZNctvWNDVVt370UYArAgAgeAL2wNGtW7fqyiuv9GYXqKfkIUMU2bSpKk+erNF2srCQ0R4AADyo15yet956S//5z3+cf78w8JSUlOjMmTNu3/fuu+8qOzu7gSXiYl3vv99j23f/+EcAKwEAIHzUK/Tce++9euONN9y2tWjRQpM8XHb5+OOP9ac//an+1cGt2iY0F378cQArAQAgfPjslnXDMOSjJ1rgEuJsNiX27u22rfLkSR1ghWYAAGrwy+KE8L++06Z5bFv91FMBrAQAgPBA6AlTrXr0UIyHVa7PHDqko5s2BbgiAABCG6EnjF15zz0e27787W8DWAkAAKGP0BPGapvQXH37OgAAOIfQE8bibDYlDxvmsb3g978PYDUAAIS2ei9OuGPHDr311lv1atvh5rEJ8I3+jz6qA3l5btuOrFunMrvdq0dfAADQWNTrMRRWq1UWi6XeBzEMQxaLRVVVVfV+b6CEy2Mo3Pn47rt1ZP16t209p0xR9wceCHBFAAAERn2+v+sVejp06NCg0FNt9+7dDX6vv4Vz6Dm6aZOW3Xmn27aIuDiN+uqrAFcEAEBg+O3ZW3v27PGmLvhJqx49FNu6tc4cPlyjraqsTGuff179Hn00CJUBABA6mMjcSGTk5Hhs2/7OOyqz2wNXDAAAIYjQ00hUP33dk13//GcAqwEAIPQQehqRgS++6LHtm7/+NYCVAAAQegg9jUhtoz08iBQAYHaEnkamttGeLx5+OICVAAAQWgg9jUzykCGyxsa6bau+kwsAADMi9DRCAx5/3GMbd3IBAMyK0NMIdbr9dlljYjy2M9oDADAjQk8jdd0f/+ixbX9eHqM9AADTIfQ0Updat2fFgw8GsBoAAIKP0NOI1XYn1/GtW3V006YAVgMAQHARehqx5CFDFN2ihcf2L7KzA1gNAADBRehp5Ia++qrHtlPff89oDwDANAg9jVyrHj0U37mzx/ZPJ0wIYDUAAAQPoccEfvTaax7bzpaWcgs7AMAUCD0mEGezKWngQI/tLFgIADADQo9JZD79dK3tn/7iFwGqBACA4CD0mESczabLx4zx2F6yYweTmgEAjRqhx0T6P/ZYrQsWfjxuXACrAQAgsAg9JjPsjTc8thnl5fr4rrsCWA0AAIFD6DGZS93CfmTDBi5zAQAaJUKPCdV2C7vE2j0AgMYppEPPrFmz1KFDB8XGxiojI0Nr1qzx2Pf111/XddddpxYtWqhFixbKysqqtb+Zxdls6vrAAx7bWbsHANAYhWzoWbBggbKzs/Xkk09q3bp1Sk9P1/Dhw3Xo0CG3/fPz8zV69GgtX75cK1euVEpKim644QYdOHAgwJWHh15TpuiytDSP7azdAwBobCyGYRjBLsKdjIwM9e/fX6+88ookyeFwKCUlRZMnT9YjjzxyyfdXVVWpRYsWeuWVVzR27NhL9i8pKVFCQoKKi4sVHx/vdf3hoMxu1+Jhwzy2W2NjdWdBQQArAgCgfurz/R2SIz0VFRUqKChQVlaWc5vValVWVpZWrlxZp32UlZXp7Nmzatmypdv28vJylZSUuLzMJs5mU4cRIzy2O86c4W4uAECjEZKh58iRI6qqqlJSUpLL9qSkJNnreMll2rRpateunUtwulBubq4SEhKcr5SUFK/rDkfXPvdcrWv3cDcXAKCxCMnQ460XXnhB8+fP16JFixQbG+u2z/Tp01VcXOx87du3L8BVho7/Xr1aslg8ti+7884AVgMAgH+EZOhJTExURESEioqKXLYXFRXJZrPV+t6XX35ZL7zwgj7++GP17NnTY7+YmBjFx8e7vMxs8Pm5U578Y8CAAFUCAIB/hGToiY6OVt++fZWXl+fc5nA4lJeXp8zMTI/v+/3vf69nnnlGS5cuVb9+/QJRaqORPGSImtQSKCtPnWJ+DwAgrIVk6JGk7Oxsvf766/rb3/6mrVu3auLEiTp16pTGjx8vSRo7dqymT5/u7P/iiy/qd7/7nebMmaMOHTrIbrfLbrfr5MmTwfoIYef2vDwpIsJjO/N7AADhLGRDz6hRo/Tyyy/riSeeUK9evbRhwwYtXbrUObm5sLBQBw8edPZ/9dVXVVFRoTvuuENt27Z1vl5++eVgfYSwNOLjj2ttZ34PACBchew6PYFmxnV6PPnquef03bx5HtsjmjTRqLVrA1gRAADuhf06PQiu/o89ptjWrT22V50+rcUelgIAACBUEXrg1k/z82ud31N28KBWPv544AoCAMBLhB54dKn5PbsXLdI3c+YEqBoAALxD6IFHcTabLh8zptY+G/7wBx5MCgAIC4Qe1Kr/Y4/Vun6PpFofWgoAQKgg9OCSbs/LU0STJrX2mVfL6tcAAIQCQg/qZNTatbVObFZVlealpweuIAAA6onQgzobs3Fj7R0qKwk+AICQRehBvYy44HlobhF8AAAhitCDeomz2dTroYdq70TwAQCEIEIP6q3rffdd8lZ2gg8AINQQetAg/R97TG0HDaq9U2Uld3UBAEIGoQcNNvTVV9VxxIjaO1VVaV63bgGpBwCA2hB64JXM555Tm4yMS/ab160bKzcDAIKK0AOvZc2Zo6Q6BJ/Fw4bxrC4AQNAQeuATw+bMufSlLp17Vtd/HnvM/wUBAHARQg98JvO55y49uVnSnsWLtYjndQEAAozQA58a+uqr6vbAA5fsd9pu1/y+fQNQEQAA5xB64HPpU6ZceuVmSY4zZ5jgDAAIGEIP/CLOZtOYLVvq1HfxsGH66rnn/FwRAMDsCD3wqzFbtkiRkZfs9928eVp43XUBqAgAYFaEHvjdmK+/VkSTJpfsV37smOZ166ajmzYFoCoAgNkQehAQo9auVdOUlDr1XXbnnfpXHW5/BwCgPgg9CJhbly7V4Fmz6tS35LvvNK9bNx3Iz/dvUQAA07AYhmEEu4hQUFJSooSEBBUXFys+Pj7Y5TR687p3l+r4oxfburV+SvgBALhRn+9vRnoQFGM2b1ZsYmKd+p45fFjzunXTrkWL/FwVAKAxI/QgaH66YoWuuOuuOvdf9fjjmt+nD+v6AAAahNCDoOr36KMakZcna2xsnfo7ysu1eNgwffqrX/m5MgBAY0PoQdDF2Wy6s6BAnepxx5Z9xQoueQEA6oWJzOcxkTk0lNnt+ufNN8tx5kzd32S16pqnn1an22/3X2EAgJDERGaErYaM+sjh0KrHH9e8nj25xR0A4BEjPecx0hN6yux2fXjHHao4frxe77NGR+u6//kfJQ8Z4p/CAAAhg5EeNApxNpvu+OKLcwsaWix1fp+jokIrJk3Su+npzPkBADgRehDykocM0ZjNm5U6fHi93mdUVp677NW9u7bOneuf4gAAYYPLW+dxeSs8lNntWnb33Tp98GCD3n/F3Xer3/TpPq4KABAs9fn+JvScR+gJL0c3bVLe/fer8tSpBr2/Rdeu6jlpEvN+ACDMEXoagNATng7k5+uzqVNlnD3bsB1Yrer90EO6+t57fVoXACAwCD0NQOgJb7sWLdLqp55qePiRlNinj/r+9rdq1aOHDysDAPgToacBCD2Nw4H8fH2enS1HeXnDd2K16ooxY5j7AwBhgNDTAISexuVAfr7+8+ijOltc7NV+ouLjdcWYMUqfPNlHlQEAfInQ0wCEnsbp6KZN2jBjhopWrfJ6XzGJieo6fjzzfwAghBB6GoDQ0/itzc3V9rff9sm+GAECgNBA6GkAQo957Fq0SOteflkVJ074ZH8RcXFKTE9XrylTmAQNAAFG6GkAQo/5lNnt+io3Vwf+/W+f7rdpWpq6T5jAU98BIAAIPQ1A6DG3XYsWaeOrr6rswAGf7tcaF6d2116rftOnK85m8+m+AQCEngYh9KDa1zNnauvcuXKcOePzfTMXCAB8i9DTAIQeXKzMbtf6GTO096OPpMpK3x8gIkJRl11GCAIALxB6GoDQg9pU3/p+aN06GRUV/jlIZKSs0dFcDgOAeiD0NAChB3VVPQK0f/lyVZ086ddjWWJidJnNxsRoAPCA0NMAhB401Na5c/XNnDkqP3rU/weLiJA1Kkrt/uu/GA0CABF6GoTQA184kJ+v9TNmqLSwUIY3z/+qj8hIRcTGqvOIETwvDIDp1Of72xqgmupt1qxZ6tChg2JjY5WRkaE1a9Z47LtlyxaNHDlSHTp0kMVi0YwZMwJXKHCB5CFD9JPFizV63TqN2bJF3X75S0U1by5FRPjvoJWVqjp5UtvfflvzunXTvJ49Na9nTy3o3195P/+5jm7a5L9jA0AYiQx2Ae4sWLBA2dnZmj17tjIyMjRjxgwNHz5c27ZtU5s2bWr0LysrU6dOnfSzn/1Mv/71r4NQMeBe+uTJzjuzXOYCnT4tVVX556Dn91tVVqailSu1bOXKc9sjIyWrVbEtWujqsWN5hhgA0wnJy1sZGRnq37+/XnnlFUmSw+FQSkqKJk+erEceeaTW93bo0EFTp07V1KlT63VMLm8hGNbm5mrHwoVyVFT4LwTV5vwIlDUqSgmdOqnnpElKHjIk8HUAQAPV5/s75EZ6KioqVFBQoOkXzE2wWq3KysrSyur/YvWB8vJylV8w56KkpMRn+wbqqt/06S7zcJyToouL/bM20MXOBy1HVZWOf/ONVkyadG47YQhAIxRyoefIkSOqqqpSUlKSy/akpCR9++23PjtObm6unnrqKZ/tD/CFq++91+Wyk8vE6KqqwAQh6ZJhSFaropo21RWjRrGwIoCwEXKhJ1CmT5+u7Oxs599LSkqUkpISxIqAmpKHDKkxwuIyGmQYgb0sVn2sqiqdPX5cW2bP1pbZs8+FIYvlXD0REYpPS1PvqVMZHQIQUkIu9CQmJioiIkJFRUUu24uKimTz4ZokMTExiomJ8dn+gEC5eDRIOjc3aOeiRaoqLw98EJJcj1dVpZLvvvthdCgy8lxNkiyRkSy2CCBoQi70REdHq2/fvsrLy9OIESMknZvInJeXpwcffDC4xQEh6uK5QdIFI0IlJT8EoWDct3DBJTmjqkon9+7Vqscf16rHH//hctn5UaKImBh1/ulPWW8IgF+EXOiRpOzsbI0bN079+vXTgAEDNGPGDJ06dUrjx4+XJI0dO1bJycnKzc2VdG7y8zfffOP884EDB7RhwwY1bdpUXbp0CdrnAILJ3YjQ0U2bVPDiizq2dasclZXBGRW60EXHrior0/a339b2t992GSGSxSJJioqL4wGtABosJG9Zl6RXXnlFL730kux2u3r16qU///nPysjIkCQNGTJEHTp00Ny5cyVJe/bsUceOHWvsY/DgwcrPz6/T8bhlHWa2a9EibXz1VZUdPiw5HMEPQ3Vx4TwiSbJYWJkaMCEeQ9EAhB6gJrdhqPp/Q92Fl86qWSyKSUhQ1/HjWZwRaCQIPQ1A6AHqrsbq0tK5cBGoW+p9xc1oEcEICC+EngYg9AC+Ub22UMnevecukVWHilC/XOaJu2AkKSI6Wonp6eo1ZYpa9egRxAIBcyP0NAChB/C/GostXhgkwm2U6GIX3YlW/WdLZKRade2qvr/9LeEI8ANCTwMQeoDg+3rmTG2bN0+VZWXnNlT/81Q9l6gxcDdyZBjnLqs1b85lNaCeCD0NQOgBQpvHeUTV/4SF6+UzTzyMHFX/ObJJE7UfOlS9pk5VnA8XbgXCDaGnAQg9QPirsTK19MPdW8FanDEQLBbJav3hzxd/douFZ6Wh0SL0NAChB2j8jm7apA0zZujIxo3ngpHkGhLC5XZ8b11iFIm5SAgnhJ4GIPQAkOoQjBrDpOuG8HAXm6QftkVEKLZFC109dizzkhAwhJ4GIPQAqI8azzZzN2LS2OYZ1VcdRpSYxA1vEXoagNADwB+2zp2rrX/7m86cOPFDCLr4i78xzzdqiLqGJUmyWpmvZHKEngYg9AAIpjK7XV/l5urgl1/KcfZs7V/2Zh9Bqo2nwFSNO+EaHUJPAxB6AIQTj2saXfxl73A0njWOAqG2O+FqCVGWyEhdZrOp+4QJ6nT77QEsGISeBiD0AGis6jWKJDGS5Av1uUR3cYiKiFB8Wpp6T52q5CFDAlZyuCL0NAChBwB+UKe5SNW45OZfkZF1C061BKrI2NhGewmP0NMAhB4A8M7RTZtU8OKLOrZ1qxyVlXX7QmYSd+DV9RJeHS/tRV12ma4YMyZoE8kJPQ1A6AGA4Kj35bfqP5tlMclwUsuolL8WvCT0NAChBwDCT50DUzXuhAsJHW+7TZnPP++TfRF6GoDQAwDmVOc74ZjP5FPD58/3yYhPfb6/I70+GgAAYSx98mSv56McyM/X+hkzVFpYKOPCeUr1nSdjokecHF6/PuDPdSP0AADgpeQhQ3x2e3l1gCrZu/fcKFJDJhuHwbPiWvfuHfBjEnoAAAghvgxQ1Rp0Cc+Pl/Y63nZbwEd5JOb0ODGnBwCA+lubm6sdCxfKUVFRa1ji7q0QQugBACD81Of72xqgmgAAAIKK0AMAAEyB0AMAAEyB0AMAAEyB0AMAAEyB0AMAAEyB0AMAAEyB0AMAAEyB0AMAAEyB0AMAAEyB0AMAAEyBp6yfV/0IspKSkiBXAgAA6qr6e7sujxIl9JxXWloqSUpJSQlyJQAAoL5KS0uVkJBQax+esn6ew+HQ999/r2bNmslisfh03yUlJUpJSdG+fft4gvslcK7qjnNVd5yr+uF81R3nqu78da4Mw1BpaanatWsnq7X2WTuM9JxntVrVvn17vx4jPj6eX4o64lzVHeeq7jhX9cP5qjvOVd3541xdaoSnGhOZAQCAKRB6AACAKRB6AiAmJkZPPvmkYmJigl1KyONc1R3nqu44V/XD+ao7zlXdhcK5YiIzAAAwBUZ6AACAKRB6AACAKRB6AACAKRB6AACAKRB6/GzWrFnq0KGDYmNjlZGRoTVr1gS7pIDLycmRxWJxeV111VXO9jNnzmjSpElq1aqVmjZtqpEjR6qoqMhlH4WFhbr55psVFxenNm3a6OGHH1ZlZWWgP4rPffbZZ7rlllvUrl07WSwWLV682KXdMAw98cQTatu2rZo0aaKsrCx99913Ln2OHTumu+66S/Hx8WrevLnuv/9+nTx50qXPxo0bdd111yk2NlYpKSn6/e9/7++P5nOXOlf33ntvjZ+zG2+80aWPWc5Vbm6u+vfvr2bNmqlNmzYaMWKEtm3b5tLHV793+fn56tOnj2JiYtSlSxfNnTvX3x/Pp+pyroYMGVLjZ+uXv/ylSx8znKtXX31VPXv2dC4umJmZqY8++sjZHhY/Uwb8Zv78+UZ0dLQxZ84cY8uWLcaECROM5s2bG0VFRcEuLaCefPJJo1u3bsbBgwedr8OHDzvbf/nLXxopKSlGXl6esXbtWuOaa64xrr32Wmd7ZWWl0b17dyMrK8tYv3698eGHHxqJiYnG9OnTg/FxfOrDDz80HnvsMeP99983JBmLFi1yaX/hhReMhIQEY/HixcbXX39t3HrrrUbHjh2N06dPO/vceOONRnp6urFq1Srj888/N7p06WKMHj3a2V5cXGwkJSUZd911l7F582bj3XffNZo0aWL85S9/CdTH9IlLnatx48YZN954o8vP2bFjx1z6mOVcDR8+3HjzzTeNzZs3Gxs2bDB+/OMfG6mpqcbJkyedfXzxe7dr1y4jLi7OyM7ONr755htj5syZRkREhLF06dKAfl5v1OVcDR482JgwYYLLz1ZxcbGz3Szn6p///KfxwQcfGNu3bze2bdtmPProo0ZUVJSxefNmwzDC42eK0ONHAwYMMCZNmuT8e1VVldGuXTsjNzc3iFUF3pNPPmmkp6e7bTtx4oQRFRVl/N///Z9z29atWw1JxsqVKw3DOPdlZ7VaDbvd7uzz6quvGvHx8UZ5eblfaw+ki7/IHQ6HYbPZjJdeesm57cSJE0ZMTIzx7rvvGoZhGN98840hyfjqq6+cfT766CPDYrEYBw4cMAzDMP73f//XaNGihcu5mjZtmnHllVf6+RP5j6fQc9ttt3l8j1nPlWEYxqFDhwxJxooVKwzD8N3v3W9/+1ujW7duLscaNWqUMXz4cH9/JL+5+FwZxrnQM2XKFI/vMeu5MgzDaNGihfHGG2+Ezc8Ul7f8pKKiQgUFBcrKynJus1qtysrK0sqVK4NYWXB89913ateunTp16qS77rpLhYWFkqSCggKdPXvW5TxdddVVSk1NdZ6nlStXqkePHkpKSnL2GT58uEpKSrRly5bAfpAA2r17t+x2u8u5SUhIUEZGhsu5ad68ufr16+fsk5WVJavVqtWrVzv7DBo0SNHR0c4+w4cP17Zt23T8+PEAfZrAyM/PV5s2bXTllVdq4sSJOnr0qLPNzOequLhYktSyZUtJvvu9W7lypcs+qvuE879xF5+rau+8844SExPVvXt3TZ8+XWVlZc42M56rqqoqzZ8/X6dOnVJmZmbY/EzxwFE/OXLkiKqqqlz+z5WkpKQkffvtt0GqKjgyMjI0d+5cXXnllTp48KCeeuopXXfdddq8ebPsdruio6PVvHlzl/ckJSXJbrdLkux2u9vzWN3WWFV/Nnef/cJz06ZNG5f2yMhItWzZ0qVPx44da+yjuq1FixZ+qT/QbrzxRv30pz9Vx44dtXPnTj366KO66aabtHLlSkVERJj2XDkcDk2dOlUDBw5U9+7dJclnv3ee+pSUlOj06dNq0qSJPz6S37g7V5I0ZswYpaWlqV27dtq4caOmTZumbdu26f3335dkrnO1adMmZWZm6syZM2ratKkWLVqkrl27asOGDWHxM0Xogd/ddNNNzj/37NlTGRkZSktL0z/+8Y+w+UVH6Lvzzjudf+7Ro4d69uypzp07Kz8/X8OGDQtiZcE1adIkbd68WV988UWwSwl5ns7VAw884Pxzjx491LZtWw0bNkw7d+5U586dA11mUF155ZXasGGDiouL9d5772ncuHFasWJFsMuqMy5v+UliYqIiIiJqzFwvKiqSzWYLUlWhoXnz5rriiiu0Y8cO2Ww2VVRU6MSJEy59LjxPNpvN7Xmsbmusqj9bbT9DNptNhw4dcmmvrKzUsWPHTH/+OnXqpMTERO3YsUOSOc/Vgw8+qH/9619avny52rdv79zuq987T33i4+PD7j9oPJ0rdzIyMiTJ5WfLLOcqOjpaXbp0Ud++fZWbm6v09HT96U9/CpufKUKPn0RHR6tv377Ky8tzbnM4HMrLy1NmZmYQKwu+kydPaufOnWrbtq369u2rqKgol/O0bds2FRYWOs9TZmamNm3a5PKF9cknnyg+Pl5du3YNeP2B0rFjR9lsNpdzU1JSotWrV7ucmxMnTqigoMDZ59NPP5XD4XD+w5yZmanPPvtMZ8+edfb55JNPdOWVV4bl5Zq62r9/v44ePaq2bdtKMte5MgxDDz74oBYtWqRPP/20xiU7X/3eZWZmuuyjuk84/Rt3qXPlzoYNGyTJ5WfLDOfKHYfDofLy8vD5mfLJdGi4NX/+fCMmJsaYO3eu8c033xgPPPCA0bx5c5eZ62bw0EMPGfn5+cbu3buNL7/80sjKyjISExONQ4cOGYZx7jbH1NRU49NPPzXWrl1rZGZmGpmZmc73V9/meMMNNxgbNmwwli5darRu3bpR3LJeWlpqrF+/3li/fr0hyfjjH/9orF+/3ti7d69hGOduWW/evLmxZMkSY+PGjcZtt93m9pb13r17G6tXrza++OIL4/LLL3e5DfvEiRNGUlKScc899xibN2825s+fb8TFxYXdbdi1navS0lLjN7/5jbFy5Upj9+7dxr///W+jT58+xuWXX26cOXPGuQ+znKuJEycaCQkJRn5+vstt1mVlZc4+vvi9q769+OGHHza2bt1qzJo1K+xuw77UudqxY4fx9NNPG2vXrjV2795tLFmyxOjUqZMxaNAg5z7Mcq4eeeQRY8WKFcbu3buNjRs3Go888ohhsViMjz/+2DCM8PiZIvT42cyZM43U1FQjOjraGDBggLFq1apglxRwo0aNMtq2bWtER0cbycnJxqhRo4wdO3Y420+fPm386le/Mlq0aGHExcUZt99+u3Hw4EGXfezZs8e46aabjCZNmhiJiYnGQw89ZJw9ezbQH8Xnli9fbkiq8Ro3bpxhGOduW//d735nJCUlGTExMcawYcOMbdu2uezj6NGjxujRo42mTZsa8fHxxvjx443S0lKXPl9//bXxX//1X0ZMTIyRnJxsvPDCC4H6iD5T27kqKyszbrjhBqN169ZGVFSUkZaWZkyYMKHGf2CY5Vy5O0+SjDfffNPZx1e/d8uXLzd69eplREdHG506dXI5Rji41LkqLCw0Bg0aZLRs2dKIiYkxunTpYjz88MMu6/QYhjnO1X333WekpaUZ0dHRRuvWrY1hw4Y5A49hhMfPlMUwDMM3Y0YAAAChizk9AADAFAg9AADAFAg9AADAFAg9AADAFAg9AADAFAg9AADAFAg9AADAFAg9AADAFAg9AIJq586dslgsslqtOnz4sNs+b7/9tiwWiywWi95++223fQ4fPiyr1SqLxaKdO3dqz549zvdUv5599tlL1nPy5El16tTJ+Z79+/fX6GO322vsOycnp16fG0DgEXoABFXnzp2VkpIiwzC0YsUKt32WL1/u/HN+fr7bPvn5+TIMQykpKercubNz+2WXXaZx48Zp3LhxSk9Pv2Q9Dz/8sPbs2VNrnyZNmtRrnwBCA6EHQNANHTpUkmu4uVB+fr5at26t9u3b1xp6LtxXtcTERM2dO1dz587VLbfcUmsdn3zyiWbPnq1JkybV2i8hIcG5zxEjRtTaF0DoIPQACLraQs++ffu0a9cuDR48WIMHD9bOnTu1b9++Gv2q33tx6KmrkpIS3X///erYsaNeeOGFBu0DQGgj9AAIuuqgsnXrVhUVFbm0VY/gDBkyRIMHD3bZVq2oqEhbt2512Vd9TZ06Vfv379cbb7yhyy67rEH7ABDaCD0Agi4tLU0dO3aUVDPQVP+9eqRHqjkiVN2nY8eOSktLq/fxP/jgA7355puaMGGCfvSjH9X7/QDCA6EHQEjwdImrej5Pt27ddMUVV8hms3kMRg0Z5Tl+/LgmTJiglJQUvfTSSw2qHUB4IPQACAnuQk9hYaF27dqlQYMGyWKxSDo34rN7927t3bvX2c+b+TwPPvigDh48qNdee03x8fHefAQAIY7QAyAkVAeW7du36+DBg5JcL21Vu3hej91u17Zt21z2UVfvv/++5s2bp/Hjx+vGG2/0pnwAYYDQAyAkJCcn6/LLL5f0w8jNhZOYq10ceqr/9/LLL1dycnKdj3fkyBFNnDhR7dq10x//+EfvigcQFiKDXQAAVBs6dKi+++47LV++XGPGjFF+fr5atWql7t27O/t07dpVrVu3dgajhl7a+uKLL3To0CG1b9++1rV2fvaznykmJkb33nuv7r333np/JgChg9ADIGQMHTpUr732mpYvX67CwkLt3r1bt99+u3M+T7VBgwZp4cKF2rNnj1eTmCVp//79bh81UW3VqlWSXEebAIQnQg+AkFEdLHbu3Ol8xpa7sDF48GAtXLhQ77zzjrZv3+6xX21GjBghwzA8tlcHrX379ql9+/b12jeA0MScHgAhw2az6eqrr5Yk/eEPf5DkOfRIcs7Fufrqq2Wz2QJTJICwRegBEFKqL1MdO3ZMLVu2VI8ePWr06dGjh1q2bKljx465vAcAakPoARBSLgwwF67PcyGLxaLrrrvO7XsAwBPm9AAIKXfccUetc22qLV682K911KUGAOGF0AOgUTty5IjzVvORI0fqlltu8XqfxcXFmjJliiRpw4YNXu8PQGAQegA0aqdOndLf/vY3SVKXLl18EnpOnz7t3CeA8GExGMMFAAAmwERmAABgCoQeAABgCoQeAABgCoQeAABgCoQeAABgCoQeAABgCoQeAABgCoQeAABgCoQeAABgCv8fdlUcGEDOocwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w0_evolution, \".\", color = 'r')\n",
    "ax.set_xlabel(\"W[0]\", fontsize=16)\n",
    "ax.set_ylabel(\"Evolution\", fontsize=16)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w1_evolution, \".\", color = 'purple')\n",
    "ax.set_xlabel(\"W[1]\", fontsize=16)\n",
    "ax.set_ylabel(\"Evolution\", fontsize=16)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w2_evolution, \".\", color = 'green')\n",
    "ax.set_xlabel(\"W[2]\", fontsize=16)\n",
    "ax.set_ylabel(\"Evolution\", fontsize=16)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w3_evolution, \".\", color = 'orange')\n",
    "ax.set_xlabel(\"W[3]\", fontsize=16)\n",
    "ax.set_ylabel(\"Evolution\", fontsize=16)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w4_evolution, \".\", color = 'brown')\n",
    "ax.set_xlabel(\"W[4]\", fontsize=16)\n",
    "ax.set_ylabel(\"Evolution\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d6e2b-6bc5-48f1-ae94-2fb25464091d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c6d0d-fb94-467b-9a96-4be3a64c8892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dfc16-ad86-41db-b02a-cf00af218566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
